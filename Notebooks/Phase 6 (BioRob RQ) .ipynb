{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a62d91-e3c2-46a0-a993-f6421e377d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# Phase 6 (BioRob RQ) — SSL → Safety-Aware Tri-Modal LOSO Trainer\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "\n",
    "class CFG:\n",
    "    # Paths (must match Phase 5 exports)\n",
    "    ROOT_DIR = Path(r\"/home/tsultan1/BioRob(Final)/Data\")\n",
    "    DATASET_DIR = ROOT_DIR / \"_dataset_icml_v1\"\n",
    "    print(\"Dataset dir:\", DATASET_DIR)\n",
    "\n",
    "    # Phase-5 export prefixes\n",
    "    SSL_PREFIX      = \"exports_v1_ssl\"        # unbalanced, for SSL stage\n",
    "    BALANCED_PREFIX = \"exports_v1_balanced\"   # balanced, for supervised LOSO\n",
    "\n",
    "    # Task codes used (with rest)\n",
    "    TASK_CODES      = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "    # Device / loader\n",
    "    DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    NUM_WORKERS = 2\n",
    "\n",
    "    # ---------------- SSL training (Stage 1) ----------------\n",
    "    USE_SSL          = True\n",
    "    SSL_EPOCHS       = 30          # was 20 (or 10); more SSL → better backbone\n",
    "    SSL_BATCH        = 64\n",
    "    SSL_LR           = 1e-3\n",
    "    SSL_MAX_WINDOWS  = 3000        # was 1500; use more SSL windows for final run\n",
    "    SSL_MASK_PROB    = 0.15\n",
    "    SSL_TEMPERATURE  = 0.1\n",
    "    SSL_MODALITY_DROPOUT = 0.25\n",
    "\n",
    "    \n",
    "    # ---------------- Supervised fine-tuning (Stage 2) ----------------\n",
    "    SUP_EPOCHS        = 40         # was 30\n",
    "    SUP_BATCH         = 64\n",
    "    SUP_LR            = 1e-4\n",
    "    WEIGHT_DECAY      = 1e-4\n",
    "    PATIENCE          = 7          # was 5 → allow more epochs before early stop\n",
    "    BACKBONE_LR_SCALE = 0.1\n",
    "\n",
    "    # Relative weight of action-vs-task losses\n",
    "    ALPHA_ACTION      = 0.4   # weight for action loss\n",
    "    BETA_TASK         = 0.6   # weight for task loss\n",
    "    \n",
    "    \n",
    "    # Architecture\n",
    "    D_MODEL           = 128\n",
    "    DROPOUT           = 0.2\n",
    "    N_HEADS_FUSE      = 4\n",
    "    N_LAYERS_FUSE     = 2\n",
    "\n",
    "    # Optional temporal pooling before fusion\n",
    "    POOL_STRIDE       = 2          # was 4 → keep more temporal detail for tasks\n",
    "\n",
    "    # SSL loss weights\n",
    "    LAMBDA_MASK       = 1.0\n",
    "    LAMBDA_CONTRAST   = 0.5        # was 0.3 → stronger cross-view consistency\n",
    "    LAMBDA_ORDER      = 0.0\n",
    "    LAMBDA_GATE       = 0.0\n",
    "    LAMBDA_XMOD       = 0.3        # was 0.2 → slightly stronger cross-modal link\n",
    "\n",
    "    # Supervised ET dropout\n",
    "    SUP_ET_DROPOUT    = 0.1        # was 0.2 → ET still regularized but less brutal\n",
    "\n",
    "    # Metrics\n",
    "    TOPK              = (1, 3)\n",
    "\n",
    "    # Policy / trade-off grid for P2\n",
    "    P2_THRESHOLDS     = [round(x, 2) for x in np.linspace(0.1, 0.9, 17)]\n",
    "\n",
    "    # Random seed\n",
    "    SEED              = 42\n",
    "\n",
    "    # Feature usage (from Phase 5.5)\n",
    "    USE_EEG_PSD_FEATURES = True   # use EEG PSD/Hjorth vectors\n",
    "    USE_EMG_FEATURES     = True   # use EMG feature vectors\n",
    "\n",
    "\n",
    "# Ablations for MSG (multimodal synergy)\n",
    "ABLATIONS = {\n",
    "    \"all\": {\"use_eeg\": True,  \"use_emg\": True,  \"use_et\": True},\n",
    "    \"eeg\": {\"use_eeg\": True,  \"use_emg\": False, \"use_et\": False},\n",
    "    \"emg\": {\"use_eeg\": False, \"use_emg\": True,  \"use_et\": False},\n",
    "    \"et\":  {\"use_eeg\": False, \"use_emg\": False, \"use_et\": True},\n",
    "}\n",
    "\n",
    "# Robustness scenarios S0–S3 (sensor failures)\n",
    "SCENARIOS = {\n",
    "    \"S0\": {\"drop_eeg\": False, \"drop_emg\": False, \"drop_et\": False,\n",
    "           \"description\": \"All sensors OK\"},\n",
    "    \"S1\": {\"drop_eeg\": True,  \"drop_emg\": False, \"drop_et\": False,\n",
    "           \"description\": \"EEG failure (EEG dropped)\"},\n",
    "    \"S2\": {\"drop_eeg\": False, \"drop_emg\": True,  \"drop_et\": False,\n",
    "           \"description\": \"EMG failure (EMG dropped)\"},\n",
    "    \"S3\": {\"drop_eeg\": False, \"drop_emg\": False, \"drop_et\": True,\n",
    "           \"description\": \"Eye-tracking failure (ET dropped)\"},\n",
    "}\n",
    "\n",
    "POLICIES = [\"P0\", \"P1\", \"P2\"]  # naive, gating-aware, safety-first\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(CFG.SEED)\n",
    "\n",
    "\n",
    "# ---------------- POSITIONAL ENCODING (for fusion only) ----------------\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 4000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float32)\n",
    "            * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 == 1:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)   # (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:, :T]\n",
    "\n",
    "\n",
    "# ---------------- BIO-ROB TCN+GRU ENCODERS ----------------\n",
    "\n",
    "class EEGTCNGRUEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    BioRob EEG encoder:\n",
    "      - Temporal ConvNet (dilated conv) + BiGRU\n",
    "      - Lighter than ICML EEGGATEncoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, d_model: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        hidden = d_model\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_ch, hidden, kernel_size=5, padding=2, dilation=1)\n",
    "        self.bn1   = nn.BatchNorm1d(hidden)\n",
    "        self.conv2 = nn.Conv1d(hidden, hidden, kernel_size=5, padding=4, dilation=2)\n",
    "        self.bn2   = nn.BatchNorm1d(hidden)\n",
    "        self.conv3 = nn.Conv1d(hidden, hidden, kernel_size=5, padding=8, dilation=4)\n",
    "        self.bn3   = nn.BatchNorm1d(hidden)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden,\n",
    "            hidden_size=hidden // 2,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        self.proj = nn.Linear(hidden, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, C)\n",
    "        x = x.transpose(1, 2)          # (B, C, T)\n",
    "        x = self.conv1(x); x = self.bn1(x); x = self.relu(x)\n",
    "\n",
    "        # small residual block to stabilize deeper dilations (helps EEG)\n",
    "        res = x\n",
    "        x = self.conv2(x); x = self.bn2(x); x = self.relu(x)\n",
    "        x = self.conv3(x); x = self.bn3(x)\n",
    "        x = x + res\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)            # (B, hidden, T)\n",
    "\n",
    "        x = x.transpose(1, 2)          # (B, T, hidden)\n",
    "        x, _ = self.gru(x)             # (B, T, hidden)\n",
    "        x = self.proj(x)               # (B, T, d_model)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EMGTCNGRUEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    BioRob EMG encoder: TCN + BiGRU.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, d_model: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        hidden = d_model\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_ch, hidden, kernel_size=7, padding=3, dilation=1)\n",
    "        self.bn1   = nn.BatchNorm1d(hidden)\n",
    "        self.conv2 = nn.Conv1d(hidden, hidden, kernel_size=7, padding=6, dilation=2)\n",
    "        self.bn2   = nn.BatchNorm1d(hidden)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden,\n",
    "            hidden_size=hidden // 2,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        self.proj = nn.Linear(hidden, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, C)\n",
    "        x = x.transpose(1, 2)          # (B, C, T)\n",
    "\n",
    "        # small residual block here too\n",
    "        res = self.conv1(x); res = self.bn1(res); res = self.relu(res)\n",
    "        x = self.conv2(res); x = self.bn2(x); x = self.relu(x)\n",
    "        x = x + res\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.transpose(1, 2)          # (B, T, hidden)\n",
    "        x, _ = self.gru(x)             # (B, T, hidden)\n",
    "        x = self.proj(x)               # (B, T, d_model)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EyeTinyGRUEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    BioRob Eye-tracking encoder: tiny MLP + BiGRU.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, d_model: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_ch, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=d_model,\n",
    "            hidden_size=d_model // 2,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, C)\n",
    "        x = self.mlp(x)                # (B, T, d_model)\n",
    "        x, _ = self.gru(x)             # (B, T, d_model)\n",
    "        x = self.proj(x)               # (B, T, d_model)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------------- GATED CROSS-MODAL SELF-ATTENTION ----------------\n",
    "\n",
    "class GatedSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    One layer of gated multihead self-attention + FFN.\n",
    "\n",
    "    - x:        (B, L, D)\n",
    "    - g_tokens: (B, L, D), per-token, per-feature gates applied to K,V.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        g_tokens: torch.Tensor,\n",
    "        return_attn: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        Q = x\n",
    "        K = x * g_tokens\n",
    "        V = x * g_tokens\n",
    "\n",
    "        attn_out, attn_weights = self.mha(\n",
    "            Q, K, V,\n",
    "            need_weights=return_attn,\n",
    "            average_attn_weights=True,\n",
    "        )\n",
    "        if not return_attn:\n",
    "            attn_weights = None\n",
    "\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_out))\n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "class GatedCrossModalEncoder(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, n_layers: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            GatedSelfAttentionBlock(d_model, n_heads, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tokens: torch.Tensor,\n",
    "        g_tokens: torch.Tensor,\n",
    "        return_attn: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        x = self.pos_encoding(tokens)\n",
    "        last_attn = None\n",
    "        for li, layer in enumerate(self.layers):\n",
    "            want_attn = return_attn and (li == len(self.layers) - 1)\n",
    "            x, attn = layer(x, g_tokens, return_attn=want_attn)\n",
    "            if attn is not None:\n",
    "                last_attn = attn\n",
    "        return (x, last_attn) if return_attn else (x, None)\n",
    "\n",
    "\n",
    "# ---------------- TRI-MODAL SAFETY-AWARE MODEL ----------------\n",
    "\n",
    "class TriModalSafetyTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Tri-modal backbone:\n",
    "      - EEGTCNGRUEncoder, EMGTCNGRUEncoder, EyeTinyGRUEncoder\n",
    "      - Gated cross-modal self-attention over concatenated tokens\n",
    "      - Modality-specific CLS embeddings (z_EEG, z_EMG, z_ET)\n",
    "      - Feature-wise modality gates (g_EEG, g_EMG, g_ET)\n",
    "      - Fused CLS z_fused = Σ_m g_m ⊙ z_m\n",
    "      - Optional Phase 5.5 feature fusion (EEG-PSD, EMG features) with z-scoring\n",
    "      - SSL decoders + cross-modal prediction heads\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eeg_ch: int,\n",
    "        emg_ch: int,\n",
    "        et_ch: int,\n",
    "        num_task_classes: int,\n",
    "        d_model: int = 128,\n",
    "        dropout: float = 0.2,\n",
    "        use_eeg_psd: bool = False,\n",
    "        use_emg_feat: bool = False,\n",
    "        eeg_psd_dim: int = 0,\n",
    "        emg_feat_dim: int = 0,\n",
    "        eeg_psd_mean: Optional[torch.Tensor] = None,\n",
    "        eeg_psd_std: Optional[torch.Tensor] = None,\n",
    "        emg_feat_mean: Optional[torch.Tensor] = None,\n",
    "        emg_feat_std: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.use_eeg_psd = use_eeg_psd and (eeg_psd_dim > 0)\n",
    "        self.use_emg_feat = use_emg_feat and (emg_feat_dim > 0)\n",
    "        self.eeg_psd_dim = eeg_psd_dim\n",
    "        self.emg_feat_dim = emg_feat_dim\n",
    "\n",
    "        # Encoders\n",
    "        self.eeg_enc = EEGTCNGRUEncoder(eeg_ch, d_model, dropout)\n",
    "        self.emg_enc = EMGTCNGRUEncoder(emg_ch, d_model, dropout)\n",
    "        self.et_enc  = EyeTinyGRUEncoder(et_ch, d_model, dropout)\n",
    "\n",
    "        # Fusion encoder\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.fuse_encoder = GatedCrossModalEncoder(\n",
    "            d_model=d_model,\n",
    "            n_heads=CFG.N_HEADS_FUSE,\n",
    "            n_layers=CFG.N_LAYERS_FUSE,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # Projections for window-level EEG/EMG features (Phase 5.5)\n",
    "        if self.use_eeg_psd:\n",
    "            self.eeg_psd_proj = nn.Sequential(\n",
    "                nn.Linear(eeg_psd_dim, d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            )\n",
    "            # Fold-wise stats (if provided) or default (0,1)\n",
    "            if eeg_psd_mean is not None and eeg_psd_std is not None:\n",
    "                self.register_buffer(\"eeg_psd_mean\", eeg_psd_mean.view(1, -1))\n",
    "                self.register_buffer(\"eeg_psd_std\", eeg_psd_std.view(1, -1))\n",
    "            else:\n",
    "                self.register_buffer(\"eeg_psd_mean\", torch.zeros(1, eeg_psd_dim))\n",
    "                self.register_buffer(\"eeg_psd_std\", torch.ones(1, eeg_psd_dim))\n",
    "\n",
    "        if self.use_emg_feat:\n",
    "            self.emg_feat_proj = nn.Sequential(\n",
    "                nn.Linear(emg_feat_dim, d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            )\n",
    "            # Fold-wise stats (if provided) or default (0,1)\n",
    "            if emg_feat_mean is not None and emg_feat_std is not None:\n",
    "                self.register_buffer(\"emg_feat_mean\", emg_feat_mean.view(1, -1))\n",
    "                self.register_buffer(\"emg_feat_std\", emg_feat_std.view(1, -1))\n",
    "            else:\n",
    "                self.register_buffer(\"emg_feat_mean\", torch.zeros(1, emg_feat_dim))\n",
    "                self.register_buffer(\"emg_feat_std\", torch.ones(1, emg_feat_dim))\n",
    "\n",
    "        # Gating MLP → feature-wise modality gates\n",
    "        self.gate_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model * 3, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, d_model * 3),\n",
    "        )\n",
    "\n",
    "        # Classification heads\n",
    "        self.action_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 2),\n",
    "        )\n",
    "        self.task_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_task_classes),\n",
    "        )\n",
    "\n",
    "        # SSL decoders\n",
    "        self.dec_eeg = nn.Linear(d_model, eeg_ch)\n",
    "        self.dec_emg = nn.Linear(d_model, emg_ch)\n",
    "        self.dec_et  = nn.Linear(d_model, et_ch)\n",
    "\n",
    "        # Temporal order (correct vs shuffled segments)\n",
    "        self.order_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 2),\n",
    "        )\n",
    "\n",
    "        # Cross-modal prediction (CLS_→CLS_)\n",
    "        self.cross_eeg2emg = nn.Linear(d_model, d_model)\n",
    "        self.cross_eeg2et  = nn.Linear(d_model, d_model)\n",
    "        self.cross_emg2eeg = nn.Linear(d_model, d_model)\n",
    "        self.cross_et2eeg  = nn.Linear(d_model, d_model)\n",
    "\n",
    "    # --------- Backbone (shared) ---------\n",
    "\n",
    "    def forward_backbone(\n",
    "        self,\n",
    "        x_eeg: torch.Tensor,\n",
    "        x_emg: torch.Tensor,\n",
    "        x_et: torch.Tensor,\n",
    "        eeg_psd: Optional[torch.Tensor] = None,\n",
    "        emg_feat: Optional[torch.Tensor] = None,\n",
    "        return_attn: bool = False,\n",
    "    ):\n",
    "        # (B, T, C) → (B, T, d)\n",
    "        z_eeg = self.eeg_enc(x_eeg)\n",
    "        z_emg = self.emg_enc(x_emg)\n",
    "        z_et  = self.et_enc(x_et)\n",
    "\n",
    "        # Optional temporal pooling to shorten sequences before attention\n",
    "        pool_s = getattr(CFG, \"POOL_STRIDE\", 1)\n",
    "        if pool_s > 1:\n",
    "            z_eeg = z_eeg[:, ::pool_s, :]\n",
    "            z_emg = z_emg[:, ::pool_s, :]\n",
    "            z_et  = z_et[:,  ::pool_s, :]\n",
    "\n",
    "        B, T_eeg, _ = z_eeg.shape\n",
    "        _, T_emg, _ = z_emg.shape\n",
    "        _, T_et,  _ = z_et.shape\n",
    "\n",
    "        # Pre-fusion pooled features\n",
    "        p_eeg = z_eeg.mean(dim=1)   # (B, d)\n",
    "        p_emg = z_emg.mean(dim=1)\n",
    "        p_et  = z_et.mean(dim=1)\n",
    "\n",
    "        gates_logits = self.gate_mlp(torch.cat([p_eeg, p_emg, p_et], dim=-1))  # (B, 3d)\n",
    "        gates_logits = gates_logits.view(-1, 3, self.d_model)                 # (B,3,d)\n",
    "        gates = torch.softmax(gates_logits, dim=1)                             # (B,3,d)\n",
    "\n",
    "        g_eeg = gates[:, 0, :].unsqueeze(1)   # (B,1,d)\n",
    "        g_emg = gates[:, 1, :].unsqueeze(1)\n",
    "        g_et  = gates[:, 2, :].unsqueeze(1)\n",
    "\n",
    "        g_eeg_tokens = g_eeg.expand(-1, T_eeg, -1)\n",
    "        g_emg_tokens = g_emg.expand(-1, T_emg, -1)\n",
    "        g_et_tokens  = g_et.expand(-1, T_et,  -1)\n",
    "        g_tokens_no_cls = torch.cat([g_eeg_tokens, g_emg_tokens, g_et_tokens], dim=1)\n",
    "\n",
    "        # Concatenate tokens and add CLS\n",
    "        z_cat = torch.cat([z_eeg, z_emg, z_et], dim=1)   # (B, T_all, d)\n",
    "        cls = self.cls_token.expand(B, 1, self.d_model)\n",
    "        tokens = torch.cat([cls, z_cat], dim=1)          # (B, 1+T_all, d)\n",
    "\n",
    "        g_cls = torch.ones(B, 1, self.d_model, device=tokens.device)\n",
    "        g_tokens = torch.cat([g_cls, g_tokens_no_cls], dim=1)  # (B, 1+T_all, d)\n",
    "\n",
    "        z_fused_all, attn = self.fuse_encoder(tokens, g_tokens, return_attn=return_attn)\n",
    "\n",
    "        # Slice modality tokens after fusion\n",
    "        idx_eeg_start = 1\n",
    "        idx_eeg_end   = 1 + T_eeg\n",
    "        idx_emg_start = idx_eeg_end\n",
    "        idx_emg_end   = idx_emg_start + T_emg\n",
    "        idx_et_start  = idx_emg_end\n",
    "        idx_et_end    = idx_et_start + T_et\n",
    "\n",
    "        z_eeg_post = z_fused_all[:, idx_eeg_start:idx_eeg_end, :]\n",
    "        z_emg_post = z_fused_all[:, idx_emg_start:idx_emg_end, :]\n",
    "        z_et_post  = z_fused_all[:, idx_et_start:idx_et_end, :]\n",
    "\n",
    "        cls_eeg = z_eeg_post.mean(dim=1)\n",
    "        cls_emg = z_emg_post.mean(dim=1)\n",
    "        cls_et  = z_et_post.mean(dim=1)\n",
    "\n",
    "        g_eeg_feat = gates[:, 0, :]\n",
    "        g_emg_feat = gates[:, 1, :]\n",
    "        g_et_feat  = gates[:, 2, :]\n",
    "\n",
    "        z_cls = (\n",
    "            g_eeg_feat * cls_eeg\n",
    "            + g_emg_feat * cls_emg\n",
    "            + g_et_feat  * cls_et\n",
    "        )\n",
    "\n",
    "        # Optionally fuse EEG/EMG window-level features (Phase 5.5) with z-scoring\n",
    "        feat_list = []\n",
    "        if eeg_psd is not None and hasattr(self, \"eeg_psd_proj\"):\n",
    "            if hasattr(self, \"eeg_psd_mean\") and hasattr(self, \"eeg_psd_std\"):\n",
    "                eeg_psd_norm = (eeg_psd - self.eeg_psd_mean) / (self.eeg_psd_std + 1e-6)\n",
    "            else:\n",
    "                eeg_psd_norm = eeg_psd\n",
    "            feat_list.append(self.eeg_psd_proj(eeg_psd_norm))   # (B, d_model)\n",
    "\n",
    "        if emg_feat is not None and hasattr(self, \"emg_feat_proj\"):\n",
    "            if hasattr(self, \"emg_feat_mean\") and hasattr(self, \"emg_feat_std\"):\n",
    "                emg_feat_norm = (emg_feat - self.emg_feat_mean) / (self.emg_feat_std + 1e-6)\n",
    "            else:\n",
    "                emg_feat_norm = emg_feat\n",
    "            feat_list.append(self.emg_feat_proj(emg_feat_norm)) # (B, d_model)\n",
    "\n",
    "        if feat_list:\n",
    "            # Simple additive fusion: average projected features and add to z_cls\n",
    "            feat_fused = torch.stack(feat_list, dim=0).mean(dim=0)  # (B, d_model)\n",
    "            z_cls = z_cls + feat_fused\n",
    "\n",
    "        return z_eeg_post, z_emg_post, z_et_post, z_cls, gates, cls_eeg, cls_emg, cls_et, attn\n",
    "\n",
    "    # --------- Supervised forward ---------\n",
    "\n",
    "    def forward_supervised(\n",
    "        self,\n",
    "        x_eeg: torch.Tensor,\n",
    "        x_emg: torch.Tensor,\n",
    "        x_et: torch.Tensor,\n",
    "        eeg_psd: Optional[torch.Tensor] = None,\n",
    "        emg_feat: Optional[torch.Tensor] = None,\n",
    "        return_attn: bool = False,\n",
    "    ):\n",
    "        z_eeg, z_emg, z_et, z_cls, gates, cls_eeg, cls_emg, cls_et, attn = self.forward_backbone(\n",
    "            x_eeg, x_emg, x_et,\n",
    "            eeg_psd=eeg_psd,\n",
    "            emg_feat=emg_feat,\n",
    "            return_attn=return_attn,\n",
    "        )\n",
    "\n",
    "        logits_action = self.action_head(z_cls)\n",
    "        logits_task   = self.task_head(z_cls)\n",
    "        return logits_action, logits_task, {\n",
    "            \"gates\": gates, \"cls_eeg\": cls_eeg, \"cls_emg\": cls_emg,\n",
    "            \"cls_et\": cls_et, \"attn\": attn,\n",
    "        }\n",
    "\n",
    "    # --------- SSL forwards ---------\n",
    "\n",
    "    def forward_ssl(\n",
    "        self,\n",
    "        x_eeg_masked: torch.Tensor,\n",
    "        x_emg_masked: torch.Tensor,\n",
    "        x_et_masked: torch.Tensor,\n",
    "    ):\n",
    "        z_eeg, z_emg, z_et, z_cls, gates, cls_eeg, cls_emg, cls_et, _ = self.forward_backbone(\n",
    "            x_eeg_masked, x_emg_masked, x_et_masked, return_attn=False\n",
    "        )\n",
    "        x_hat_eeg = self.dec_eeg(z_eeg)\n",
    "        x_hat_emg = self.dec_emg(z_emg)\n",
    "        x_hat_et  = self.dec_et(z_et)\n",
    "        return x_hat_eeg, x_hat_emg, x_hat_et, {\n",
    "            \"z_cls\": z_cls,\n",
    "            \"gates\": gates,\n",
    "            \"cls_eeg\": cls_eeg,\n",
    "            \"cls_emg\": cls_emg,\n",
    "            \"cls_et\":  cls_et,\n",
    "        }\n",
    "\n",
    "    def forward_order(\n",
    "        self,\n",
    "        x_eeg: torch.Tensor,\n",
    "        x_emg: torch.Tensor,\n",
    "        x_et: torch.Tensor,\n",
    "    ):\n",
    "        _, _, _, z_cls, _, _, _, _, _ = self.forward_backbone(\n",
    "            x_eeg, x_emg, x_et, return_attn=False\n",
    "        )\n",
    "        logits_order = self.order_head(z_cls)\n",
    "        return logits_order\n",
    "\n",
    "    \n",
    "    \n",
    "# ---------------- DATASET: PHASE 5 EXPORTS ----------------\n",
    "\n",
    "class ShardWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Reads NPZ shards from:\n",
    "        _dataset_icml_v1/{prefix}_foldK/{train,val,test}/split_shard_*.npz\n",
    "\n",
    "    Keys:\n",
    "        X_EEG: (N, T, C_eeg), X_EMG: (N, T, C_emg), X_ET: (N, T, C_et)\n",
    "        y_action ∈ {0,1}, y_task ∈ {0,1,2,4,6,8,...}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        fold_dir: Path,\n",
    "        split: str,\n",
    "        ssl_mode: bool = False,\n",
    "        max_windows: Optional[int] = None,\n",
    "        task2idx: Optional[Dict[int, int]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fold_dir = Path(fold_dir)\n",
    "        self.split = split\n",
    "        self.ssl_mode = ssl_mode\n",
    "\n",
    "        split_dir = self.fold_dir / split\n",
    "        if not split_dir.exists():\n",
    "            raise FileNotFoundError(f\"Split dir not found: {split_dir}\")\n",
    "\n",
    "        shard_paths = sorted(split_dir.glob(\"*_shard_*.npz\"))\n",
    "        if not shard_paths:\n",
    "            raise FileNotFoundError(f\"No shard npz files in {split_dir}\")\n",
    "\n",
    "        X_eeg_list, X_emg_list, X_et_list = [], [], []\n",
    "        y_action_list, y_task_list = [], []\n",
    "\n",
    "        for shard_path in shard_paths:\n",
    "            z = np.load(shard_path, allow_pickle=True)\n",
    "            X_eeg = z[\"X_EEG\"]\n",
    "            X_emg = z[\"X_EMG\"]\n",
    "            X_et  = z[\"X_ET\"]\n",
    "            y_action = z[\"y_action\"]\n",
    "            y_task   = z[\"y_task\"]\n",
    "\n",
    "            N = y_task.shape[0]\n",
    "            if N == 0:\n",
    "                continue\n",
    "\n",
    "            if ssl_mode:\n",
    "                mask_keep = np.ones(N, dtype=bool)\n",
    "            else:\n",
    "                mask_keep = np.isin(y_task, CFG.TASK_CODES)\n",
    "\n",
    "            if not mask_keep.any():\n",
    "                continue\n",
    "\n",
    "            X_eeg_list.append(X_eeg[mask_keep])\n",
    "            X_emg_list.append(X_emg[mask_keep])\n",
    "            X_et_list.append(X_et[mask_keep])\n",
    "            y_action_list.append(y_action[mask_keep])\n",
    "            y_task_list.append(y_task[mask_keep])\n",
    "\n",
    "        if not X_eeg_list:\n",
    "            raise RuntimeError(\n",
    "                f\"No windows loaded for {fold_dir} / {split} (ssl_mode={ssl_mode})\"\n",
    "            )\n",
    "\n",
    "        self.X_eeg = np.concatenate(X_eeg_list, axis=0)\n",
    "        self.X_emg = np.concatenate(X_emg_list, axis=0)\n",
    "        self.X_et  = np.concatenate(X_et_list,  axis=0)\n",
    "        self.y_action = np.concatenate(y_action_list, axis=0)\n",
    "        self.y_task   = np.concatenate(y_task_list,   axis=0)\n",
    "\n",
    "        if max_windows is not None and self.X_eeg.shape[0] > max_windows:\n",
    "            rng = np.random.RandomState(CFG.SEED)\n",
    "            idx = rng.choice(self.X_eeg.shape[0], size=max_windows, replace=False)\n",
    "            self.X_eeg    = self.X_eeg[idx]\n",
    "            self.X_emg    = self.X_emg[idx]\n",
    "            self.X_et     = self.X_et[idx]\n",
    "            self.y_action = self.y_action[idx]\n",
    "            self.y_task   = self.y_task[idx]\n",
    "\n",
    "        self.eeg_ch = self.X_eeg.shape[-1]\n",
    "        self.emg_ch = self.X_emg.shape[-1]\n",
    "        self.et_ch  = self.X_et.shape[-1]\n",
    "\n",
    "        uniq_tasks = sorted(np.unique(self.y_task))\n",
    "        if task2idx is not None:\n",
    "            self.task2idx = dict(task2idx)\n",
    "        else:\n",
    "            desired = CFG.TASK_CODES\n",
    "            present_desired = [t for t in desired if t in uniq_tasks]\n",
    "            self.task2idx = {t: i for i, t in enumerate(present_desired)}\n",
    "        self.num_task_classes = len(self.task2idx)\n",
    "\n",
    "        print(\n",
    "            f\"[ShardWindowDataset] {split_dir} | N={self.X_eeg.shape[0]}, \"\n",
    "            f\"ssl_mode={self.ssl_mode}, tasks={sorted(np.unique(self.y_task))}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Shapes: X_eeg={self.X_eeg.shape}, X_emg={self.X_emg.shape}, X_et={self.X_et.shape}\"\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.X_eeg.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        x_eeg = torch.from_numpy(self.X_eeg[idx]).float()\n",
    "        x_emg = torch.from_numpy(self.X_emg[idx]).float()\n",
    "        x_et  = torch.from_numpy(self.X_et[idx]).float()\n",
    "\n",
    "        y_action_raw = int(self.y_action[idx])  # 0 or 1\n",
    "        y_task_raw   = int(self.y_task[idx])    # 0 or task-code\n",
    "\n",
    "        action_label = 1 if y_action_raw == 1 else 0\n",
    "        task_label = self.task2idx.get(y_task_raw, -1)\n",
    "\n",
    "        sample = {\n",
    "            \"eeg\": x_eeg,\n",
    "            \"emg\": x_emg,\n",
    "            \"et\":  x_et,\n",
    "            \"action\": torch.tensor(action_label, dtype=torch.long),\n",
    "            \"task\":   torch.tensor(task_label,   dtype=torch.long),\n",
    "            \"y_task_raw\": torch.tensor(y_task_raw, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "        # Optional Phase 5.5 features (only present for balanced supervised folds)\n",
    "        if hasattr(self, \"X_eeg_psd\"):\n",
    "            eeg_psd = torch.from_numpy(self.X_eeg_psd[idx]).float()\n",
    "            sample[\"eeg_psd\"] = eeg_psd\n",
    "        if hasattr(self, \"X_emg_feat\"):\n",
    "            emg_feat = torch.from_numpy(self.X_emg_feat[idx]).float()\n",
    "            sample[\"emg_feat\"] = emg_feat\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "def attach_features_to_dataset(ds, fold_id: int, split: str):\n",
    "    \"\"\"\n",
    "    Attach EEG-PSD and EMG feature vectors from Phase 5.5 to a balanced dataset.\n",
    "\n",
    "    Expects files:\n",
    "        features_v1_eeg_psd_full_fold{fold_id}_{split}.npz\n",
    "    with keys: X_psd (N, F_psd), X_emg (N, F_emg),\n",
    "    where N == len(ds).\n",
    "    \"\"\"\n",
    "    feat_path = CFG.DATASET_DIR / f\"features_v1_eeg_psd_full_fold{fold_id}_{split}.npz\"\n",
    "    if not feat_path.exists():\n",
    "        print(f\"[attach_features] No feature file for fold={fold_id}, split={split}: {feat_path}\")\n",
    "        return\n",
    "\n",
    "    z = np.load(feat_path, allow_pickle=True)\n",
    "    X_psd = z[\"X_psd\"]   # (N, F_psd)\n",
    "    X_emg = z[\"X_emg\"]   # (N, F_emg)\n",
    "\n",
    "    if X_psd.shape[0] != len(ds) or X_emg.shape[0] != len(ds):\n",
    "        print(\n",
    "            f\"[attach_features] MISMATCH fold={fold_id}, split={split}: \"\n",
    "            f\"len(ds)={len(ds)}, X_psd={X_psd.shape}, X_emg={X_emg.shape}. Skipping.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    ds.X_eeg_psd = X_psd\n",
    "    ds.X_emg_feat = X_emg\n",
    "    ds.eeg_psd_dim = X_psd.shape[1]\n",
    "    ds.emg_feat_dim = X_emg.shape[1]\n",
    "\n",
    "    print(\n",
    "        f\"[attach_features] Attached features for fold={fold_id}, split={split}: \"\n",
    "        f\"X_psd={X_psd.shape}, X_emg={X_emg.shape}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------- FOLD DISCOVERY & DATALOADERS ----------------\n",
    "\n",
    "def discover_folds(prefix: str) -> List[int]:\n",
    "    folds = []\n",
    "    for p in CFG.DATASET_DIR.glob(f\"{prefix}_fold*\"):\n",
    "        name = p.name\n",
    "        try:\n",
    "            fid = int(name.split(\"fold\")[-1])\n",
    "            folds.append(fid)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return sorted(set(folds))\n",
    "\n",
    "\n",
    "def make_dataloader(ds: Dataset, batch_size: int, shuffle: bool) -> DataLoader:\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=CFG.NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def make_ssl_dataset() -> Tuple[ConcatDataset, int, int, int]:\n",
    "    folds = discover_folds(CFG.SSL_PREFIX)\n",
    "    if not folds:\n",
    "        raise SystemExit(\n",
    "            f\"No SSL folds found with prefix {CFG.SSL_PREFIX}_fold* in {CFG.DATASET_DIR}\"\n",
    "        )\n",
    "    ds_list = []\n",
    "    eeg_ch = emg_ch = et_ch = None\n",
    "    for fid in folds:\n",
    "        fold_dir = CFG.DATASET_DIR / f\"{CFG.SSL_PREFIX}_fold{fid}\"\n",
    "        ds = ShardWindowDataset(\n",
    "            fold_dir=fold_dir,\n",
    "            split=\"train\",\n",
    "            ssl_mode=True,\n",
    "            max_windows=CFG.SSL_MAX_WINDOWS,   # CHANGED: subsample per SSL fold\n",
    "            task2idx=None,\n",
    "        )\n",
    "        ds_list.append(ds)\n",
    "        if eeg_ch is None:\n",
    "            eeg_ch, emg_ch, et_ch = ds.eeg_ch, ds.emg_ch, ds.et_ch\n",
    "        else:\n",
    "            assert eeg_ch == ds.eeg_ch and emg_ch == ds.emg_ch and et_ch == ds.et_ch, \\\n",
    "                \"Channel mismatch across SSL folds\"\n",
    "    ssl_dataset = ConcatDataset(ds_list)\n",
    "    return ssl_dataset, eeg_ch, emg_ch, et_ch\n",
    "\n",
    "\n",
    "def make_supervised_datasets_for_fold(\n",
    "    fold_id: int,\n",
    ") -> Tuple[ShardWindowDataset, ShardWindowDataset, ShardWindowDataset]:\n",
    "    fold_dir = CFG.DATASET_DIR / f\"{CFG.BALANCED_PREFIX}_fold{fold_id}\"\n",
    "    if not fold_dir.exists():\n",
    "        raise FileNotFoundError(f\"Balanced fold directory not found: {fold_dir}\")\n",
    "\n",
    "    train_ds = ShardWindowDataset(\n",
    "        fold_dir=fold_dir,\n",
    "        split=\"train\",\n",
    "        ssl_mode=False,\n",
    "        max_windows=None,\n",
    "        task2idx=None,\n",
    "    )\n",
    "    val_ds = ShardWindowDataset(\n",
    "        fold_dir=fold_dir,\n",
    "        split=\"val\",\n",
    "        ssl_mode=False,\n",
    "        max_windows=None,\n",
    "        task2idx=train_ds.task2idx,\n",
    "    )\n",
    "    test_ds = ShardWindowDataset(\n",
    "        fold_dir=fold_dir,\n",
    "        split=\"test\",\n",
    "        ssl_mode=False,\n",
    "        max_windows=None,\n",
    "        task2idx=train_ds.task2idx,\n",
    "    )\n",
    "\n",
    "    # Attach Phase 5.5 EEG-PSD + EMG features (if available)\n",
    "    attach_features_to_dataset(train_ds, fold_id, \"train\")\n",
    "    attach_features_to_dataset(val_ds,   fold_id, \"val\")\n",
    "    attach_features_to_dataset(test_ds,  fold_id, \"test\")\n",
    "\n",
    "    assert train_ds.eeg_ch == val_ds.eeg_ch == test_ds.eeg_ch\n",
    "    assert train_ds.emg_ch == val_ds.emg_ch == test_ds.emg_ch\n",
    "    assert train_ds.et_ch  == val_ds.et_ch  == test_ds.et_ch\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "# ---------------- SSL HELPERS ----------------\n",
    "\n",
    "def apply_ssl_mask(x: torch.Tensor, mask_prob: float):\n",
    "    B, T, C = x.shape\n",
    "    mask = (torch.rand(B, T, 1, device=x.device) < mask_prob).float()\n",
    "    x_masked = x * (1.0 - mask)\n",
    "    return x_masked, mask\n",
    "\n",
    "\n",
    "def apply_modality_dropout(x: torch.Tensor, drop_prob: float) -> torch.Tensor:\n",
    "    if drop_prob <= 0.0:\n",
    "        return x\n",
    "    B = x.size(0)\n",
    "    mask = (torch.rand(B, 1, 1, device=x.device) < drop_prob).float()\n",
    "    return x * (1.0 - mask)\n",
    "\n",
    "\n",
    "def ssl_loss_reconstruction(\n",
    "    x_hat: torch.Tensor, x_true: torch.Tensor, mask: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Reconstruction loss that is robust to temporal pooling.\n",
    "\n",
    "    - x_hat:  (B, T_hat, C)  → decoder output (after POOL_STRIDE)\n",
    "    - x_true: (B, T_true, C) → original time series (before pooling)\n",
    "    - mask:   (B, T_true, 1) → which timesteps were masked in x_true\n",
    "\n",
    "    If T_hat != T_true (e.g., T_true=500, T_hat=125 with stride 4),\n",
    "    we downsample x_true and mask to match T_hat before computing MSE.\n",
    "    \"\"\"\n",
    "    if x_true.size(-1) == 0:\n",
    "        return torch.tensor(0.0, device=x_true.device)\n",
    "\n",
    "    B, T_hat, C = x_hat.shape\n",
    "    _, T_true, _ = x_true.shape\n",
    "\n",
    "    # Align temporal dimension if pooling was applied\n",
    "    if T_hat != T_true:\n",
    "        # Try integer downsampling factor first\n",
    "        if T_true % T_hat == 0:\n",
    "            factor = T_true // T_hat\n",
    "            x_true = x_true[:, ::factor, :]        # (B, T_hat, C)\n",
    "            mask   = mask[:,  ::factor, :]        # (B, T_hat, 1)\n",
    "        else:\n",
    "            # Fallback: interpolate to T_hat (shouldn't happen with 500/4=125)\n",
    "            x_true = x_true.transpose(1, 2)       # (B, C, T_true)\n",
    "            x_true = F.interpolate(\n",
    "                x_true, size=T_hat, mode=\"linear\", align_corners=False\n",
    "            )\n",
    "            x_true = x_true.transpose(1, 2)       # (B, T_hat, C)\n",
    "\n",
    "            mask = mask.transpose(1, 2)           # (B, 1, T_true)\n",
    "            mask = F.interpolate(mask, size=T_hat, mode=\"nearest\")\n",
    "            mask = mask.transpose(1, 2)           # (B, T_hat, 1)\n",
    "\n",
    "    # Now x_hat, x_true, mask all have temporal dim T_hat\n",
    "    m = mask.expand_as(x_hat)                     # (B, T_hat, C)\n",
    "    diff2 = (x_hat - x_true) ** 2\n",
    "    num = (diff2 * m).sum()\n",
    "    denom = m.sum() + 1e-8\n",
    "    return num / denom\n",
    "\n",
    "\n",
    "\n",
    "def contrastive_loss(\n",
    "    z1: torch.Tensor, z2: torch.Tensor, temperature: float = 0.1\n",
    ") -> torch.Tensor:\n",
    "    if z1.size(0) < 2:\n",
    "        return torch.tensor(0.0, device=z1.device)\n",
    "\n",
    "    z1 = F.normalize(z1, dim=-1)\n",
    "    z2 = F.normalize(z2, dim=-1)\n",
    "    logits = z1 @ z2.t() / temperature\n",
    "    labels = torch.arange(z1.size(0), device=z1.device)\n",
    "    loss_12 = F.cross_entropy(logits, labels)\n",
    "    loss_21 = F.cross_entropy(logits.t(), labels)\n",
    "    return 0.5 * (loss_12 + loss_21)\n",
    "\n",
    "\n",
    "def gating_regularization(gates: torch.Tensor) -> torch.Tensor:\n",
    "    # gates: (B,3,D)\n",
    "    # NOTE: currently disabled via LAMBDA_GATE=0.0 in CFG\n",
    "    gates_mean = gates.mean(dim=-1)  # (B,3)\n",
    "    eps = 1e-8\n",
    "    ent = -(gates_mean * (gates_mean + eps).log()).sum(dim=1)  # (B,)\n",
    "    max_ent = math.log(gates_mean.size(1))\n",
    "    ent_norm = ent / (max_ent + eps)\n",
    "    return 1.0 - ent_norm.mean()  # high entropy → low loss\n",
    "\n",
    "\n",
    "def temporal_order_loss(\n",
    "    model: TriModalSafetyTransformer,\n",
    "    x_eeg: torch.Tensor,\n",
    "    x_emg: torch.Tensor,\n",
    "    x_et: torch.Tensor,\n",
    "    segments: int = 4,\n",
    ") -> torch.Tensor:\n",
    "    B, T, Ce = x_eeg.shape\n",
    "    if T < segments * 2:\n",
    "        return torch.tensor(0.0, device=x_eeg.device)\n",
    "\n",
    "    seg_len = T // segments\n",
    "    T_used = seg_len * segments\n",
    "\n",
    "    x_eeg_seg = x_eeg[:, :T_used, :].view(B, segments, seg_len, Ce)\n",
    "    x_emg_seg = x_emg[:, :T_used, :].view(B, segments, seg_len, x_emg.shape[-1])\n",
    "    x_et_seg  = x_et[:,  :T_used, :].view(B, segments, seg_len, x_et.shape[-1])\n",
    "\n",
    "    indices = list(range(segments))\n",
    "    perm = indices.copy()\n",
    "    random.shuffle(perm)\n",
    "\n",
    "    is_identity = (perm == indices)\n",
    "    label = 0 if is_identity else 1\n",
    "    y_order = torch.full((B,), label, dtype=torch.long, device=x_eeg.device)\n",
    "\n",
    "    perm_tensor = torch.tensor(perm, dtype=torch.long, device=x_eeg.device)\n",
    "\n",
    "    x_eeg_perm = x_eeg_seg[:, perm_tensor, :, :].reshape(B, T_used, Ce)\n",
    "    x_emg_perm = x_emg_seg[:, perm_tensor, :, :].reshape(B, T_used, x_emg.shape[-1])\n",
    "    x_et_perm  = x_et_seg[:,  perm_tensor, :, :].reshape(B, T_used, x_et.shape[-1])\n",
    "\n",
    "    logits_order = model.forward_order(x_eeg_perm, x_emg_perm, x_et_perm)\n",
    "    loss = F.cross_entropy(logits_order, y_order)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def cross_modal_prediction_loss(\n",
    "    model: TriModalSafetyTransformer,\n",
    "    cls_eeg: torch.Tensor,\n",
    "    cls_emg: torch.Tensor,\n",
    "    cls_et: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    if cls_eeg.size(0) == 0:\n",
    "        return torch.tensor(0.0, device=cls_eeg.device)\n",
    "\n",
    "    tgt_eeg = cls_eeg.detach()\n",
    "    tgt_emg = cls_emg.detach()\n",
    "    tgt_et  = cls_et.detach()\n",
    "\n",
    "    pred_emg_from_eeg = model.cross_eeg2emg(cls_eeg)\n",
    "    pred_et_from_eeg  = model.cross_eeg2et(cls_eeg)\n",
    "    pred_eeg_from_emg = model.cross_emg2eeg(cls_emg)\n",
    "    pred_eeg_from_et  = model.cross_et2eeg(cls_et)\n",
    "\n",
    "    loss = 0.0\n",
    "    loss += F.mse_loss(pred_emg_from_eeg, tgt_emg)\n",
    "    loss += F.mse_loss(pred_et_from_eeg,  tgt_et)\n",
    "    loss += F.mse_loss(pred_eeg_from_emg, tgt_eeg)\n",
    "    loss += F.mse_loss(pred_eeg_from_et,  tgt_eeg)\n",
    "    return loss / 4.0\n",
    "\n",
    "\n",
    "# ---------------- STAGE 1 — SSL PRETRAINING ----------------\n",
    "\n",
    "def pretrain_ssl(\n",
    "    eeg_ch: int, emg_ch: int, et_ch: int, ssl_dataset: Dataset\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    print(\"\\n================ STAGE 1 — SSL PRETRAINING ================\")\n",
    "    dummy_num_task_classes = 6  # just for the task head; not used in SSL\n",
    "\n",
    "    model = TriModalSafetyTransformer(\n",
    "        eeg_ch=eeg_ch,\n",
    "        emg_ch=emg_ch,\n",
    "        et_ch=et_ch,\n",
    "        num_task_classes=dummy_num_task_classes,\n",
    "        d_model=CFG.D_MODEL,\n",
    "        dropout=CFG.DROPOUT,\n",
    "    ).to(CFG.DEVICE)\n",
    "\n",
    "    loader = make_dataloader(ssl_dataset, batch_size=CFG.SSL_BATCH, shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CFG.SSL_LR,\n",
    "        weight_decay=CFG.WEIGHT_DECAY,\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, CFG.SSL_EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_batches = 0\n",
    "\n",
    "        for batch in loader:\n",
    "            x_eeg = batch[\"eeg\"].to(CFG.DEVICE)\n",
    "            x_emg = batch[\"emg\"].to(CFG.DEVICE)\n",
    "            x_et  = batch[\"et\"].to(CFG.DEVICE)\n",
    "\n",
    "            # View 1: masked\n",
    "            x_eeg_v1, m_eeg_v1 = apply_ssl_mask(x_eeg, CFG.SSL_MASK_PROB)\n",
    "            x_emg_v1, m_emg_v1 = apply_ssl_mask(x_emg, CFG.SSL_MASK_PROB)\n",
    "            x_et_v1,  m_et_v1  = apply_ssl_mask(x_et,  CFG.SSL_MASK_PROB)\n",
    "\n",
    "            # View 2: masked + modality dropout\n",
    "            x_eeg_v2, _ = apply_ssl_mask(x_eeg, CFG.SSL_MASK_PROB)\n",
    "            x_emg_v2, _ = apply_ssl_mask(x_emg, CFG.SSL_MASK_PROB)\n",
    "            x_et_v2,  _ = apply_ssl_mask(x_et,  CFG.SSL_MASK_PROB)\n",
    "\n",
    "            x_eeg_v2 = apply_modality_dropout(x_eeg_v2, CFG.SSL_MODALITY_DROPOUT)\n",
    "            x_emg_v2 = apply_modality_dropout(x_emg_v2, CFG.SSL_MODALITY_DROPOUT)\n",
    "            x_et_v2  = apply_modality_dropout(x_et_v2,  CFG.SSL_MODALITY_DROPOUT)\n",
    "\n",
    "            x_hat_eeg, x_hat_emg, x_hat_et, aux1 = model.forward_ssl(\n",
    "                x_eeg_v1, x_emg_v1, x_et_v1\n",
    "            )\n",
    "            z_cls1 = aux1[\"z_cls\"]\n",
    "            gates1 = aux1[\"gates\"]\n",
    "            cls_eeg = aux1[\"cls_eeg\"]\n",
    "            cls_emg = aux1[\"cls_emg\"]\n",
    "            cls_et  = aux1[\"cls_et\"]\n",
    "\n",
    "            _, _, _, z_cls2, _, _, _, _, _ = model.forward_backbone(\n",
    "                x_eeg_v2, x_emg_v2, x_et_v2\n",
    "            )\n",
    "\n",
    "            loss_eeg = ssl_loss_reconstruction(x_hat_eeg, x_eeg, m_eeg_v1)\n",
    "            loss_emg = ssl_loss_reconstruction(x_hat_emg, x_emg, m_emg_v1)\n",
    "            loss_et  = ssl_loss_reconstruction(x_hat_et,  x_et,  m_et_v1)\n",
    "\n",
    "            # Upweight EEG & EMG reconstruction in SSL\n",
    "            w_eeg, w_emg, w_et = 1.5, 1.5, 1.0   # CHANGED\n",
    "            loss_mask = w_eeg * loss_eeg + w_emg * loss_emg + w_et * loss_et\n",
    "\n",
    "            loss_contrast = contrastive_loss(z_cls1, z_cls2, CFG.SSL_TEMPERATURE) \\\n",
    "                if CFG.LAMBDA_CONTRAST > 0 else torch.tensor(0.0, device=CFG.DEVICE)\n",
    "            loss_gate = gating_regularization(gates1) \\\n",
    "                if CFG.LAMBDA_GATE > 0 else torch.tensor(0.0, device=CFG.DEVICE)\n",
    "            loss_order = temporal_order_loss(model, x_eeg, x_emg, x_et) \\\n",
    "                if CFG.LAMBDA_ORDER > 0 else torch.tensor(0.0, device=CFG.DEVICE)\n",
    "            loss_xmod = cross_modal_prediction_loss(model, cls_eeg, cls_emg, cls_et) \\\n",
    "                if CFG.LAMBDA_XMOD > 0 else torch.tensor(0.0, device=CFG.DEVICE)\n",
    "\n",
    "            loss = (\n",
    "                CFG.LAMBDA_MASK     * loss_mask\n",
    "                + CFG.LAMBDA_CONTRAST * loss_contrast\n",
    "                + CFG.LAMBDA_GATE   * loss_gate\n",
    "                + CFG.LAMBDA_ORDER  * loss_order\n",
    "                + CFG.LAMBDA_XMOD   * loss_xmod\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += float(loss.item())\n",
    "            total_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / max(1, total_batches)\n",
    "        print(f\"[SSL] Epoch {epoch:02d}/{CFG.SSL_EPOCHS} loss={avg_loss:.4f}\")\n",
    "\n",
    "    print(\"[SSL] Pretraining complete.\")\n",
    "    return model.state_dict()\n",
    "\n",
    "\n",
    "# ---------------- METRIC HELPERS ----------------\n",
    "\n",
    "def compute_per_class_metrics_from_cm(cm: np.ndarray):\n",
    "    num_classes = cm.shape[0]\n",
    "    total = cm.sum()\n",
    "    metrics = {}\n",
    "    for cls in range(num_classes):\n",
    "        tp = cm[cls, cls]\n",
    "        support = cm[cls, :].sum()\n",
    "        pred_count = cm[:, cls].sum()\n",
    "        fn = support - tp\n",
    "        fp = pred_count - tp\n",
    "        tn = total - (tp + fn + fp)\n",
    "\n",
    "        prec = float(tp / pred_count) if pred_count > 0 else 0.0\n",
    "        rec = float(tp / support) if support > 0 else 0.0\n",
    "        acc_ovr = float(tp + tn) / float(total) if total > 0 else 0.0\n",
    "\n",
    "        tpr = rec\n",
    "        tnr = float(tn) / float(tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        bal_acc = 0.5 * (tpr + tnr) if (support > 0 or (tn + fp) > 0) else 0.0\n",
    "\n",
    "        f1 = float(2 * prec * rec / (prec + rec)) if (prec + rec) > 0 else 0.0\n",
    "\n",
    "        metrics[int(cls)] = {\n",
    "            \"class_index\": int(cls),\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1\": f1,\n",
    "            \"accuracy\": acc_ovr,\n",
    "            \"balanced_accuracy\": bal_acc,\n",
    "            \"support\": int(support),\n",
    "        }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_topk_accuracies_from_logits(\n",
    "    logits: np.ndarray,\n",
    "    targets: np.ndarray,\n",
    "    ks=(1, 3),\n",
    ") -> Dict[int, float]:\n",
    "    if logits.shape[0] == 0:\n",
    "        return {k: 0.0 for k in ks}\n",
    "    t_logits = torch.from_numpy(logits)\n",
    "    t_targets = torch.from_numpy(targets)\n",
    "    maxk = max(ks)\n",
    "    _, pred = t_logits.topk(maxk, dim=1, largest=True, sorted=True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(t_targets.view(1, -1).expand_as(pred))\n",
    "    res = {}\n",
    "    for k in ks:\n",
    "        correct_k = correct[:k].any(dim=0).float().sum().item()\n",
    "        res[k] = correct_k / t_targets.size(0)\n",
    "    return res\n",
    "\n",
    "\n",
    "# ---------------- CLASS-WEIGHTED LOSSES ----------------\n",
    "\n",
    "def _collect_labels_from_dataset(train_ds):\n",
    "    if isinstance(train_ds, ShardWindowDataset):\n",
    "        y_action = np.array(train_ds.y_action, dtype=int)\n",
    "        y_task_raw = np.array(train_ds.y_task, dtype=int)\n",
    "        task2idx = dict(train_ds.task2idx)\n",
    "    elif isinstance(train_ds, torch.utils.data.Subset):\n",
    "        base = train_ds.dataset\n",
    "        indices = np.array(train_ds.indices, dtype=int)\n",
    "        assert isinstance(base, ShardWindowDataset)\n",
    "        y_action = np.array(base.y_action, dtype=int)[indices]\n",
    "        y_task_raw = np.array(base.y_task, dtype=int)[indices]\n",
    "        task2idx = dict(base.task2idx)\n",
    "    else:\n",
    "        acts, tasks_raw = [], []\n",
    "        for i in range(len(train_ds)):\n",
    "            item = train_ds[i]\n",
    "            acts.append(int(item[\"action\"]))\n",
    "            tasks_raw.append(int(item[\"y_task_raw\"]))\n",
    "        y_action = np.array(acts, dtype=int)\n",
    "        y_task_raw = np.array(tasks_raw, dtype=int)\n",
    "        uniq = sorted(np.unique(y_task_raw))\n",
    "        task2idx = {t: i for i, t in enumerate(uniq)}\n",
    "    return y_action, y_task_raw, task2idx\n",
    "\n",
    "\n",
    "def _collect_labels_from_dataset(train_ds):\n",
    "    if isinstance(train_ds, ShardWindowDataset):\n",
    "        y_action = np.array(train_ds.y_action, dtype=int)\n",
    "        y_task_raw = np.array(train_ds.y_task, dtype=int)\n",
    "        task2idx = dict(train_ds.task2idx)\n",
    "    elif isinstance(train_ds, torch.utils.data.Subset):\n",
    "        base = train_ds.dataset\n",
    "        indices = np.array(train_ds.indices, dtype=int)\n",
    "        assert isinstance(base, ShardWindowDataset)\n",
    "        y_action = np.array(base.y_action, dtype=int)[indices]\n",
    "        y_task_raw = np.array(base.y_task, dtype=int)[indices]\n",
    "        task2idx = dict(base.task2idx)\n",
    "    else:\n",
    "        acts, tasks_raw = [], []\n",
    "        for i in range(len(train_ds)):\n",
    "            item = train_ds[i]\n",
    "            acts.append(int(item[\"action\"]))\n",
    "            tasks_raw.append(int(item[\"y_task_raw\"]))\n",
    "        y_action = np.array(acts, dtype=int)\n",
    "        y_task_raw = np.array(tasks_raw, dtype=int)\n",
    "        uniq = sorted(np.unique(y_task_raw))\n",
    "        task2idx = {t: i for i, t in enumerate(uniq)}\n",
    "    return y_action, y_task_raw, task2idx\n",
    "\n",
    "\n",
    "# ---------- NEW: class-weighted action loss ----------\n",
    "def build_action_criterion(train_ds) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Build a class-weighted CrossEntropy loss for the binary action head\n",
    "    (REST=0, ACTION=1), based on the action label distribution in train_ds.\n",
    "    \"\"\"\n",
    "    y_action, _, _ = _collect_labels_from_dataset(train_ds)   # y_action ∈ {0,1}\n",
    "    class_counts = np.bincount(y_action, minlength=2)         # [count_0, count_1]\n",
    "    # Inverse-frequency weighting\n",
    "    weights = len(y_action) / (2.0 * (class_counts + 1e-6))\n",
    "    print(\"[action weights]\", weights)\n",
    "\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32, device=CFG.DEVICE)\n",
    "    return nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- NEW: class-weighted TASK loss ----------\n",
    "def build_task_criterion(train_ds,\n",
    "                         task2idx: Dict[int, int],\n",
    "                         num_task_classes: int) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Build a class-weighted CrossEntropy loss for the task head.\n",
    "\n",
    "    We compute the label distribution over ACTION windows only\n",
    "    (y_action == 1), because the task head is trained only on those\n",
    "    samples in train_one_fold().\n",
    "    \"\"\"\n",
    "    # Get raw labels from the dataset\n",
    "    y_action, y_task_raw, _ = _collect_labels_from_dataset(train_ds)\n",
    "\n",
    "    # Use only ACTION windows for task weighting\n",
    "    mask_action = (y_action == 1)\n",
    "    y_task_raw_action = y_task_raw[mask_action]\n",
    "\n",
    "    if y_task_raw_action.size == 0:\n",
    "        print(\"[task weights] No ACTION windows in train_ds; using unweighted CE.\")\n",
    "        return nn.CrossEntropyLoss()\n",
    "\n",
    "    # Map raw task codes (e.g., 0,1,2,3,4,5) to indices [0..num_task_classes-1]\n",
    "    y_task_idx = np.array(\n",
    "        [task2idx[int(t)] for t in y_task_raw_action],\n",
    "        dtype=int,\n",
    "    )\n",
    "\n",
    "    # Count per-class frequency\n",
    "    class_counts = np.bincount(y_task_idx, minlength=num_task_classes)\n",
    "\n",
    "    # Inverse-frequency weighting\n",
    "    weights = len(y_task_idx) / (num_task_classes * (class_counts + 1e-6))\n",
    "    print(\"[task weights]\", weights)\n",
    "\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32, device=CFG.DEVICE)\n",
    "    return nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "# ---------------- ACTION THRESHOLD TUNING ----------------\n",
    "\n",
    "def tune_action_threshold(\n",
    "    model: TriModalSafetyTransformer,\n",
    "    val_loader: DataLoader,\n",
    "    use_eeg: bool,\n",
    "    use_emg: bool,\n",
    "    use_et: bool,\n",
    "    device: str,\n",
    "    num_thresholds: int = 17,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_y = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x_eeg = batch[\"eeg\"].to(device)\n",
    "            x_emg = batch[\"emg\"].to(device)\n",
    "            x_et  = batch[\"et\"].to(device)\n",
    "            y_action = batch[\"action\"].to(device)\n",
    "\n",
    "            eeg_psd = batch.get(\"eeg_psd\", None)\n",
    "            if eeg_psd is not None:\n",
    "                eeg_psd = eeg_psd.to(device)\n",
    "            emg_feat = batch.get(\"emg_feat\", None)\n",
    "            if emg_feat is not None:\n",
    "                emg_feat = emg_feat.to(device)\n",
    "\n",
    "            # Ablation-level modality usage\n",
    "            if not use_eeg:\n",
    "                x_eeg = torch.zeros_like(x_eeg)\n",
    "                if eeg_psd is not None:\n",
    "                    eeg_psd = torch.zeros_like(eeg_psd)\n",
    "            if not use_emg:\n",
    "                x_emg = torch.zeros_like(x_emg)\n",
    "                if emg_feat is not None:\n",
    "                    emg_feat = torch.zeros_like(emg_feat)\n",
    "            if not use_et:\n",
    "                x_et  = torch.zeros_like(x_et)\n",
    "\n",
    "            logits_action, _, _ = model.forward_supervised(\n",
    "                x_eeg, x_emg, x_et,\n",
    "                eeg_psd=eeg_psd,\n",
    "                emg_feat=emg_feat,\n",
    "            )\n",
    "            probs_action = F.softmax(logits_action, dim=1)[:, 1]\n",
    "\n",
    "            all_probs.append(probs_action.cpu().numpy())\n",
    "            all_y.append(y_action.cpu().numpy())\n",
    "\n",
    "    if not all_probs:\n",
    "        return 0.5\n",
    "\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    all_y = np.concatenate(all_y)\n",
    "\n",
    "    thresholds = np.linspace(0.1, 0.9, num_thresholds)\n",
    "    best_t = 0.5\n",
    "    best_bal = 0.0\n",
    "\n",
    "    for t in thresholds:\n",
    "        preds = (all_probs >= t).astype(int)\n",
    "        bal = balanced_accuracy_score(all_y, preds)\n",
    "        if bal > best_bal:\n",
    "            best_bal = bal\n",
    "            best_t = float(t)\n",
    "\n",
    "    print(f\"[tune_action_threshold] best τ={best_t:.3f}, val_bal_acc={best_bal:.3f}\")\n",
    "    return best_t\n",
    "\n",
    "\n",
    "# ---------------- POLICY EVALUATION (S0–S3, P0–P2) ----------------\n",
    "# (unchanged from your version; omitted comments to keep length manageable)\n",
    "\n",
    "def evaluate_policies_for_scenarios(\n",
    "    model: TriModalSafetyTransformer,\n",
    "    test_loader: DataLoader,\n",
    "    base_threshold: float,\n",
    "    num_task_classes: int,\n",
    "    use_eeg: bool,\n",
    "    use_emg: bool,\n",
    "    use_et: bool,\n",
    ") -> Dict:\n",
    "    model.eval()\n",
    "    rest_idx = 0\n",
    "\n",
    "    scenario_results = {}\n",
    "\n",
    "    for scen_name, scen_cfg in SCENARIOS.items():\n",
    "        drop_eeg = scen_cfg[\"drop_eeg\"]\n",
    "        drop_emg = scen_cfg[\"drop_emg\"]\n",
    "        drop_et  = scen_cfg[\"drop_et\"]\n",
    "\n",
    "        all_gt_action = []\n",
    "        all_gt_task   = []\n",
    "        all_p_act     = []\n",
    "        all_logits_task = []\n",
    "        all_gates_mean = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x_eeg = batch[\"eeg\"].to(CFG.DEVICE)\n",
    "                x_emg = batch[\"emg\"].to(CFG.DEVICE)\n",
    "                x_et  = batch[\"et\"].to(CFG.DEVICE)\n",
    "                y_action = batch[\"action\"].to(CFG.DEVICE)\n",
    "                y_task   = batch[\"task\"].to(CFG.DEVICE)\n",
    "\n",
    "                eeg_psd = batch.get(\"eeg_psd\", None)\n",
    "                if eeg_psd is not None:\n",
    "                    eeg_psd = eeg_psd.to(CFG.DEVICE)\n",
    "                emg_feat = batch.get(\"emg_feat\", None)\n",
    "                if emg_feat is not None:\n",
    "                    emg_feat = emg_feat.to(CFG.DEVICE)\n",
    "\n",
    "                # Ablation + scenario-level drops\n",
    "                if (not use_eeg) or drop_eeg:\n",
    "                    x_eeg = torch.zeros_like(x_eeg)\n",
    "                    if eeg_psd is not None:\n",
    "                        eeg_psd = torch.zeros_like(eeg_psd)\n",
    "                if (not use_emg) or drop_emg:\n",
    "                    x_emg = torch.zeros_like(x_emg)\n",
    "                    if emg_feat is not None:\n",
    "                        emg_feat = torch.zeros_like(emg_feat)\n",
    "                if (not use_et) or drop_et:\n",
    "                    x_et  = torch.zeros_like(x_et)\n",
    "\n",
    "                logits_action, logits_task, aux = model.forward_supervised(\n",
    "                    x_eeg, x_emg, x_et,\n",
    "                    eeg_psd=eeg_psd,\n",
    "                    emg_feat=emg_feat,\n",
    "                )\n",
    "\n",
    "                probs_action = F.softmax(logits_action, dim=1)[:, 1]\n",
    "                gates = aux[\"gates\"]  # (B,3,D)\n",
    "                g_mean = gates.mean(dim=-1)  # (B,3)\n",
    "\n",
    "                all_gt_action.append(y_action.cpu().numpy())\n",
    "                all_gt_task.append(y_task.cpu().numpy())\n",
    "                all_p_act.append(probs_action.cpu().numpy())\n",
    "                all_logits_task.append(logits_task.cpu().numpy())\n",
    "                all_gates_mean.append(g_mean.cpu().numpy())\n",
    "\n",
    "        if not all_gt_action:\n",
    "            continue\n",
    "\n",
    "        gt_action = np.concatenate(all_gt_action)\n",
    "        gt_task   = np.concatenate(all_gt_task)\n",
    "        p_act     = np.concatenate(all_p_act)\n",
    "        logits_task = np.concatenate(all_logits_task)\n",
    "        gates_mean  = np.concatenate(all_gates_mean)  # (N,3)\n",
    "\n",
    "        scen_mask = np.array([\n",
    "            0.0 if (drop_eeg or not use_eeg) else 1.0,\n",
    "            0.0 if (drop_emg or not use_emg) else 1.0,\n",
    "            0.0 if (drop_et  or not use_et)  else 1.0,\n",
    "        ], dtype=np.float32)\n",
    "        healthy_modalities = scen_mask.sum()\n",
    "        if healthy_modalities <= 0:\n",
    "            print(f\"[WARN] Scenario {scen_name}: no healthy modalities, skipping.\")\n",
    "            continue\n",
    "\n",
    "        g_healthy = (gates_mean * scen_mask[None, :]).sum(axis=1)  # (N,)\n",
    "        r_reliab = g_healthy\n",
    "\n",
    "        p_task = torch.softmax(torch.from_numpy(logits_task), dim=1).numpy()\n",
    "        eps = 1e-8\n",
    "        H_task = -(p_task * np.log(p_task + eps)).sum(axis=1)\n",
    "        H_task_norm = H_task / (math.log(num_task_classes) + eps)\n",
    "\n",
    "        gates_prob = gates_mean / (gates_mean.sum(axis=1, keepdims=True) + eps)\n",
    "        H_gate = -(gates_prob * np.log(gates_prob + eps)).sum(axis=1)\n",
    "        H_gate_norm = H_gate / (math.log(3.0) + eps)\n",
    "\n",
    "        gmue = float(H_gate.mean())\n",
    "        gate_mean = gates_prob.mean(axis=0)\n",
    "        gate_std  = gates_prob.std(axis=0)\n",
    "\n",
    "        policy_entries = {}\n",
    "\n",
    "        def compute_policy_metrics(do_move: np.ndarray,\n",
    "                                   pred_task_idx: np.ndarray,\n",
    "                                   score_vec: np.ndarray):\n",
    "            gt_a = gt_action\n",
    "            gt_t = gt_task\n",
    "\n",
    "            total = len(gt_a)\n",
    "            total_action = int((gt_a == 1).sum())\n",
    "            total_rest   = int((gt_a == 0).sum())\n",
    "\n",
    "            correct_move = (\n",
    "                (do_move == 1)\n",
    "                & (gt_a == 1)\n",
    "                & (pred_task_idx == gt_t)\n",
    "            )\n",
    "            unsafe_move = (\n",
    "                (do_move == 1)\n",
    "                & (\n",
    "                    (gt_a == 0)\n",
    "                    | ((gt_a == 1) & (pred_task_idx != gt_t))\n",
    "                )\n",
    "            )\n",
    "            missed_intent = ((do_move == 0) & (gt_a == 1))\n",
    "            safe_idle = ((do_move == 0) & (gt_a == 0))\n",
    "\n",
    "            n_correct = int(correct_move.sum())\n",
    "            n_unsafe  = int(unsafe_move.sum())\n",
    "            n_missed  = int(missed_intent.sum())\n",
    "            n_safeidle = int(safe_idle.sum())\n",
    "\n",
    "            denom_actions = max(1, n_correct + n_unsafe)\n",
    "            SAE = n_unsafe / denom_actions\n",
    "\n",
    "            denom_intent = max(1, total_action)\n",
    "            UAR = n_correct / denom_intent\n",
    "            MIR = n_missed / denom_intent\n",
    "\n",
    "            denom_rest = max(1, total_rest)\n",
    "            NRA = n_safeidle / denom_rest\n",
    "\n",
    "            action_pred = do_move.astype(int)\n",
    "            acc_action = accuracy_score(gt_a, action_pred)\n",
    "            f1_action_macro = f1_score(gt_a, action_pred, average=\"macro\")\n",
    "            f1_action_micro = f1_score(gt_a, action_pred, average=\"micro\")\n",
    "            bal_action = balanced_accuracy_score(gt_a, action_pred)\n",
    "            try:\n",
    "                auc_roc = roc_auc_score(gt_a, score_vec)\n",
    "            except Exception:\n",
    "                auc_roc = None\n",
    "            try:\n",
    "                auprc = average_precision_score(gt_a, score_vec)\n",
    "            except Exception:\n",
    "                auprc = None\n",
    "\n",
    "            cm_action = confusion_matrix(gt_a, action_pred, labels=[0, 1])\n",
    "            per_class_action = compute_per_class_metrics_from_cm(cm_action)\n",
    "\n",
    "            acc_task = accuracy_score(gt_t, pred_task_idx)\n",
    "            f1_task_macro = f1_score(gt_t, pred_task_idx, average=\"macro\")\n",
    "            f1_task_micro = f1_score(gt_t, pred_task_idx, average=\"micro\")\n",
    "            bal_task = balanced_accuracy_score(gt_t, pred_task_idx)\n",
    "            cm_task = confusion_matrix(gt_t, pred_task_idx, labels=list(range(num_task_classes)))\n",
    "            per_class_task = compute_per_class_metrics_from_cm(cm_task)\n",
    "\n",
    "            task_topk = compute_topk_accuracies_from_logits(\n",
    "                logits_task, gt_t, ks=CFG.TOPK\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"classic\": {\n",
    "                    \"action_acc\": float(acc_action),\n",
    "                    \"action_macro_f1\": float(f1_action_macro),\n",
    "                    \"action_micro_f1\": float(f1_action_micro),\n",
    "                    \"action_bal_acc\": float(bal_action),\n",
    "                    \"action_auc_roc\": float(auc_roc) if auc_roc is not None else None,\n",
    "                    \"action_auprc\": float(auprc) if auprc is not None else None,\n",
    "                    \"cm_action\": cm_action.tolist(),\n",
    "                    \"per_class_action\": per_class_action,\n",
    "                    \"task_acc\": float(acc_task),\n",
    "                    \"task_macro_f1\": float(f1_task_macro),\n",
    "                    \"task_micro_f1\": float(f1_task_micro),\n",
    "                    \"task_bal_acc\": float(bal_task),\n",
    "                    \"cm_task\": cm_task.tolist(),\n",
    "                    \"per_class_task\": per_class_task,\n",
    "                    \"task_topk\": {\n",
    "                        f\"top_{k}\": float(task_topk.get(k, 0.0)) for k in CFG.TOPK\n",
    "                    },\n",
    "                },\n",
    "                \"safety\": {\n",
    "                    \"SAE\": float(SAE),\n",
    "                    \"UAR\": float(UAR),\n",
    "                    \"MIR\": float(MIR),\n",
    "                    \"NRA\": float(NRA),\n",
    "                    \"counts\": {\n",
    "                        \"total\": int(total),\n",
    "                        \"total_action\": int(total_action),\n",
    "                        \"total_rest\": int(total_rest),\n",
    "                        \"correct_move\": n_correct,\n",
    "                        \"unsafe_move\": n_unsafe,\n",
    "                        \"missed_intent\": n_missed,\n",
    "                        \"safe_idle\": n_safeidle,\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "\n",
    "        tau0 = base_threshold\n",
    "        p_eff0 = p_act.copy()\n",
    "        do_move0 = (p_eff0 >= tau0).astype(int)\n",
    "\n",
    "        logits_task_mod0 = logits_task.copy()\n",
    "        move_idx0 = (do_move0 == 1)\n",
    "        logits_task_mod0[move_idx0, rest_idx] = -1e9\n",
    "        pred_task0 = logits_task_mod0.argmax(axis=1)\n",
    "        pred_task0[do_move0 == 0] = rest_idx\n",
    "\n",
    "        policy_entries[\"P0\"] = compute_policy_metrics(\n",
    "            do_move0, pred_task0, p_eff0\n",
    "        )\n",
    "\n",
    "        p_eff1 = p_act * r_reliab\n",
    "        tau1 = base_threshold\n",
    "        do_move1 = (p_eff1 >= tau1).astype(int)\n",
    "\n",
    "        logits_task_mod1 = logits_task.copy()\n",
    "        move_idx1 = (do_move1 == 1)\n",
    "        logits_task_mod1[move_idx1, rest_idx] = -1e9\n",
    "        pred_task1 = logits_task_mod1.argmax(axis=1)\n",
    "        pred_task1[do_move1 == 0] = rest_idx\n",
    "\n",
    "        policy_entries[\"P1\"] = compute_policy_metrics(\n",
    "            do_move1, pred_task1, p_eff1\n",
    "        )\n",
    "\n",
    "        unc = 0.5 * (H_task_norm + H_gate_norm)\n",
    "        safety_factor = r_reliab * (1.0 - unc)\n",
    "        p_eff2 = p_act * safety_factor\n",
    "        tau2 = base_threshold\n",
    "        do_move2 = (p_eff2 >= tau2).astype(int)\n",
    "\n",
    "        logits_task_mod2 = logits_task.copy()\n",
    "        move_idx2 = (do_move2 == 1)\n",
    "        logits_task_mod2[move_idx2, rest_idx] = -1e9\n",
    "        pred_task2 = logits_task_mod2.argmax(axis=1)\n",
    "        pred_task2[do_move2 == 0] = rest_idx\n",
    "\n",
    "        policy_entries[\"P2\"] = compute_policy_metrics(\n",
    "            do_move2, pred_task2, p_eff2\n",
    "        )\n",
    "\n",
    "        SAE_curve = []\n",
    "        MIR_curve = []\n",
    "        thresholds = CFG.P2_THRESHOLDS\n",
    "        for t in thresholds:\n",
    "            do_move_t = (p_eff2 >= t).astype(int)\n",
    "            logits_task_mod_t = logits_task.copy()\n",
    "            move_idx_t = (do_move_t == 1)\n",
    "            logits_task_mod_t[move_idx_t, rest_idx] = -1e9\n",
    "            pred_task_t = logits_task_mod_t.argmax(axis=1)\n",
    "            pred_task_t[do_move_t == 0] = rest_idx\n",
    "\n",
    "            metrics_t = compute_policy_metrics(\n",
    "                do_move_t, pred_task_t, p_eff2\n",
    "            )[\"safety\"]\n",
    "            SAE_curve.append(metrics_t[\"SAE\"])\n",
    "            MIR_curve.append(metrics_t[\"MIR\"])\n",
    "\n",
    "        scenario_results[scen_name] = {\n",
    "            \"description\": scen_cfg[\"description\"],\n",
    "            \"gmue\": gmue,\n",
    "            \"gates_mean\": gate_mean.tolist(),\n",
    "            \"gates_std\": gate_std.tolist(),\n",
    "            \"policies\": policy_entries,\n",
    "            \"P2_curve\": {\n",
    "                \"thresholds\": thresholds,\n",
    "                \"SAE\": SAE_curve,\n",
    "                \"MIR\": MIR_curve,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"base_threshold\": base_threshold,\n",
    "        \"scenarios\": scenario_results,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- STAGE 2 — SUPERVISED LOSO TRAINING ----------------\n",
    "\n",
    "def train_one_fold(\n",
    "    fold_id: int,\n",
    "    base_state: Optional[Dict[str, torch.Tensor]],\n",
    "    eeg_ch: int,\n",
    "    emg_ch: int,\n",
    "    et_ch: int,\n",
    "    ablation_name: str,\n",
    "    use_eeg: bool,\n",
    "    use_emg: bool,\n",
    "    use_et: bool,\n",
    ") -> Dict:\n",
    "\n",
    "    print(\n",
    "        f\"\\n================ STAGE 2 — Supervised LOSO (FOLD {fold_id}, \"\n",
    "        f\"ablation={ablation_name}) ================\"\n",
    "    )\n",
    "\n",
    "    train_ds, val_ds, test_ds = make_supervised_datasets_for_fold(fold_id)\n",
    "    num_task_classes = train_ds.num_task_classes\n",
    "\n",
    "    print(\n",
    "        f\"[Fold {fold_id}][{ablation_name}] num_task_classes={num_task_classes}, \"\n",
    "        f\"tasks={sorted(train_ds.task2idx.keys())}\"\n",
    "    )\n",
    "\n",
    "    eeg_psd_dim = getattr(train_ds, \"eeg_psd_dim\", 0)\n",
    "    emg_feat_dim = getattr(train_ds, \"emg_feat_dim\", 0)\n",
    "\n",
    "    use_eeg_psd = (eeg_psd_dim > 0) and CFG.USE_EEG_PSD_FEATURES and use_eeg\n",
    "    use_emg_feat = (emg_feat_dim > 0) and CFG.USE_EMG_FEATURES and use_emg\n",
    "\n",
    "    # ---- NEW: fold-wise normalization stats for Phase 5.5 features ----\n",
    "    eeg_psd_mean_t = eeg_psd_std_t = None\n",
    "    emg_feat_mean_t = emg_feat_std_t = None\n",
    "\n",
    "    if use_eeg_psd:\n",
    "        Xp = train_ds.X_eeg_psd.astype(np.float32)   # (N_train, F_psd)\n",
    "        eeg_psd_mean = Xp.mean(axis=0)\n",
    "        eeg_psd_std  = Xp.std(axis=0) + 1e-6\n",
    "        eeg_psd_mean_t = torch.from_numpy(eeg_psd_mean).float().to(CFG.DEVICE)\n",
    "        eeg_psd_std_t  = torch.from_numpy(eeg_psd_std).float().to(CFG.DEVICE)\n",
    "\n",
    "    if use_emg_feat:\n",
    "        Xe = train_ds.X_emg_feat.astype(np.float32)  # (N_train, F_emg)\n",
    "        emg_feat_mean = Xe.mean(axis=0)\n",
    "        emg_feat_std  = Xe.std(axis=0) + 1e-6\n",
    "        emg_feat_mean_t = torch.from_numpy(emg_feat_mean).float().to(CFG.DEVICE)\n",
    "        emg_feat_std_t  = torch.from_numpy(emg_feat_std).float().to(CFG.DEVICE)\n",
    "\n",
    "    if use_eeg_psd or use_emg_feat:\n",
    "        print(\n",
    "            f\"[Fold {fold_id}][{ablation_name}] Using features: \"\n",
    "            f\"EEG_PSD_dim={eeg_psd_dim if use_eeg_psd else 0}, \"\n",
    "            f\"EMG_feat_dim={emg_feat_dim if use_emg_feat else 0}\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"[Fold {fold_id}][{ablation_name}] No feature vectors used.\")\n",
    "\n",
    "    model = TriModalSafetyTransformer(\n",
    "        eeg_ch=eeg_ch,\n",
    "        emg_ch=emg_ch,\n",
    "        et_ch=et_ch,\n",
    "        num_task_classes=num_task_classes,\n",
    "        d_model=CFG.D_MODEL,\n",
    "        dropout=CFG.DROPOUT,\n",
    "        use_eeg_psd=use_eeg_psd,\n",
    "        use_emg_feat=use_emg_feat,\n",
    "        eeg_psd_dim=eeg_psd_dim,\n",
    "        emg_feat_dim=emg_feat_dim,\n",
    "        eeg_psd_mean=eeg_psd_mean_t,\n",
    "        eeg_psd_std=eeg_psd_std_t,\n",
    "        emg_feat_mean=emg_feat_mean_t,\n",
    "        emg_feat_std=emg_feat_std_t,\n",
    "    ).to(CFG.DEVICE)\n",
    "\n",
    "    if CFG.USE_SSL and base_state is not None:\n",
    "        missing, unexpected = model.load_state_dict(base_state, strict=False)\n",
    "        print(\n",
    "            f\"[Fold {fold_id}][{ablation_name}] Loaded SSL backbone. \"\n",
    "            f\"Missing={len(missing)}, Unexpected={len(unexpected)}\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"[Fold {fold_id}][{ablation_name}] Random init (no SSL).\")\n",
    "\n",
    "\n",
    "    train_loader = make_dataloader(train_ds, batch_size=CFG.SUP_BATCH, shuffle=True)\n",
    "    val_loader   = make_dataloader(val_ds,   batch_size=CFG.SUP_BATCH, shuffle=False)\n",
    "    test_loader  = make_dataloader(test_ds,  batch_size=CFG.SUP_BATCH, shuffle=False)\n",
    "\n",
    "    crit_action = build_action_criterion(train_ds)\n",
    "    crit_task   = build_task_criterion(train_ds, task2idx=train_ds.task2idx,\n",
    "                                       num_task_classes=num_task_classes)\n",
    "\n",
    "    head_params = []\n",
    "    backbone_params = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if name.startswith(\"action_head.\") or name.startswith(\"task_head.\"):\n",
    "            head_params.append(p)\n",
    "        else:\n",
    "            backbone_params.append(p)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [\n",
    "            {\"params\": backbone_params, \"lr\": CFG.SUP_LR * CFG.BACKBONE_LR_SCALE},\n",
    "            {\"params\": head_params,     \"lr\": CFG.SUP_LR},\n",
    "        ],\n",
    "        weight_decay=CFG.WEIGHT_DECAY,\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    # ---------------- TRAINING LOOP ----------------\n",
    "    for epoch in range(1, CFG.SUP_EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        total_action_correct = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            x_eeg = batch[\"eeg\"].to(CFG.DEVICE)\n",
    "            x_emg = batch[\"emg\"].to(CFG.DEVICE)\n",
    "            x_et  = batch[\"et\"].to(CFG.DEVICE)\n",
    "            y_action = batch[\"action\"].to(CFG.DEVICE)\n",
    "            y_task   = batch[\"task\"].to(CFG.DEVICE)\n",
    "\n",
    "            eeg_psd = batch.get(\"eeg_psd\", None)\n",
    "            if eeg_psd is not None:\n",
    "                eeg_psd = eeg_psd.to(CFG.DEVICE)\n",
    "            emg_feat = batch.get(\"emg_feat\", None)\n",
    "            if emg_feat is not None:\n",
    "                emg_feat = emg_feat.to(CFG.DEVICE)\n",
    "\n",
    "            # Optional ET dropout during supervised training\n",
    "            if use_et and CFG.SUP_ET_DROPOUT > 0.0:\n",
    "                x_et = apply_modality_dropout(x_et, CFG.SUP_ET_DROPOUT)\n",
    "\n",
    "            if not use_eeg:\n",
    "                x_eeg = torch.zeros_like(x_eeg)\n",
    "                if eeg_psd is not None:\n",
    "                    eeg_psd = torch.zeros_like(eeg_psd)\n",
    "            if not use_emg:\n",
    "                x_emg = torch.zeros_like(x_emg)\n",
    "                if emg_feat is not None:\n",
    "                    emg_feat = torch.zeros_like(emg_feat)\n",
    "            if not use_et:\n",
    "                x_et  = torch.zeros_like(x_et)\n",
    "\n",
    "            logits_action, logits_task, _ = model.forward_supervised(\n",
    "                x_eeg, x_emg, x_et,\n",
    "                eeg_psd=eeg_psd,\n",
    "                emg_feat=emg_feat,\n",
    "            )\n",
    "\n",
    "            loss_action = crit_action(logits_action, y_action)\n",
    "            action_mask = (y_action == 1)\n",
    "            if action_mask.any():\n",
    "                loss_task = crit_task(logits_task[action_mask], y_task[action_mask])\n",
    "            else:\n",
    "                loss_task = torch.tensor(0.0, device=CFG.DEVICE)\n",
    "\n",
    "            loss = CFG.ALPHA_ACTION * loss_action + CFG.BETA_TASK * loss_task\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            B = y_action.size(0)\n",
    "            total_loss += float(loss.item()) * B\n",
    "            total_samples += B\n",
    "\n",
    "            preds_action = logits_action.argmax(dim=1)\n",
    "            total_action_correct += (preds_action == y_action).sum().item()\n",
    "\n",
    "        train_loss = total_loss / max(1, total_samples)\n",
    "        train_action_acc = total_action_correct / max(1, total_samples)\n",
    "\n",
    "        # ---------------- VALIDATION ----------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_samples = 0\n",
    "        val_action_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_eeg = batch[\"eeg\"].to(CFG.DEVICE)\n",
    "                x_emg = batch[\"emg\"].to(CFG.DEVICE)\n",
    "                x_et  = batch[\"et\"].to(CFG.DEVICE)\n",
    "                y_action = batch[\"action\"].to(CFG.DEVICE)\n",
    "                y_task   = batch[\"task\"].to(CFG.DEVICE)\n",
    "\n",
    "                eeg_psd = batch.get(\"eeg_psd\", None)\n",
    "                if eeg_psd is not None:\n",
    "                    eeg_psd = eeg_psd.to(CFG.DEVICE)\n",
    "                emg_feat = batch.get(\"emg_feat\", None)\n",
    "                if emg_feat is not None:\n",
    "                    emg_feat = emg_feat.to(CFG.DEVICE)\n",
    "\n",
    "                if not use_eeg:\n",
    "                    x_eeg = torch.zeros_like(x_eeg)\n",
    "                    if eeg_psd is not None:\n",
    "                        eeg_psd = torch.zeros_like(eeg_psd)\n",
    "                if not use_emg:\n",
    "                    x_emg = torch.zeros_like(x_emg)\n",
    "                    if emg_feat is not None:\n",
    "                        emg_feat = torch.zeros_like(emg_feat)\n",
    "                if not use_et:\n",
    "                    x_et  = torch.zeros_like(x_et)\n",
    "\n",
    "                logits_action, logits_task, _ = model.forward_supervised(\n",
    "                    x_eeg, x_emg, x_et,\n",
    "                    eeg_psd=eeg_psd,\n",
    "                    emg_feat=emg_feat,\n",
    "                )\n",
    "\n",
    "                loss_action = crit_action(logits_action, y_action)\n",
    "                action_mask = (y_action == 1)\n",
    "                if action_mask.any():\n",
    "                    loss_task = crit_task(logits_task[action_mask], y_task[action_mask])\n",
    "                else:\n",
    "                    loss_task = torch.tensor(0.0, device=CFG.DEVICE)\n",
    "\n",
    "                loss = CFG.ALPHA_ACTION * loss_action + CFG.BETA_TASK * loss_task\n",
    "\n",
    "                B = y_action.size(0)\n",
    "                val_loss += float(loss.item()) * B\n",
    "                val_samples += B\n",
    "\n",
    "                preds_action = logits_action.argmax(dim=1)\n",
    "                val_action_correct += (preds_action == y_action).sum().item()\n",
    "\n",
    "        val_loss /= max(1, val_samples)\n",
    "        val_action_acc = val_action_correct / max(1, val_samples)\n",
    "\n",
    "        print(\n",
    "            f\"[Fold {fold_id}][{ablation_name}] Epoch {epoch:02d}/{CFG.SUP_EPOCHS} \"\n",
    "            f\"train_loss={train_loss:.4f} train_acc={train_action_acc:.3f} \"\n",
    "            f\"val_loss={val_loss:.4f} val_acc={val_action_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "        if val_loss < best_val_loss - 1e-4:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= CFG.PATIENCE:\n",
    "                print(f\"[Fold {fold_id}][{ablation_name}] Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    base_threshold = tune_action_threshold(\n",
    "        model, val_loader,\n",
    "        use_eeg=use_eeg, use_emg=use_emg, use_et=use_et,\n",
    "        device=CFG.DEVICE,\n",
    "    )\n",
    "\n",
    "    # ---------------- TEST EVALUATION (S0, P0 ONLY FOR CLASSIC METRICS) ----------------\n",
    "    model.eval()\n",
    "    all_action_true, all_action_pred = [], []\n",
    "    all_action_scores = []\n",
    "    all_task_true, all_task_pred = [], []\n",
    "    all_task_logits = []\n",
    "\n",
    "    rest_idx = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x_eeg = batch[\"eeg\"].to(CFG.DEVICE)\n",
    "            x_emg = batch[\"emg\"].to(CFG.DEVICE)\n",
    "            x_et  = batch[\"et\"].to(CFG.DEVICE)\n",
    "            y_action = batch[\"action\"].to(CFG.DEVICE)\n",
    "            y_task   = batch[\"task\"].to(CFG.DEVICE)\n",
    "\n",
    "            eeg_psd = batch.get(\"eeg_psd\", None)\n",
    "            if eeg_psd is not None:\n",
    "                eeg_psd = eeg_psd.to(CFG.DEVICE)\n",
    "            emg_feat = batch.get(\"emg_feat\", None)\n",
    "            if emg_feat is not None:\n",
    "                emg_feat = emg_feat.to(CFG.DEVICE)\n",
    "\n",
    "            if not use_eeg:\n",
    "                x_eeg = torch.zeros_like(x_eeg)\n",
    "                if eeg_psd is not None:\n",
    "                    eeg_psd = torch.zeros_like(eeg_psd)\n",
    "            if not use_emg:\n",
    "                x_emg = torch.zeros_like(x_emg)\n",
    "                if emg_feat is not None:\n",
    "                    emg_feat = torch.zeros_like(emg_feat)\n",
    "            if not use_et:\n",
    "                x_et  = torch.zeros_like(x_et)\n",
    "\n",
    "            logits_action, logits_task, _ = model.forward_supervised(\n",
    "                x_eeg, x_emg, x_et,\n",
    "                eeg_psd=eeg_psd,\n",
    "                emg_feat=emg_feat,\n",
    "            )\n",
    "\n",
    "            probs_action = F.softmax(logits_action, dim=1)[:, 1]\n",
    "            preds_action = (probs_action >= base_threshold).long()\n",
    "\n",
    "            logits_task_mod = logits_task.clone()\n",
    "            move_idx = (preds_action == 1)\n",
    "            logits_task_mod[move_idx, rest_idx] = -1e9\n",
    "            preds_task = logits_task_mod.argmax(dim=1)\n",
    "            preds_task[preds_action == 0] = rest_idx\n",
    "\n",
    "            all_action_true.append(y_action.cpu().numpy())\n",
    "            all_action_pred.append(preds_action.cpu().numpy())\n",
    "            all_action_scores.append(probs_action.cpu().numpy())\n",
    "            all_task_true.append(y_task.cpu().numpy())\n",
    "            all_task_pred.append(preds_task.cpu().numpy())\n",
    "            all_task_logits.append(logits_task_mod.cpu().numpy())\n",
    "\n",
    "    all_action_true = np.concatenate(all_action_true)\n",
    "    all_action_pred = np.concatenate(all_action_pred)\n",
    "    all_action_scores = np.concatenate(all_action_scores)\n",
    "    all_task_true = np.concatenate(all_task_true)\n",
    "    all_task_pred = np.concatenate(all_task_pred)\n",
    "    all_task_logits = np.concatenate(all_task_logits)\n",
    "\n",
    "    acc_action = accuracy_score(all_action_true, all_action_pred)\n",
    "    f1_action_macro = f1_score(all_action_true, all_action_pred, average=\"macro\")\n",
    "    f1_action_micro = f1_score(all_action_true, all_action_pred, average=\"micro\")\n",
    "    bal_action = balanced_accuracy_score(all_action_true, all_action_pred)\n",
    "    try:\n",
    "        auc_roc = roc_auc_score(all_action_true, all_action_scores)\n",
    "    except Exception:\n",
    "        auc_roc = None\n",
    "    try:\n",
    "        auprc = average_precision_score(all_action_true, all_action_scores)\n",
    "    except Exception:\n",
    "        auprc = None\n",
    "    cm_action = confusion_matrix(all_action_true, all_action_pred, labels=[0, 1])\n",
    "    per_class_action = compute_per_class_metrics_from_cm(cm_action)\n",
    "\n",
    "    print(\n",
    "        f\"[Fold {fold_id}][{ablation_name}] TEST P0/S0 — Action: \"\n",
    "        f\"acc={acc_action:.3f}, macro-F1={f1_action_macro:.3f}, \"\n",
    "        f\"micro-F1={f1_action_micro:.3f}, bal-acc={bal_action:.3f}\"\n",
    "    )\n",
    "\n",
    "    acc_task = accuracy_score(all_task_true, all_task_pred)\n",
    "    f1_task_macro = f1_score(all_task_true, all_task_pred, average=\"macro\")\n",
    "    f1_task_micro = f1_score(all_task_true, all_task_pred, average=\"micro\")\n",
    "    bal_task = balanced_accuracy_score(all_task_true, all_task_pred)\n",
    "    cm_task = confusion_matrix(all_task_true, all_task_pred,\n",
    "                               labels=list(range(num_task_classes)))\n",
    "    per_class_task = compute_per_class_metrics_from_cm(cm_task)\n",
    "    task_topk = compute_topk_accuracies_from_logits(\n",
    "        all_task_logits, all_task_true, ks=CFG.TOPK\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"[Fold {fold_id}][{ablation_name}] TEST P0/S0 — Task: \"\n",
    "        f\"acc={acc_task:.3f}, macro-F1={f1_task_macro:.3f}, \"\n",
    "        f\"micro-F1={f1_task_micro:.3f}, bal-acc={bal_task:.3f}\"\n",
    "    )\n",
    "\n",
    "    print(f\"[Fold {fold_id}][{ablation_name}] Per-class ACTION metrics (P0/S0):\")\n",
    "    for cls_idx, m in sorted(per_class_action.items()):\n",
    "        label = \"REST\" if cls_idx == 0 else \"ACTION\"\n",
    "        print(\n",
    "            f\"  class {cls_idx} ({label}): \"\n",
    "            f\"prec={m['precision']:.3f}, rec={m['recall']:.3f}, \"\n",
    "            f\"f1={m['f1']:.3f}, bal_acc={m['balanced_accuracy']:.3f}, \"\n",
    "            f\"support={m['support']}\"\n",
    "        )\n",
    "\n",
    "    print(f\"[Fold {fold_id}][{ablation_name}] Per-class TASK metrics (P0/S0):\")\n",
    "    idx2task = {idx: code for code, idx in train_ds.task2idx.items()}\n",
    "    for cls_idx, m in sorted(per_class_task.items()):\n",
    "        task_code = idx2task.get(cls_idx, None)\n",
    "        print(\n",
    "            f\"  class {cls_idx} (task_code={task_code}): \"\n",
    "            f\"prec={m['precision']:.3f}, rec={m['recall']:.3f}, \"\n",
    "            f\"f1={m['f1']:.3f}, bal_acc={m['balanced_accuracy']:.3f}, \"\n",
    "            f\"support={m['support']}\"\n",
    "        )\n",
    "\n",
    "    safety_results = None\n",
    "    gmue_fold = None\n",
    "    if ablation_name == \"all\":\n",
    "        safety_results = evaluate_policies_for_scenarios(\n",
    "            model=model,\n",
    "            test_loader=test_loader,\n",
    "            base_threshold=base_threshold,\n",
    "            num_task_classes=num_task_classes,\n",
    "            use_eeg=use_eeg,\n",
    "            use_emg=use_emg,\n",
    "            use_et=use_et,\n",
    "        )\n",
    "        scen0 = safety_results[\"scenarios\"].get(\"S0\", None)\n",
    "        if scen0 is not None:\n",
    "            gmue_fold = scen0.get(\"gmue\", None)\n",
    "\n",
    "    return {\n",
    "        \"fold\": fold_id,\n",
    "        \"ablation\": ablation_name,\n",
    "        \"use_eeg\": use_eeg,\n",
    "        \"use_emg\": use_emg,\n",
    "        \"use_et\": use_et,\n",
    "        \"num_task_classes\": int(num_task_classes),\n",
    "        \"task2idx\": train_ds.task2idx,\n",
    "        \"classic\": {\n",
    "            \"action_acc\": float(acc_action),\n",
    "            \"action_macro_f1\": float(f1_action_macro),\n",
    "            \"action_micro_f1\": float(f1_action_micro),\n",
    "            \"action_bal_acc\": float(bal_action),\n",
    "            \"action_auc_roc\": float(auc_roc) if auc_roc is not None else None,\n",
    "            \"action_auprc\": float(auprc) if auprc is not None else None,\n",
    "            \"cm_action\": cm_action.tolist(),\n",
    "            \"per_class_action\": per_class_action,\n",
    "            \"task_acc\": float(acc_task),\n",
    "            \"task_macro_f1\": float(f1_task_macro),\n",
    "            \"task_micro_f1\": float(f1_task_micro),\n",
    "            \"task_bal_acc\": float(bal_task),\n",
    "            \"cm_task\": cm_task.tolist(),\n",
    "            \"per_class_task\": per_class_task,\n",
    "            \"task_topk\": {\n",
    "                f\"top_{k}\": float(task_topk.get(k, 0.0)) for k in CFG.TOPK\n",
    "            },\n",
    "        },\n",
    "        \"base_threshold\": float(base_threshold),\n",
    "        \"safety\": safety_results,\n",
    "        \"gmue\": gmue_fold,\n",
    "    }\n",
    "\n",
    "#=================================\n",
    "\n",
    "# ---------------- RUNTIME / LATENCY BENCHMARK HELPERS (NEW) ----------------\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    \"\"\"Total trainable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def _measure_latency_one_device(\n",
    "    model: TriModalSafetyTransformer,\n",
    "    batch: Dict[str, torch.Tensor],\n",
    "    device: str,\n",
    "    drop_et: bool,\n",
    "    n_warmup: int = 5,\n",
    "    n_iters: int = 20,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Returns (latency_ms_per_window, control_rate_hz) on a single device.\n",
    "    Uses forward_supervised only (no policy logic), which dominates runtime.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x_eeg = batch[\"eeg\"].to(device)\n",
    "    x_emg = batch[\"emg\"].to(device)\n",
    "    x_et  = batch[\"et\"].to(device)\n",
    "\n",
    "    eeg_psd = batch.get(\"eeg_psd\")\n",
    "    if eeg_psd is not None:\n",
    "        eeg_psd = eeg_psd.to(device)\n",
    "\n",
    "    emg_feat = batch.get(\"emg_feat\")\n",
    "    if emg_feat is not None:\n",
    "        emg_feat = emg_feat.to(device)\n",
    "\n",
    "    if drop_et:\n",
    "        # S3-style runtime (ET failed → zeroed)\n",
    "        x_et = torch.zeros_like(x_et)\n",
    "\n",
    "    B = x_eeg.size(0)\n",
    "    if B == 0:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    def _forward():\n",
    "        with torch.no_grad():\n",
    "            _ = model.forward_supervised(\n",
    "                x_eeg, x_emg, x_et,\n",
    "                eeg_psd=eeg_psd,\n",
    "                emg_feat=emg_feat,\n",
    "            )\n",
    "\n",
    "    # Warm-up\n",
    "    for _ in range(n_warmup):\n",
    "        _forward()\n",
    "        if device.startswith(\"cuda\"):\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    # Timed runs\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(n_iters):\n",
    "        _forward()\n",
    "        if device.startswith(\"cuda\"):\n",
    "            torch.cuda.synchronize()\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    avg_batch_time = (end - start) / float(n_iters)\n",
    "    latency_ms = (avg_batch_time / float(B)) * 1000.0\n",
    "    ctrl_hz = 1000.0 / latency_ms if latency_ms > 0 else 0.0\n",
    "\n",
    "    return latency_ms, ctrl_hz\n",
    "\n",
    "\n",
    "def benchmark_runtime_latency(\n",
    "    eeg_ch: int,\n",
    "    emg_ch: int,\n",
    "    et_ch: int,\n",
    "    folds: List[int],\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Build rows for 'Table Y – Computational cost and latency of TriSaFe-Trans'.\n",
    "    Uses the first balanced fold to get realistic window shapes.\n",
    "    \"\"\"\n",
    "    if not folds:\n",
    "        return []\n",
    "\n",
    "    fold0 = folds[0]\n",
    "    train_ds0, _, test_ds0 = make_supervised_datasets_for_fold(fold0)\n",
    "\n",
    "    eeg_psd_dim = getattr(train_ds0, \"eeg_psd_dim\", 0)\n",
    "    emg_feat_dim = getattr(train_ds0, \"emg_feat_dim\", 0)\n",
    "    use_eeg_psd = (eeg_psd_dim > 0) and CFG.USE_EEG_PSD_FEATURES\n",
    "    use_emg_feat = (emg_feat_dim > 0) and CFG.USE_EMG_FEATURES\n",
    "\n",
    "    # Small test batch (works on CPU / Jetson too)\n",
    "    runtime_loader = make_dataloader(\n",
    "        test_ds0,\n",
    "        batch_size=min(32, CFG.SUP_BATCH),\n",
    "        shuffle=False,\n",
    "    )\n",
    "    runtime_batch = next(iter(runtime_loader))\n",
    "\n",
    "    # Base model on CPU (weights don't matter for latency / param count)\n",
    "    model_cpu = TriModalSafetyTransformer(\n",
    "        eeg_ch=eeg_ch,\n",
    "        emg_ch=emg_ch,\n",
    "        et_ch=et_ch,\n",
    "        num_task_classes=train_ds0.num_task_classes,\n",
    "        d_model=CFG.D_MODEL,\n",
    "        dropout=CFG.DROPOUT,\n",
    "        use_eeg_psd=use_eeg_psd,\n",
    "        use_emg_feat=use_emg_feat,\n",
    "        eeg_psd_dim=eeg_psd_dim,\n",
    "        emg_feat_dim=emg_feat_dim,\n",
    "        eeg_psd_mean=None,\n",
    "        eeg_psd_std=None,\n",
    "        emg_feat_mean=None,\n",
    "        emg_feat_std=None,\n",
    "    ).cpu()\n",
    "\n",
    "    params_m = count_parameters(model_cpu) / 1e6\n",
    "\n",
    "    # CPU latency (representative of Jetson-class hardware)\n",
    "    cpu_lat_s0, cpu_rate_s0 = _measure_latency_one_device(\n",
    "        model_cpu, runtime_batch, device=\"cpu\", drop_et=False,\n",
    "    )\n",
    "    cpu_lat_s3, cpu_rate_s3 = _measure_latency_one_device(\n",
    "        model_cpu, runtime_batch, device=\"cpu\", drop_et=True,\n",
    "    )\n",
    "\n",
    "    rows = [\n",
    "        {\n",
    "            \"model\": \"TriSaFe-Trans\",\n",
    "            \"scenario\": \"S0: EEG+EMG+ET\",\n",
    "            \"params_m\": params_m,\n",
    "            \"flops_per_window\": \"\",  # optional, fill manually if you want\n",
    "            \"latency_ms_gpu\": \"\",\n",
    "            \"control_rate_hz_gpu\": \"\",\n",
    "            \"latency_ms_cpu\": cpu_lat_s0,\n",
    "            \"control_rate_hz_cpu\": cpu_rate_s0,\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"TriSaFe-Trans\",\n",
    "            \"scenario\": \"S3: EEG+EMG (ET dropped)\",\n",
    "            \"params_m\": params_m,\n",
    "            \"flops_per_window\": \"\",\n",
    "            \"latency_ms_gpu\": \"\",\n",
    "            \"control_rate_hz_gpu\": \"\",\n",
    "            \"latency_ms_cpu\": cpu_lat_s3,\n",
    "            \"control_rate_hz_cpu\": cpu_rate_s3,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Optional GPU timings (desktop / cluster)\n",
    "    if torch.cuda.is_available():\n",
    "        model_gpu = TriModalSafetyTransformer(\n",
    "            eeg_ch=eeg_ch,\n",
    "            emg_ch=emg_ch,\n",
    "            et_ch=et_ch,\n",
    "            num_task_classes=train_ds0.num_task_classes,\n",
    "            d_model=CFG.D_MODEL,\n",
    "            dropout=CFG.DROPOUT,\n",
    "            use_eeg_psd=use_eeg_psd,\n",
    "            use_emg_feat=use_emg_feat,\n",
    "            eeg_psd_dim=eeg_psd_dim,\n",
    "            emg_feat_dim=emg_feat_dim,\n",
    "            eeg_psd_mean=None,\n",
    "            eeg_psd_std=None,\n",
    "            emg_feat_mean=None,\n",
    "            emg_feat_std=None,\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        gpu_lat_s0, gpu_rate_s0 = _measure_latency_one_device(\n",
    "            model_gpu, runtime_batch, device=\"cuda\", drop_et=False,\n",
    "        )\n",
    "        gpu_lat_s3, gpu_rate_s3 = _measure_latency_one_device(\n",
    "            model_gpu, runtime_batch, device=\"cuda\", drop_et=True,\n",
    "        )\n",
    "\n",
    "        rows[0][\"latency_ms_gpu\"] = gpu_lat_s0\n",
    "        rows[0][\"control_rate_hz_gpu\"] = gpu_rate_s0\n",
    "        rows[1][\"latency_ms_gpu\"] = gpu_lat_s3\n",
    "        rows[1][\"control_rate_hz_gpu\"] = gpu_rate_s3\n",
    "\n",
    "    # NOTE: If you later add a TriModal-GRU baseline class, you can call the\n",
    "    # same helpers here and append another row with model=\"TriModal-GRU\".\n",
    "    return rows\n",
    "\n",
    "\n",
    "#================\n",
    "# ---------------- MAIN RUNNER (BioRob RQ) ----------------\n",
    "\n",
    "def run_biorob_phase6():\n",
    "    print(\"========== Phase 6 (BioRob) — SSL → Safety-Aware LOSO ==========\")\n",
    "    print(f\"Device: {CFG.DEVICE}\")\n",
    "    print(f\"Dataset dir: {CFG.DATASET_DIR}\")\n",
    "    print(f\"Task codes (with rest): {CFG.TASK_CODES}\")\n",
    "\n",
    "    if CFG.USE_SSL:\n",
    "        ssl_dataset, eeg_ch, emg_ch, et_ch = make_ssl_dataset()\n",
    "        print(\n",
    "            f\"SSL dataset size: {len(ssl_dataset)}, \"\n",
    "            f\"channels: EEG={eeg_ch}, EMG={emg_ch}, ET={et_ch}\"\n",
    "        )\n",
    "        base_state = pretrain_ssl(eeg_ch, emg_ch, et_ch, ssl_dataset)\n",
    "    else:\n",
    "        print(\"[Phase 6] USE_SSL=False → skipping SSL pretraining.\")\n",
    "        base_state = None\n",
    "        folds_tmp = discover_folds(CFG.BALANCED_PREFIX)\n",
    "        if not folds_tmp:\n",
    "            raise SystemExit(\n",
    "                f\"No balanced folds found with prefix {CFG.BALANCED_PREFIX}_fold*\"\n",
    "            )\n",
    "        first_fold = folds_tmp[0]\n",
    "        tmp_train_ds, _, _ = make_supervised_datasets_for_fold(first_fold)\n",
    "        eeg_ch = tmp_train_ds.eeg_ch\n",
    "        emg_ch = tmp_train_ds.emg_ch\n",
    "        et_ch  = tmp_train_ds.et_ch\n",
    "        print(\n",
    "            f\"[Phase 6] Channels inferred from balanced fold {first_fold}: \"\n",
    "            f\"EEG={eeg_ch}, EMG={emg_ch}, ET={et_ch}\"\n",
    "        )\n",
    "\n",
    "    folds = discover_folds(CFG.BALANCED_PREFIX)\n",
    "    if not folds:\n",
    "        raise SystemExit(\n",
    "            f\"No balanced folds found with prefix {CFG.BALANCED_PREFIX}_fold*\"\n",
    "        )\n",
    "    print(f\"LOSO folds: {folds}\")\n",
    "\n",
    "    all_results: Dict[str, List[Dict]] = {name: [] for name in ABLATIONS.keys()}\n",
    "\n",
    "    for ablation_name, flags in ABLATIONS.items():\n",
    "        use_eeg = flags[\"use_eeg\"]\n",
    "        use_emg = flags[\"use_emg\"]\n",
    "        use_et  = flags[\"use_et\"]\n",
    "        print(\n",
    "            f\"\\n\\n############################################################\\n\"\n",
    "            f\"### Ablation: {ablation_name} (EEG={use_eeg}, EMG={use_emg}, ET={use_et})\\n\"\n",
    "            f\"############################################################\"\n",
    "        )\n",
    "\n",
    "        for fid in folds:\n",
    "            res = train_one_fold(\n",
    "                fold_id=fid,\n",
    "                base_state=base_state,\n",
    "                eeg_ch=eeg_ch,\n",
    "                emg_ch=emg_ch,\n",
    "                et_ch=et_ch,\n",
    "                ablation_name=ablation_name,\n",
    "                use_eeg=use_eeg,\n",
    "                use_emg=use_emg,\n",
    "                use_et=use_et,\n",
    "            )\n",
    "            all_results[ablation_name].append(res)\n",
    "\n",
    "    # ---------------- SUMMARY (MSG, MRS, GMUE, Safety) ----------------\n",
    "\n",
    "    summary = {\n",
    "        \"folds\": folds,\n",
    "        \"cfg\": {\n",
    "            \"use_ssl\": CFG.USE_SSL,\n",
    "            \"ssl_epochs\": CFG.SSL_EPOCHS,\n",
    "            \"sup_epochs\": CFG.SUP_EPOCHS,\n",
    "            \"d_model\": CFG.D_MODEL,\n",
    "            \"dropout\": CFG.DROPOUT,\n",
    "            \"topk\": CFG.TOPK,\n",
    "            \"task_codes\": CFG.TASK_CODES,\n",
    "            \"scenarios\": SCENARIOS,\n",
    "            \"policies\": POLICIES,\n",
    "        },\n",
    "        \"ablations\": {},\n",
    "        \"MSG\": {},\n",
    "        \"MRS\": {},\n",
    "        \"GMUE\": {},\n",
    "        \"safety\": {},\n",
    "        \"gating\": {},\n",
    "        \"P2_tradeoff_curves\": {},\n",
    "        \"confusion_matrices\": {},\n",
    "    }\n",
    "    # --- Per-ablation classic metrics (for your \"one small table\") ---\n",
    "    table1_rows = []\n",
    "\n",
    "    for ablation_name, res_list in all_results.items():\n",
    "        # Collect per-fold metrics\n",
    "        action_accs       = [r[\"classic\"][\"action_acc\"]       for r in res_list]\n",
    "        action_bal        = [r[\"classic\"][\"action_bal_acc\"]   for r in res_list]\n",
    "        action_f1_macro   = [r[\"classic\"][\"action_macro_f1\"]  for r in res_list]\n",
    "        action_f1_micro   = [r[\"classic\"][\"action_micro_f1\"]  for r in res_list]\n",
    "        action_auc        = [r[\"classic\"][\"action_auc_roc\"]   for r in res_list\n",
    "                             if r[\"classic\"][\"action_auc_roc\"] is not None]\n",
    "        action_auprc      = [r[\"classic\"][\"action_auprc\"]     for r in res_list\n",
    "                             if r[\"classic\"][\"action_auprc\"] is not None]\n",
    "\n",
    "        task_accs         = [r[\"classic\"][\"task_acc\"]         for r in res_list]\n",
    "        task_bal          = [r[\"classic\"][\"task_bal_acc\"]     for r in res_list]\n",
    "        task_f1_macro     = [r[\"classic\"][\"task_macro_f1\"]    for r in res_list]\n",
    "        task_f1_micro     = [r[\"classic\"][\"task_micro_f1\"]    for r in res_list]\n",
    "        top1_vals         = [r[\"classic\"][\"task_topk\"].get(\"top_1\", 0.0)\n",
    "                             for r in res_list]\n",
    "        top3_vals         = [r[\"classic\"][\"task_topk\"].get(\"top_3\", 0.0)\n",
    "                             for r in res_list]\n",
    "\n",
    "        def _mean_std(xs):\n",
    "            if len(xs) == 0:\n",
    "                return (None, None)\n",
    "            return (float(np.mean(xs)), float(np.std(xs)))\n",
    "\n",
    "        a_acc_mean, a_acc_std     = _mean_std(action_accs)\n",
    "        a_bal_mean, a_bal_std     = _mean_std(action_bal)\n",
    "        a_f1M_mean, a_f1M_std     = _mean_std(action_f1_macro)\n",
    "        a_f1m_mean, a_f1m_std     = _mean_std(action_f1_micro)\n",
    "        a_auc_mean, a_auc_std     = _mean_std(action_auc)\n",
    "        a_auprc_mean, a_auprc_std = _mean_std(action_auprc)\n",
    "\n",
    "        t_acc_mean, t_acc_std     = _mean_std(task_accs)\n",
    "        t_bal_mean, t_bal_std     = _mean_std(task_bal)\n",
    "        t_f1M_mean, t_f1M_std     = _mean_std(task_f1_macro)\n",
    "        t_f1m_mean, t_f1m_std     = _mean_std(task_f1_micro)\n",
    "        t_top1_mean, t_top1_std   = _mean_std(top1_vals)\n",
    "        t_top3_mean, t_top3_std   = _mean_std(top3_vals)\n",
    "\n",
    "        summary[\"ablations\"][ablation_name] = {\n",
    "            \"action\": {\n",
    "                \"mean_acc\": a_acc_mean,\n",
    "                \"std_acc\": a_acc_std,\n",
    "                \"mean_bal_acc\": a_bal_mean,\n",
    "                \"std_bal_acc\": a_bal_std,\n",
    "                \"macro_f1_mean\": a_f1M_mean,\n",
    "                \"macro_f1_std\": a_f1M_std,\n",
    "                \"micro_f1_mean\": a_f1m_mean,\n",
    "                \"micro_f1_std\": a_f1m_std,\n",
    "                \"auc_mean\": a_auc_mean,\n",
    "                \"auc_std\": a_auc_std,\n",
    "                \"auprc_mean\": a_auprc_mean,\n",
    "                \"auprc_std\": a_auprc_std,\n",
    "            },\n",
    "            \"task\": {\n",
    "                \"mean_acc\": t_acc_mean,\n",
    "                \"std_acc\": t_acc_std,\n",
    "                \"mean_bal_acc\": t_bal_mean,\n",
    "                \"std_bal_acc\": t_bal_std,\n",
    "                \"macro_f1_mean\": t_f1M_mean,\n",
    "                \"macro_f1_std\": t_f1M_std,\n",
    "                \"micro_f1_mean\": t_f1m_mean,\n",
    "                \"micro_f1_std\": t_f1m_std,\n",
    "                \"top1_mean\": t_top1_mean,\n",
    "                \"top1_std\": t_top1_std,\n",
    "                \"top3_mean\": t_top3_mean,\n",
    "                \"top3_std\": t_top3_std,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        print(\n",
    "            f\"\\n[Ablation={ablation_name}] Action acc={a_acc_mean:.3f}±{a_acc_std:.3f}, \"\n",
    "            f\"bal-acc={a_bal_mean:.3f}±{a_bal_std:.3f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"[Ablation={ablation_name}] Task   acc={t_acc_mean:.3f}±{t_acc_std:.3f}, \"\n",
    "            f\"bal-acc={t_bal_mean:.3f}±{t_bal_std:.3f}\"\n",
    "        )\n",
    "\n",
    "        # Row for Table 1\n",
    "        table1_rows.append([\n",
    "            ablation_name,\n",
    "            a_acc_mean, a_acc_std,\n",
    "            a_f1M_mean, a_f1M_std,\n",
    "            a_f1m_mean, a_f1m_std,\n",
    "            a_bal_mean, a_bal_std,\n",
    "            a_auc_mean, a_auc_std,\n",
    "            a_auprc_mean, a_auprc_std,\n",
    "            t_acc_mean, t_acc_std,\n",
    "            t_f1M_mean, t_f1M_std,\n",
    "            t_f1m_mean, t_f1m_std,\n",
    "            t_bal_mean, t_bal_std,\n",
    "            t_top1_mean, t_top1_std,\n",
    "            t_top3_mean, t_top3_std,\n",
    "        ])\n",
    "\n",
    "    # --- MSG (Multimodal Synergy Gain) from ablations (using balanced acc) ---\n",
    "    if all(k in summary[\"ablations\"] for k in (\"all\", \"eeg\", \"emg\", \"et\")):\n",
    "        ba_all_task = summary[\"ablations\"][\"all\"][\"task\"][\"mean_bal_acc\"]\n",
    "        ba_eeg_task = summary[\"ablations\"][\"eeg\"][\"task\"][\"mean_bal_acc\"]\n",
    "        ba_emg_task = summary[\"ablations\"][\"emg\"][\"task\"][\"mean_bal_acc\"]\n",
    "        ba_et_task  = summary[\"ablations\"][\"et\"][\"task\"][\"mean_bal_acc\"]\n",
    "        msg_task = float(ba_all_task - max(ba_eeg_task, ba_emg_task, ba_et_task))\n",
    "\n",
    "        ba_all_action = summary[\"ablations\"][\"all\"][\"action\"][\"mean_bal_acc\"]\n",
    "        ba_eeg_action = summary[\"ablations\"][\"eeg\"][\"action\"][\"mean_bal_acc\"]\n",
    "        ba_emg_action = summary[\"ablations\"][\"emg\"][\"action\"][\"mean_bal_acc\"]\n",
    "        ba_et_action  = summary[\"ablations\"][\"et\"][\"action\"][\"mean_bal_acc\"]\n",
    "        msg_action = float(ba_all_action - max(ba_eeg_action, ba_emg_action, ba_et_action))\n",
    "\n",
    "        summary[\"MSG\"] = {\n",
    "            \"task_bal_acc\": msg_task,\n",
    "            \"action_bal_acc\": msg_action,\n",
    "        }\n",
    "        print(\n",
    "            f\"\\nMSG (task bal-acc)   : {msg_task:.3f}\\n\"\n",
    "            f\"MSG (action bal-acc) : {msg_action:.3f}\"\n",
    "        )\n",
    "    else:\n",
    "        msg_action = None\n",
    "        msg_task = None\n",
    "\n",
    "    # --- GMUE (mean gating entropy, S0) ---\n",
    "    gmue_vals = []\n",
    "    for r in all_results[\"all\"]:\n",
    "        g = r.get(\"gmue\", None)\n",
    "        if g is not None:\n",
    "            gmue_vals.append(g)\n",
    "    if gmue_vals:\n",
    "        gmue_mean = float(np.mean(gmue_vals))\n",
    "        gmue_std  = float(np.std(gmue_vals))\n",
    "        summary[\"GMUE\"] = {\"mean\": gmue_mean, \"std\": gmue_std}\n",
    "        print(f\"\\nGMUE (gating entropy, S0, all-fold mean±std): {gmue_mean:.4f}±{gmue_std:.4f}\")\n",
    "\n",
    "    # --- Safety metrics (SAE, UAR, MIR, NRA) across folds for each scenario/policy ---\n",
    "    safety_summary = {}\n",
    "    for scen_name in SCENARIOS.keys():\n",
    "        scen_entry = {\"policies\": {}}\n",
    "        for policy in POLICIES:\n",
    "            SAE_vals = []\n",
    "            UAR_vals = []\n",
    "            MIR_vals = []\n",
    "            NRA_vals = []\n",
    "\n",
    "            for r in all_results[\"all\"]:\n",
    "                s = r.get(\"safety\", None)\n",
    "                if s is None:\n",
    "                    continue\n",
    "                scen_dict = s[\"scenarios\"].get(scen_name, None)\n",
    "                if scen_dict is None:\n",
    "                    continue\n",
    "                p_entry = scen_dict[\"policies\"].get(policy, None)\n",
    "                if p_entry is None:\n",
    "                    continue\n",
    "                saf = p_entry[\"safety\"]\n",
    "                SAE_vals.append(saf[\"SAE\"])\n",
    "                UAR_vals.append(saf[\"UAR\"])\n",
    "                MIR_vals.append(saf[\"MIR\"])\n",
    "                NRA_vals.append(saf[\"NRA\"])\n",
    "\n",
    "            if SAE_vals:\n",
    "                scen_entry[\"policies\"][policy] = {\n",
    "                    \"SAE_mean\": float(np.mean(SAE_vals)),\n",
    "                    \"SAE_std\": float(np.std(SAE_vals)),\n",
    "                    \"UAR_mean\": float(np.mean(UAR_vals)),\n",
    "                    \"UAR_std\": float(np.std(UAR_vals)),\n",
    "                    \"MIR_mean\": float(np.mean(MIR_vals)),\n",
    "                    \"MIR_std\": float(np.std(MIR_vals)),\n",
    "                    \"NRA_mean\": float(np.mean(NRA_vals)),\n",
    "                    \"NRA_std\": float(np.std(NRA_vals)),\n",
    "                }\n",
    "\n",
    "        safety_summary[scen_name] = scen_entry\n",
    "    summary[\"safety\"] = safety_summary\n",
    "\n",
    "    print(\"\\nSafety metrics (SAE/UAR/MIR/NRA) — mean across folds:\")\n",
    "    for scen_name, scen_entry in safety_summary.items():\n",
    "        print(f\"  Scenario {scen_name} ({SCENARIOS[scen_name]['description']}):\")\n",
    "        for policy, vals in scen_entry[\"policies\"].items():\n",
    "            print(\n",
    "                f\"    {policy}: SAE={vals['SAE_mean']:.3f}, UAR={vals['UAR_mean']:.3f}, \"\n",
    "                f\"MIR={vals['MIR_mean']:.3f}, NRA={vals['NRA_mean']:.3f}\"\n",
    "            )\n",
    "\n",
    "    # --- Gating summary (for Fig. 8) ---\n",
    "    gating_summary = {}\n",
    "    for scen_name in SCENARIOS.keys():\n",
    "        gate_means = []\n",
    "        for r in all_results[\"all\"]:\n",
    "            s = r.get(\"safety\", None)\n",
    "            if s is None:\n",
    "                continue\n",
    "            scen_dict = s[\"scenarios\"].get(scen_name, None)\n",
    "            if scen_dict is None:\n",
    "                continue\n",
    "            gmean = scen_dict.get(\"gates_mean\", None)\n",
    "            if gmean is not None:\n",
    "                gate_means.append(np.array(gmean, dtype=float))\n",
    "        if gate_means:\n",
    "            gate_means_arr = np.stack(gate_means, axis=0)\n",
    "            mean_gate = gate_means_arr.mean(axis=0)\n",
    "            std_gate = gate_means_arr.std(axis=0)\n",
    "            gating_summary[scen_name] = {\n",
    "                \"mean_gate\": mean_gate.tolist(),   # [EEG, EMG, ET]\n",
    "                \"std_gate\": std_gate.tolist(),\n",
    "            }\n",
    "    summary[\"gating\"] = gating_summary\n",
    "\n",
    "    # --- MRS (Modality-Robustness Score) from S0 vs S1–S3 for P0 ---\n",
    "    mrs_task = {}\n",
    "    mrs_action = {}\n",
    "    for drop, scen in zip([\"drop_eeg\", \"drop_emg\", \"drop_et\"], [\"S1\", \"S2\", \"S3\"]):\n",
    "        base_ba_action = []\n",
    "        drop_ba_action = []\n",
    "        base_ba_task   = []\n",
    "        drop_ba_task   = []\n",
    "        for r in all_results[\"all\"]:\n",
    "            s = r.get(\"safety\", None)\n",
    "            if s is None:\n",
    "                continue\n",
    "            scen0 = s[\"scenarios\"].get(\"S0\", None)\n",
    "            scenD = s[\"scenarios\"].get(scen, None)\n",
    "            if scen0 is None or scenD is None:\n",
    "                continue\n",
    "            p0_0 = scen0[\"policies\"][\"P0\"][\"classic\"]\n",
    "            p0_D = scenD[\"policies\"][\"P0\"][\"classic\"]\n",
    "            base_ba_action.append(p0_0[\"action_bal_acc\"])\n",
    "            drop_ba_action.append(p0_D[\"action_bal_acc\"])\n",
    "            base_ba_task.append(p0_0[\"task_bal_acc\"])\n",
    "            drop_ba_task.append(p0_D[\"task_bal_acc\"])\n",
    "        if base_ba_action:\n",
    "            mrs_action[drop] = float(np.mean(np.array(base_ba_action) - np.array(drop_ba_action)))\n",
    "            mrs_task[drop]   = float(np.mean(np.array(base_ba_task)   - np.array(drop_ba_task)))\n",
    "\n",
    "    summary[\"MRS\"] = {\n",
    "        \"action\": mrs_action,\n",
    "        \"task\": mrs_task,\n",
    "    }\n",
    "    print(\"\\nMRS (Modality-Robustness Score, bal-acc drop under P0):\")\n",
    "    print(\"  Action:\", mrs_action)\n",
    "    print(\"  Task  :\", mrs_task)\n",
    "\n",
    "    # --- P2 trade-off curves (safety vs missed intent) averaged across folds ---\n",
    "    p2_curves_summary = {}\n",
    "    for scen_name in SCENARIOS.keys():\n",
    "        sae_mat = []\n",
    "        mir_mat = []\n",
    "        thresholds = None\n",
    "        for r in all_results[\"all\"]:\n",
    "            s = r.get(\"safety\", None)\n",
    "            if s is None:\n",
    "                continue\n",
    "            scen_dict = s[\"scenarios\"].get(scen_name, None)\n",
    "            if scen_dict is None:\n",
    "                continue\n",
    "            curve = scen_dict.get(\"P2_curve\", None)\n",
    "            if curve is None:\n",
    "                continue\n",
    "            th = curve[\"thresholds\"]\n",
    "            sae = curve[\"SAE\"]\n",
    "            mir = curve[\"MIR\"]\n",
    "            if thresholds is None:\n",
    "                thresholds = th\n",
    "            sae_mat.append(sae)\n",
    "            mir_mat.append(mir)\n",
    "        if thresholds is not None and sae_mat:\n",
    "            sae_arr = np.array(sae_mat)\n",
    "            mir_arr = np.array(mir_mat)\n",
    "            p2_curves_summary[scen_name] = {\n",
    "                \"thresholds\": thresholds,\n",
    "                \"SAE_mean\": sae_arr.mean(axis=0).tolist(),\n",
    "                \"SAE_std\": sae_arr.std(axis=0).tolist(),\n",
    "                \"MIR_mean\": mir_arr.mean(axis=0).tolist(),\n",
    "                \"MIR_std\": mir_arr.std(axis=0).tolist(),\n",
    "            }\n",
    "\n",
    "    summary[\"P2_tradeoff_curves\"] = p2_curves_summary\n",
    "\n",
    "    # --- Aggregate confusion matrices across folds (tri-modal, S0/P0) ---\n",
    "    cm_action_agg = None\n",
    "    cm_task_agg = None\n",
    "    for r in all_results[\"all\"]:\n",
    "        cm_a = np.array(r[\"classic\"][\"cm_action\"], dtype=int)\n",
    "        cm_t = np.array(r[\"classic\"][\"cm_task\"], dtype=int)\n",
    "        if cm_action_agg is None:\n",
    "            cm_action_agg = cm_a\n",
    "            cm_task_agg = cm_t\n",
    "        else:\n",
    "            cm_action_agg += cm_a\n",
    "            cm_task_agg += cm_t\n",
    "\n",
    "    if cm_action_agg is not None:\n",
    "        summary[\"confusion_matrices\"][\"S0P0_all\"] = {\n",
    "            \"action_cm\": cm_action_agg.tolist(),\n",
    "            \"task_cm\": cm_task_agg.tolist(),\n",
    "        }\n",
    "\n",
    "    # --- Create table & fig-data CSVs ---\n",
    "    tables_dir = CFG.DATASET_DIR / \"phase6_tables\"\n",
    "    figdata_dir = CFG.DATASET_DIR / \"phase6_figdata\"\n",
    "    tables_dir.mkdir(exist_ok=True, parents=True)\n",
    "    figdata_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Table 1 – Main LOSO performance (S0 / P0, all ablations)\n",
    "    table1_path = tables_dir / \"phase6_table1_loso_main_metrics.csv\"\n",
    "    with open(table1_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\n",
    "            \"ablation\",\n",
    "            \"action_acc_mean\", \"action_acc_std\",\n",
    "            \"action_macro_f1_mean\", \"action_macro_f1_std\",\n",
    "            \"action_micro_f1_mean\", \"action_micro_f1_std\",\n",
    "            \"action_bal_acc_mean\", \"action_bal_acc_std\",\n",
    "            \"action_auc_roc_mean\", \"action_auc_roc_std\",\n",
    "            \"action_auprc_mean\", \"action_auprc_std\",\n",
    "            \"task_acc_mean\", \"task_acc_std\",\n",
    "            \"task_macro_f1_mean\", \"task_macro_f1_std\",\n",
    "            \"task_micro_f1_mean\", \"task_micro_f1_std\",\n",
    "            \"task_bal_acc_mean\", \"task_bal_acc_std\",\n",
    "            \"task_top1_mean\", \"task_top1_std\",\n",
    "            \"task_top3_mean\", \"task_top3_std\",\n",
    "            \"MSG_action_bal_acc\", \"MSG_task_bal_acc\",\n",
    "        ])\n",
    "        for row in table1_rows:\n",
    "            # Extend with MSG columns (blank for ablation rows)\n",
    "            w.writerow(row + [\"\", \"\"])\n",
    "        # MSG row at bottom\n",
    "        w.writerow([\n",
    "            \"MSG\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "            \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "            \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "            msg_action if msg_action is not None else \"\",\n",
    "            msg_task if msg_task is not None else \"\",\n",
    "        ])\n",
    "\n",
    "    # Table 2 – Safety metrics per scenario & policy\n",
    "    table2_path = tables_dir / \"phase6_table2_safety_policies.csv\"\n",
    "    with open(table2_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\n",
    "            \"scenario\", \"policy\",\n",
    "            \"SAE_mean\", \"SAE_std\",\n",
    "            \"UAR_mean\", \"UAR_std\",\n",
    "            \"MIR_mean\", \"MIR_std\",\n",
    "            \"NRA_mean\", \"NRA_std\",\n",
    "        ])\n",
    "        for scen_name, scen_entry in safety_summary.items():\n",
    "            for policy, vals in scen_entry[\"policies\"].items():\n",
    "                w.writerow([\n",
    "                    scen_name, policy,\n",
    "                    vals[\"SAE_mean\"], vals[\"SAE_std\"],\n",
    "                    vals[\"UAR_mean\"], vals[\"UAR_std\"],\n",
    "                    vals[\"MIR_mean\"], vals[\"MIR_std\"],\n",
    "                    vals[\"NRA_mean\"], vals[\"NRA_std\"],\n",
    "                ])\n",
    "\n",
    "    # Table 3 – MRS + GMUE\n",
    "    table3_path = tables_dir / \"phase6_table3_mrs_gmue.csv\"\n",
    "    gmue_mean = summary.get(\"GMUE\", {}).get(\"mean\", None)\n",
    "    gmue_std  = summary.get(\"GMUE\", {}).get(\"std\", None)\n",
    "    drop2mod = {\"drop_eeg\": \"EEG\", \"drop_emg\": \"EMG\", \"drop_et\": \"ET\"}\n",
    "    with open(table3_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\n",
    "            \"modality_dropped\",\n",
    "            \"MRS_action_bal_acc_drop\",\n",
    "            \"MRS_task_bal_acc_drop\",\n",
    "            \"GMUE_mean_S0\",\n",
    "            \"GMUE_std_S0\",\n",
    "        ])\n",
    "        for drop_key, m_action in mrs_action.items():\n",
    "            m_task = mrs_task.get(drop_key, None)\n",
    "            w.writerow([\n",
    "                drop2mod.get(drop_key, drop_key),\n",
    "                m_action,\n",
    "                m_task,\n",
    "                gmue_mean,\n",
    "                gmue_std,\n",
    "            ])\n",
    "\n",
    "    # Table A2 – Per-task metrics (tri-modal, S0/P0)\n",
    "    tableA2_path = tables_dir / \"phase6_tableA2_per_task_metrics_all.csv\"\n",
    "    # Use task2idx mapping from first tri-modal result\n",
    "    idx2task = {}\n",
    "    if all_results[\"all\"]:\n",
    "        task2idx = all_results[\"all\"][0][\"task2idx\"]\n",
    "        idx2task = {idx: code for code, idx in task2idx.items()}\n",
    "\n",
    "    per_class_agg = {}\n",
    "    for r in all_results[\"all\"]:\n",
    "        per_class = r[\"classic\"][\"per_class_task\"]\n",
    "        for cls_idx, m in per_class.items():\n",
    "            cls_idx = int(cls_idx)\n",
    "            if cls_idx not in per_class_agg:\n",
    "                per_class_agg[cls_idx] = {\n",
    "                    \"precision\": [], \"recall\": [], \"f1\": [],\n",
    "                    \"balanced_accuracy\": [], \"support\": 0,\n",
    "                }\n",
    "            per_class_agg[cls_idx][\"precision\"].append(m[\"precision\"])\n",
    "            per_class_agg[cls_idx][\"recall\"].append(m[\"recall\"])\n",
    "            per_class_agg[cls_idx][\"f1\"].append(m[\"f1\"])\n",
    "            per_class_agg[cls_idx][\"balanced_accuracy\"].append(m[\"balanced_accuracy\"])\n",
    "            per_class_agg[cls_idx][\"support\"] += m[\"support\"]\n",
    "\n",
    "    with open(tableA2_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\n",
    "            \"class_index\", \"task_code\",\n",
    "            \"precision_mean\", \"recall_mean\",\n",
    "            \"f1_mean\", \"balanced_accuracy_mean\",\n",
    "            \"support_total\",\n",
    "        ])\n",
    "        for cls_idx in sorted(per_class_agg.keys()):\n",
    "            agg = per_class_agg[cls_idx]\n",
    "            prec_mean = float(np.mean(agg[\"precision\"])) if agg[\"precision\"] else 0.0\n",
    "            rec_mean  = float(np.mean(agg[\"recall\"])) if agg[\"recall\"] else 0.0\n",
    "            f1_mean   = float(np.mean(agg[\"f1\"])) if agg[\"f1\"] else 0.0\n",
    "            bal_mean  = float(np.mean(agg[\"balanced_accuracy\"])) if agg[\"balanced_accuracy\"] else 0.0\n",
    "            support   = int(agg[\"support\"])\n",
    "            task_code = idx2task.get(cls_idx, None)\n",
    "            w.writerow([\n",
    "                cls_idx, task_code,\n",
    "                prec_mean, rec_mean,\n",
    "                f1_mean, bal_mean,\n",
    "                support,\n",
    "            ])\n",
    "\n",
    "    # Table A3 – Action & task bal-acc under S0–S3 (tri-modal, P0)\n",
    "    tableA3_path = tables_dir / \"phase6_tableA3_bal_acc_per_scenario_P0.csv\"\n",
    "    # Table Y2 – Safety error breakdown for S0 / P2 (NEW)\n",
    "    tableY2_path = tables_dir / \"phase6_tableY2_error_breakdown_S0P2.csv\"\n",
    "    total_counts = {\n",
    "        \"total\": 0,\n",
    "        \"total_action\": 0,\n",
    "        \"total_rest\": 0,\n",
    "        \"correct_move\": 0,\n",
    "        \"unsafe_move\": 0,\n",
    "        \"missed_intent\": 0,\n",
    "        \"safe_idle\": 0,\n",
    "    }\n",
    "    for r in all_results[\"all\"]:\n",
    "        s = r.get(\"safety\", None)\n",
    "        if s is None:\n",
    "            continue\n",
    "        scen0 = s[\"scenarios\"].get(\"S0\", None)\n",
    "        if scen0 is None:\n",
    "            continue\n",
    "        p2 = scen0[\"policies\"].get(\"P2\", None)\n",
    "        if p2 is None:\n",
    "            continue\n",
    "        counts = p2[\"safety\"][\"counts\"]\n",
    "        for k in total_counts.keys():\n",
    "            if k in counts:\n",
    "                total_counts[k] += counts[k]\n",
    "\n",
    "    with open(tableY2_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\n",
    "            \"scenario\",\n",
    "            \"policy\",\n",
    "            \"total\",\n",
    "            \"total_action\",\n",
    "            \"total_rest\",\n",
    "            \"correct_move\",\n",
    "            \"unsafe_move\",\n",
    "            \"missed_intent\",\n",
    "            \"safe_idle\",\n",
    "        ])\n",
    "        w.writerow([\n",
    "            \"S0\",\n",
    "            \"P2\",\n",
    "            total_counts[\"total\"],\n",
    "            total_counts[\"total_action\"],\n",
    "            total_counts[\"total_rest\"],\n",
    "            total_counts[\"correct_move\"],\n",
    "            total_counts[\"unsafe_move\"],\n",
    "            total_counts[\"missed_intent\"],\n",
    "            total_counts[\"safe_idle\"],\n",
    "        ])\n",
    "    print(f\"Safety error breakdown table saved to: {tableY2_path}\")\n",
    "\n",
    "    with open(tableA3_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\n",
    "            \"scenario\",\n",
    "            \"action_bal_acc_mean\", \"action_bal_acc_std\",\n",
    "            \"task_bal_acc_mean\", \"task_bal_acc_std\",\n",
    "        ])\n",
    "        for scen_name in [\"S0\", \"S1\", \"S2\", \"S3\"]:\n",
    "            act_ba = []\n",
    "            task_ba = []\n",
    "            for r in all_results[\"all\"]:\n",
    "                s = r.get(\"safety\", None)\n",
    "                if s is None:\n",
    "                    continue\n",
    "                scen = s[\"scenarios\"].get(scen_name, None)\n",
    "                if scen is None:\n",
    "                    continue\n",
    "                classic = scen[\"policies\"][\"P0\"][\"classic\"]\n",
    "                act_ba.append(classic[\"action_bal_acc\"])\n",
    "                task_ba.append(classic[\"task_bal_acc\"])\n",
    "            if act_ba:\n",
    "                w.writerow([\n",
    "                    scen_name,\n",
    "                    float(np.mean(act_ba)), float(np.std(act_ba)),\n",
    "                    float(np.mean(task_ba)), float(np.std(task_ba)),\n",
    "                ])\n",
    "\n",
    "    # Fig. 6 – P2 trade-off curves (one CSV per scenario)\n",
    "    for scen_name, curve in summary[\"P2_tradeoff_curves\"].items():\n",
    "        path = figdata_dir / f\"phase6_fig6_P2_tradeoff_{scen_name}.csv\"\n",
    "        with open(path, \"w\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"threshold\", \"SAE_mean\", \"SAE_std\", \"MIR_mean\", \"MIR_std\"])\n",
    "            for th, sae_m, sae_s, mir_m, mir_s in zip(\n",
    "                curve[\"thresholds\"],\n",
    "                curve[\"SAE_mean\"], curve[\"SAE_std\"],\n",
    "                curve[\"MIR_mean\"], curve[\"MIR_std\"],\n",
    "            ):\n",
    "                w.writerow([th, sae_m, sae_s, mir_m, mir_s])\n",
    "\n",
    "    # Fig. 7 – Confusion matrices (aggregated across folds)\n",
    "    if cm_action_agg is not None:\n",
    "        np.savetxt(\n",
    "            figdata_dir / \"phase6_fig7_cm_action_S0P0_all.csv\",\n",
    "            cm_action_agg, fmt=\"%d\", delimiter=\",\"\n",
    "        )\n",
    "        np.savetxt(\n",
    "            figdata_dir / \"phase6_fig7_cm_task_S0P0_all.csv\",\n",
    "            cm_task_agg, fmt=\"%d\", delimiter=\",\"\n",
    "        )\n",
    "\n",
    "    # Fig. 8 – Gating distribution across modalities (per scenario)\n",
    "    fig8_path = figdata_dir / \"phase6_fig8_gating_distribution.csv\"\n",
    "    with open(fig8_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\n",
    "            \"scenario\",\n",
    "            \"EEG_mean_gate\", \"EMG_mean_gate\", \"ET_mean_gate\",\n",
    "            \"EEG_std_gate\", \"EMG_std_gate\", \"ET_std_gate\",\n",
    "        ])\n",
    "        for scen_name, gvals in summary[\"gating\"].items():\n",
    "            mg = gvals[\"mean_gate\"]\n",
    "            sg = gvals[\"std_gate\"]\n",
    "            w.writerow([\n",
    "                scen_name,\n",
    "                mg[0], mg[1], mg[2],\n",
    "                sg[0], sg[1], sg[2],\n",
    "            ])\n",
    "    # Table Y – Runtime / latency of TriSaFe-Trans (NEW)\n",
    "    try:\n",
    "        runtime_rows = benchmark_runtime_latency(eeg_ch, emg_ch, et_ch, folds)\n",
    "        if runtime_rows:\n",
    "            tableY_path = tables_dir / \"phase6_tableY_runtime_latency.csv\"\n",
    "            with open(tableY_path, \"w\", newline=\"\") as f:\n",
    "                w = csv.writer(f)\n",
    "                w.writerow([\n",
    "                    \"model\",\n",
    "                    \"scenario\",\n",
    "                    \"params_m\",\n",
    "                    \"flops_per_window\",\n",
    "                    \"latency_ms_gpu\",\n",
    "                    \"control_rate_hz_gpu\",\n",
    "                    \"latency_ms_cpu\",\n",
    "                    \"control_rate_hz_cpu\",\n",
    "                ])\n",
    "                for r in runtime_rows:\n",
    "                    w.writerow([\n",
    "                        r[\"model\"],\n",
    "                        r[\"scenario\"],\n",
    "                        f\"{r['params_m']:.3f}\",\n",
    "                        r[\"flops_per_window\"],\n",
    "                        \"\" if r[\"latency_ms_gpu\"] == \"\" else f\"{r['latency_ms_gpu']:.2f}\",\n",
    "                        \"\" if r[\"control_rate_hz_gpu\"] == \"\" else f\"{r['control_rate_hz_gpu']:.1f}\",\n",
    "                        f\"{r['latency_ms_cpu']:.2f}\",\n",
    "                        f\"{r['control_rate_hz_cpu']:.1f}\",\n",
    "                    ])\n",
    "            print(f\"Runtime / latency table saved to: {tableY_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Runtime] Skipping runtime table due to error: {e}\")\n",
    "\n",
    "    # --- Save main JSON summary ---\n",
    "    out_path = CFG.DATASET_DIR / \"phase6_biorob_safety_summary.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"\\nSaved BioRob Phase-6 safety summary to: {out_path}\")\n",
    "    print(f\"Table CSVs saved to: {tables_dir}\")\n",
    "    print(f\"Fig-data CSVs saved to: {figdata_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_biorob_phase6()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
