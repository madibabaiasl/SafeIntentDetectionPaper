{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c22f8-5901-4884-a17b-f1b8a8c9f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Phase 3 — Manifest & TRUE LOSO splits (sliding + onset-anchor windows)\n",
    "# Uses ALL available columns including label_action and task_target\n",
    "# =============================================================================\n",
    "from __future__ import annotations\n",
    "import re, glob, hashlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "ROOT_DIR = Path(r\"/home/tsultan1/BioRob(Final)/Data\")\n",
    "LABEL_SUBPATH = Path(r\"cleaned/synchronized_proper_lite_union_v3/labelonly\")\n",
    "CSV_GLOB = \"*_icml_consensus_labels.csv\"\n",
    "\n",
    "DATASET_DIR = ROOT_DIR / \"_dataset_icml_v1\"\n",
    "DATASET_DIR.mkdir(exist_ok=True)\n",
    "MANIFEST_OUT = DATASET_DIR / \"manifest_v1.csv\"\n",
    "SPLITS_OUT   = DATASET_DIR / \"splits_v1.csv\"\n",
    "\n",
    "# Windowing\n",
    "SLIDE_WIN_S     = 2.0\n",
    "SLIDE_STRIDE_S  = 0.25\n",
    "ANCHOR_PRE_S    = 2.0\n",
    "ANCHOR_POST_S   = 3.0\n",
    "ACTIVE_MAJ_FRAC = 0.50  # >=50% active → label_action=1\n",
    "\n",
    "# Val subject selection per fold\n",
    "VAL_SELECTION_MODE = \"smallest_other\"\n",
    "VAL_SUBJECT_ID     = 1\n",
    "\n",
    "WRITE_FILE_MD5 = False\n",
    "\n",
    "# Required columns - updated based on your data\n",
    "REQ = [\"Timestamp_seconds\", \"active\", \"task\", \"trial\", \"subject_id\", \"label_action\", \"task_target\"]\n",
    "\n",
    "# ---------------- HELPERS ----------------\n",
    "def _is_subdir_name(name: str) -> bool:\n",
    "    return re.match(r\"(?i)^sub-?\\d+$\", name) is not None\n",
    "\n",
    "def _collect_files() -> list[Path]:\n",
    "    files = []\n",
    "    for sub in sorted(p for p in ROOT_DIR.iterdir() if p.is_dir() and _is_subdir_name(p.name)):\n",
    "        label_dir = sub / LABEL_SUBPATH\n",
    "        if not label_dir.exists():\n",
    "            print(f\"[skip] no labelonly here: {label_dir}\")\n",
    "            continue\n",
    "        got = sorted(Path(p) for p in glob.glob(str((label_dir / CSV_GLOB).resolve())))\n",
    "        if not got:\n",
    "            print(f\"[skip] no CSV in: {label_dir}\")\n",
    "            continue\n",
    "        print(f\"[use] {sub.name} → labelonly ({len(got)} files)\")\n",
    "        files.extend(got)\n",
    "    return files\n",
    "\n",
    "def _median_fs(t: np.ndarray) -> float:\n",
    "    dt = np.diff(t)\n",
    "    dt = dt[np.isfinite(dt) & (dt > 0)]\n",
    "    if dt.size == 0: return np.nan\n",
    "    return 1.0 / np.median(dt)\n",
    "\n",
    "def _file_md5(path: Path, chunk: int = 1<<20) -> str:\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as fh:\n",
    "        while True:\n",
    "            b = fh.read(chunk)\n",
    "            if not b: break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _choose_val_subject(all_subj: list[int], held_out: int) -> int:\n",
    "    others = sorted([s for s in all_subj if s != held_out])\n",
    "    if not others:\n",
    "        return held_out\n",
    "    if VAL_SELECTION_MODE == \"fixed\" and VAL_SUBJECT_ID in others and VAL_SUBJECT_ID != held_out:\n",
    "        return VAL_SUBJECT_ID\n",
    "    return others[0]\n",
    "\n",
    "def _onsets_from_active(active: np.ndarray) -> np.ndarray:\n",
    "    x = (active.astype(int) > 0).astype(int)\n",
    "    d = np.diff(np.r_[0, x])\n",
    "    return np.where(d == 1)[0]\n",
    "\n",
    "def _to_int_safe(x):\n",
    "    return int(pd.to_numeric(x, errors=\"coerce\"))\n",
    "\n",
    "def _window_rows_for_file(file_path: Path,\n",
    "                          df: pd.DataFrame,\n",
    "                          fs: float,\n",
    "                          fold_id: int,\n",
    "                          split_tag: str) -> list[dict]:\n",
    "    \"\"\"Build sliding and onset-anchor rows using label_action and task_target columns.\"\"\"\n",
    "    rows = []\n",
    "    t = pd.to_numeric(df[\"Timestamp_seconds\"], errors=\"coerce\").to_numpy()\n",
    "    n = len(df)\n",
    "    if n < 2:\n",
    "        return rows\n",
    "\n",
    "    subj = _to_int_safe(df[\"subject_id\"].iloc[0])\n",
    "    task_code = _to_int_safe(df[\"task\"].iloc[0])\n",
    "    trial_id  = _to_int_safe(df[\"trial\"].iloc[0])\n",
    "    active = df[\"active\"].fillna(0).astype(int).to_numpy()\n",
    "    \n",
    "    # Use the actual label_action and task_target columns if available\n",
    "    has_label_action = \"label_action\" in df.columns\n",
    "    has_task_target = \"task_target\" in df.columns\n",
    "    \n",
    "    if has_label_action:\n",
    "        label_action_col = df[\"label_action\"].fillna(0).astype(int).to_numpy()\n",
    "    if has_task_target:\n",
    "        task_target_col = df[\"task_target\"].fillna(0).astype(int).to_numpy()\n",
    "\n",
    "    win_n    = max(1, int(round(SLIDE_WIN_S    * fs)))\n",
    "    stride_n = max(1, int(round(SLIDE_STRIDE_S * fs)))\n",
    "    pre_n    = max(0, int(round(ANCHOR_PRE_S   * fs)))\n",
    "    post_n   = max(1, int(round(ANCHOR_POST_S  * fs)))\n",
    "\n",
    "    # Sliding windows - use actual labels if available\n",
    "    for s in range(0, n - win_n + 1, stride_n):\n",
    "        e = s + win_n\n",
    "        \n",
    "        if has_label_action and has_task_target:\n",
    "            # Use the actual labels from the data\n",
    "            window_labels = label_action_col[s:e]\n",
    "            window_tasks = task_target_col[s:e]\n",
    "            \n",
    "            # Use majority vote for label_action\n",
    "            if len(window_labels) > 0:\n",
    "                label_action = 1 if np.mean(window_labels) >= ACTIVE_MAJ_FRAC else 0\n",
    "            else:\n",
    "                label_action = 0\n",
    "                \n",
    "            # Use most common non-zero task, or 0 if no task\n",
    "            non_zero_tasks = window_tasks[window_tasks != 0]\n",
    "            if len(non_zero_tasks) > 0:\n",
    "                task_target = np.bincount(non_zero_tasks).argmax()\n",
    "            else:\n",
    "                task_target = 0\n",
    "        else:\n",
    "            # Fallback to old method using active column\n",
    "            frac_act = float(np.mean(active[s:e])) if e > s else 0.0\n",
    "            label_action = 1 if frac_act >= ACTIVE_MAJ_FRAC else 0\n",
    "            task_target = int(task_code) if label_action == 1 else 0\n",
    "\n",
    "        rows.append(dict(\n",
    "            file=str(file_path.resolve()),\n",
    "            subject_id=subj,\n",
    "            split=split_tag,\n",
    "            type=\"sliding\",\n",
    "            task_target=int(task_target),\n",
    "            label_action=int(label_action),\n",
    "            start_idx=int(s),\n",
    "            end_idx=int(e),\n",
    "            task_code=int(task_code),\n",
    "            trial_id=int(trial_id),\n",
    "            fold_id=int(fold_id),\n",
    "        ))\n",
    "\n",
    "    # Onset-anchor windows - use actual label_action for onsets if available\n",
    "    if has_label_action:\n",
    "        onsets = _onsets_from_active(label_action_col)\n",
    "    else:\n",
    "        onsets = _onsets_from_active(active)\n",
    "        \n",
    "    for o in onsets:\n",
    "        s = o - pre_n\n",
    "        e = o + post_n\n",
    "        if s < 0 or e > n or e <= s:\n",
    "            continue\n",
    "            \n",
    "        # For onset windows, use the task at the onset point\n",
    "        if has_task_target and e <= len(task_target_col):\n",
    "            onset_task = task_target_col[o]\n",
    "        else:\n",
    "            onset_task = task_code\n",
    "\n",
    "        rows.append(dict(\n",
    "            file=str(file_path.resolve()),\n",
    "            subject_id=subj,\n",
    "            split=split_tag,\n",
    "            type=\"onset_anchor\",\n",
    "            task_target=int(onset_task),\n",
    "            label_action=1,\n",
    "            start_idx=int(s),\n",
    "            end_idx=int(e),\n",
    "            task_code=int(task_code),\n",
    "            trial_id=int(trial_id),\n",
    "            fold_id=int(fold_id),\n",
    "        ))\n",
    "    return rows\n",
    "\n",
    "def analyze_class_distribution(splits_df: pd.DataFrame):\n",
    "    \"\"\"Analyze class distribution across subjects and splits\"\"\"\n",
    "    print(\"\\n=== CLASS DISTRIBUTION ANALYSIS ===\")\n",
    "    \n",
    "    # Overall distribution\n",
    "    overall_action = splits_df[\"label_action\"].value_counts().sort_index()\n",
    "    overall_task = splits_df[\"task_target\"].value_counts().sort_index()\n",
    "    \n",
    "    print(f\"Overall label_action distribution:\")\n",
    "    for label, count in overall_action.items():\n",
    "        print(f\"  Class {label}: {count} samples ({count/len(splits_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nOverall task_target distribution:\")\n",
    "    for task, count in overall_task.items():\n",
    "        print(f\"  Task {task}: {count} samples ({count/len(splits_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Per-subject distribution\n",
    "    subjects = splits_df[\"subject_id\"].unique()\n",
    "    print(f\"\\nPer-subject distribution ({len(subjects)} subjects):\")\n",
    "    \n",
    "    subject_stats = []\n",
    "    for subject in sorted(subjects):\n",
    "        subject_data = splits_df[splits_df[\"subject_id\"] == subject]\n",
    "        action_dist = subject_data[\"label_action\"].value_counts().sort_index()\n",
    "        task_dist = subject_data[\"task_target\"].value_counts().sort_index()\n",
    "        \n",
    "        action_0 = action_dist.get(0, 0)\n",
    "        action_1 = action_dist.get(1, 0)\n",
    "        total_actions = action_0 + action_1\n",
    "        \n",
    "        subject_stats.append({\n",
    "            \"subject_id\": subject,\n",
    "            \"total_samples\": len(subject_data),\n",
    "            \"action_0\": action_0,\n",
    "            \"action_1\": action_1,\n",
    "            \"action_1_ratio\": action_1 / total_actions if total_actions > 0 else 0,\n",
    "            \"unique_tasks\": len(task_dist)\n",
    "        })\n",
    "        \n",
    "        if subject <= 5:  # Show first 5 subjects as sample\n",
    "            print(f\"  Subject {subject}: {len(subject_data)} samples, \"\n",
    "                  f\"action=0: {action_0}, action=1: {action_1} \"\n",
    "                  f\"({action_1/total_actions*100:.1f}% positive)\")\n",
    "    \n",
    "    # Per-split distribution\n",
    "    print(f\"\\nPer-split distribution:\")\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        split_data = splits_df[splits_df[\"split\"] == split]\n",
    "        if len(split_data) > 0:\n",
    "            action_dist = split_data[\"label_action\"].value_counts().sort_index()\n",
    "            action_0 = action_dist.get(0, 0)\n",
    "            action_1 = action_dist.get(1, 0)\n",
    "            total = action_0 + action_1\n",
    "            print(f\"  {split}: {len(split_data)} samples, \"\n",
    "                  f\"action=0: {action_0}, action=1: {action_1} \"\n",
    "                  f\"({action_1/total*100:.1f}% positive)\")\n",
    "    \n",
    "    return subject_stats\n",
    "\n",
    "def balance_classes_stratified(splits_df: pd.DataFrame, min_samples_per_class=100):\n",
    "    \"\"\"Balance classes using stratified sampling per subject\"\"\"\n",
    "    print(\"\\n=== APPLYING CLASS BALANCING ===\")\n",
    "    \n",
    "    balanced_splits = []\n",
    "    \n",
    "    for subject in splits_df[\"subject_id\"].unique():\n",
    "        subject_data = splits_df[splits_df[\"subject_id\"] == subject]\n",
    "        \n",
    "        # Get class distribution for this subject\n",
    "        class_counts = subject_data[\"label_action\"].value_counts()\n",
    "        min_class_count = class_counts.min()\n",
    "        \n",
    "        # If we have very few samples in one class, use min_samples_per_class\n",
    "        target_count = max(min_samples_per_class, min_class_count)\n",
    "        \n",
    "        # Stratified sampling per class\n",
    "        subject_balanced = []\n",
    "        for class_label in class_counts.index:\n",
    "            class_data = subject_data[subject_data[\"label_action\"] == class_label]\n",
    "            if len(class_data) > target_count:\n",
    "                # Undersample majority class\n",
    "                class_data = class_data.sample(n=target_count, random_state=42)\n",
    "            subject_balanced.append(class_data)\n",
    "        \n",
    "        balanced_subject = pd.concat(subject_balanced, ignore_index=True)\n",
    "        balanced_splits.append(balanced_subject)\n",
    "        \n",
    "        print(f\"  Subject {subject}: {len(subject_data)} → {len(balanced_subject)} samples\")\n",
    "    \n",
    "    balanced_df = pd.concat(balanced_splits, ignore_index=True)\n",
    "    \n",
    "    # Verify balancing\n",
    "    print(\"\\nAfter balancing:\")\n",
    "    overall_action = balanced_df[\"label_action\"].value_counts().sort_index()\n",
    "    for label, count in overall_action.items():\n",
    "        print(f\"  Class {label}: {count} samples ({count/len(balanced_df)*100:.1f}%)\")\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "# ---------------- MAIN ----------------\n",
    "files = _collect_files()\n",
    "if not files:\n",
    "    raise SystemExit(\"[stop] No label CSVs found in any subject's labelonly/\")\n",
    "\n",
    "# Manifest: one row per file\n",
    "manifest_rows, bad = [], []\n",
    "for p in files:\n",
    "    try:\n",
    "        df = pd.read_csv(p, low_memory=False, usecols=lambda c: c in set(REQ))\n",
    "        if not all(c in df.columns for c in REQ):\n",
    "            bad.append((str(p), f\"missing cols: {[c for c in REQ if c not in df.columns]}\"))\n",
    "            continue\n",
    "\n",
    "        t  = pd.to_numeric(df[\"Timestamp_seconds\"], errors=\"coerce\").to_numpy()\n",
    "        fs = _median_fs(t)\n",
    "        dur = float((t[-1] - t[0])) if len(t) else 0.0\n",
    "\n",
    "        subj  = _to_int_safe(df[\"subject_id\"].iloc[0])\n",
    "        task  = _to_int_safe(df[\"task\"].iloc[0])\n",
    "        trial = _to_int_safe(df[\"trial\"].iloc[0])\n",
    "\n",
    "        row = dict(\n",
    "            file=str(p.resolve()),\n",
    "            subject_id=subj,\n",
    "            fs_hz=round(fs, 6) if np.isfinite(fs) else np.nan,\n",
    "            duration_s=round(dur, 6),\n",
    "            task_code=task,\n",
    "            trial_id=trial,\n",
    "            fold_id=subj,   # LOSO: fold id = subject id\n",
    "        )\n",
    "        if WRITE_FILE_MD5:\n",
    "            row[\"md5\"] = _file_md5(p)\n",
    "        manifest_rows.append(row)\n",
    "    except Exception as e:\n",
    "        bad.append((str(p), str(e)))\n",
    "\n",
    "manifest = pd.DataFrame(manifest_rows).sort_values([\"subject_id\",\"file\"]).reset_index(drop=True)\n",
    "manifest.to_csv(MANIFEST_OUT, index=False)\n",
    "print(f\"[ok] manifest → {MANIFEST_OUT}  ({len(manifest)} ok, {len(bad)} skipped)\")\n",
    "\n",
    "# TRUE LOSO splits - ensure no data leakage\n",
    "subjects = sorted(manifest[\"subject_id\"].unique().tolist())\n",
    "print(f\"\\nFound {len(subjects)} subjects: {subjects}\")\n",
    "\n",
    "splits_rows = []\n",
    "\n",
    "for test_subject in subjects:\n",
    "    print(f\"\\nProcessing TRUE LOSO fold: Subject {test_subject} as test\")\n",
    "    \n",
    "    # Training subjects: all except test subject\n",
    "    train_subjects = [s for s in subjects if s != test_subject]\n",
    "    \n",
    "    # Choose validation subject from training subjects\n",
    "    val_subject = _choose_val_subject(train_subjects, test_subject)\n",
    "    \n",
    "    # Split mapping: test_subject=test, val_subject=val, others=train\n",
    "    split_for_subj = {}\n",
    "    for s in subjects:\n",
    "        if s == test_subject:\n",
    "            split_for_subj[s] = \"test\"\n",
    "        elif s == val_subject:\n",
    "            split_for_subj[s] = \"val\" \n",
    "        else:\n",
    "            split_for_subj[s] = \"train\"\n",
    "    \n",
    "    print(f\"  Train subjects: {[s for s in train_subjects if s != val_subject]}\")\n",
    "    print(f\"  Val subject: {val_subject}\")\n",
    "    print(f\"  Test subject: {test_subject}\")\n",
    "\n",
    "    for file_path_str, subj in manifest[[\"file\",\"subject_id\"]].to_records(index=False):\n",
    "        split_tag = split_for_subj[int(subj)]\n",
    "        p = Path(file_path_str)\n",
    "        try:\n",
    "            df = pd.read_csv(p, low_memory=False, usecols=lambda c: c in set(REQ))\n",
    "            if not all(c in df.columns for c in REQ):\n",
    "                print(f\"[warn] skipping (missing cols) {p}\")\n",
    "                continue\n",
    "\n",
    "            t  = pd.to_numeric(df[\"Timestamp_seconds\"], errors=\"coerce\").to_numpy()\n",
    "            fs = _median_fs(t)\n",
    "            if not np.isfinite(fs) or fs <= 0:\n",
    "                print(f\"[warn] fs not finite for {p}; skipping\")\n",
    "                continue\n",
    "\n",
    "            rows = _window_rows_for_file(p, df, fs, fold_id=test_subject, split_tag=split_tag)\n",
    "            splits_rows.extend(rows)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] windowing failed for {p}: {e}\")\n",
    "\n",
    "splits = pd.DataFrame(splits_rows)\n",
    "\n",
    "# Analyze class distribution\n",
    "subject_stats = analyze_class_distribution(splits)\n",
    "\n",
    "# Apply class balancing\n",
    "splits_balanced = balance_classes_stratified(splits, min_samples_per_class=100)\n",
    "\n",
    "# Column order for downstream\n",
    "ordered = [\"file\",\"subject_id\",\"split\",\"type\",\"task_target\",\"label_action\",\n",
    "           \"start_idx\",\"end_idx\",\"task_code\",\"trial_id\",\"fold_id\"]\n",
    "extra = [c for c in splits_balanced.columns if c not in ordered]\n",
    "splits_final = splits_balanced[ordered + extra]\n",
    "\n",
    "splits_final.to_csv(SPLITS_OUT, index=False)\n",
    "print(f\"\\n[ok] splits  → {SPLITS_OUT}  ({len(splits_final)} windows across {len(subjects)} folds)\")\n",
    "\n",
    "# Save analysis report\n",
    "analysis_report = DATASET_DIR / \"dataset_analysis_report.txt\"\n",
    "with open(analysis_report, \"w\") as f:\n",
    "    f.write(\"DATASET ANALYSIS REPORT\\n\")\n",
    "    f.write(\"======================\\n\\n\")\n",
    "    f.write(f\"Total subjects: {len(subjects)}\\n\")\n",
    "    f.write(f\"Total windows: {len(splits_final)}\\n\")\n",
    "    f.write(f\"Subjects: {subjects}\\n\\n\")\n",
    "    \n",
    "    f.write(\"PER-SUBJECT STATISTICS:\\n\")\n",
    "    for stats in subject_stats:\n",
    "        f.write(f\"Subject {stats['subject_id']}: {stats['total_samples']} samples, \"\n",
    "                f\"action_1_ratio: {stats['action_1_ratio']:.3f}, \"\n",
    "                f\"unique_tasks: {stats['unique_tasks']}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nCLASS DISTRIBUTION AFTER BALANCING:\\n\")\n",
    "    action_dist = splits_final[\"label_action\"].value_counts().sort_index()\n",
    "    for label, count in action_dist.items():\n",
    "        f.write(f\"  Class {label}: {count} samples ({count/len(splits_final)*100:.1f}%)\\n\")\n",
    "\n",
    "print(f\"[info] analysis report → {analysis_report}\")\n",
    "\n",
    "if bad:\n",
    "    skipped = DATASET_DIR / \"manifest_v1_skipped.csv\"\n",
    "    pd.DataFrame(bad, columns=[\"file\",\"reason\"]).to_csv(skipped, index=False)\n",
    "    print(f\"[info] skipped manifest files → {skipped}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
