{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee62091-e161-4568-bc91-d9998c84ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# Phase 5 â€” LOSO Exporter ( BioRob)\n",
    "# =====================================================================\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "\n",
    "# NOTE: adjust ROOT_DIR so that DATASET_DIR matches what Phase 6 is using\n",
    "ROOT_DIR    = Path(r\"/home/tsultan1/BioRob(Final)/Data\")\n",
    "DATASET_DIR = ROOT_DIR / \"_dataset_icml_v1\"\n",
    "\n",
    "SPLITS_CSV  = DATASET_DIR / \"splits_v1.csv\"\n",
    "QC_CSV      = DATASET_DIR / \"qc_summary_v1.csv\"   # NEW: QC summary from Phase 4\n",
    "\n",
    "# We now support TWO export modes:\n",
    "#   1) balanced_supervised â†’ for Phase 6 supervised LOSO (ICML + BioRob)\n",
    "#   2) ssl_unbalanced      â†’ for SSL pretraining (ICML + BioRob)\n",
    "OUT_NAME_BALANCED = \"exports_v1_balanced\"   # indicates train balancing\n",
    "OUT_NAME_SSL      = \"exports_v1_ssl\"        # no balancing, full distribution\n",
    "\n",
    "EXPORT_BALANCED   = True\n",
    "EXPORT_SSL        = True\n",
    "\n",
    "# Window types used in splits_v1.csv\n",
    "WINDOW_TYPES = [\"sliding\", \"onset_anchor\"]\n",
    "\n",
    "# Modalities to include\n",
    "INCLUDE_EEG, INCLUDE_EMG, INCLUDE_ET = True, True, True\n",
    "\n",
    "# Shard size (number of windows per NPZ)\n",
    "SHARD_SIZE   = 5000\n",
    "\n",
    "# Fixed window length in samples (must match Phase 4 windowing, e.g. 500 @ 250 Hz = 2 s)\n",
    "FIXED_WINDOW_LEN = 500\n",
    "\n",
    "# Good coverage threshold for masks\n",
    "MIN_COVERAGE = 0.40\n",
    "\n",
    "# ---------------- TASK BALANCING CONFIG ----------------\n",
    "# Used only for TRAIN within each fold, in BALANCED mode\n",
    "#   - Limit Task 0 (rest) to at most 20% of TRAIN\n",
    "#   - Make all non-zero tasks have the same number of samples in TRAIN\n",
    "#     (over/under-sampling to a shared target_per_action)\n",
    "MAX_TASK0_RATIO  = 0.20\n",
    "MIN_TASK_SAMPLES = 50\n",
    "ACTION_MAX_FACTOR = 3   # max per-action cap = ACTION_MAX_FACTOR * MIN_TASK_SAMPLES\n",
    "\n",
    "# Optional caps for quick dry-runs (leave None for full dataset)\n",
    "LIMIT_TRAIN = LIMIT_VAL = LIMIT_TEST = None\n",
    "\n",
    "# ---------------- ANALYSIS / BALANCING HELPERS ----------------\n",
    "def analyze_task_distribution(splits_df: pd.DataFrame):\n",
    "    \"\"\"Analyze and display the current overall task distribution.\"\"\"\n",
    "    print(\"\\n=== CURRENT TASK DISTRIBUTION ANALYSIS (ALL SPLITS AFTER QC) ===\")\n",
    "\n",
    "    total_windows = len(splits_df)\n",
    "    task_dist = splits_df[\"task_target\"].value_counts().sort_index()\n",
    "\n",
    "    print(f\"Total windows: {total_windows}\")\n",
    "    print(\"\\nTask distribution (task_target):\")\n",
    "    for task, count in task_dist.items():\n",
    "        percentage = count / total_windows * 100\n",
    "        print(f\"  Task {task}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "    task0_ratio = task_dist.get(0, 0) / max(total_windows, 1)\n",
    "    if task0_ratio > MAX_TASK0_RATIO:\n",
    "        print(f\"\\nâš ï¸  WARNING: Task 0 (rest) dominates with {task0_ratio:.1%} of samples\")\n",
    "        print(f\"   Target maximum for Task 0 in TRAIN (balanced export): {MAX_TASK0_RATIO:.1%}\")\n",
    "\n",
    "    return task_dist\n",
    "\n",
    "def rebalance_split(split_data: pd.DataFrame, is_train: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rebalance a single split (train/val/test).\n",
    "    Used ONLY for TRAIN within each fold in the BALANCED supervised export.\n",
    "\n",
    "    New strategy (for TRAIN):\n",
    "      - Make all non-zero tasks (actions) have the SAME number of samples:\n",
    "          target_per_action = clip(max_count, [MIN_TASK_SAMPLES, MIN_TASK_SAMPLES * ACTION_MAX_FACTOR])\n",
    "        using over/under-sampling per action.\n",
    "\n",
    "      - Then set Task 0 (rest) to MAX_TASK0_RATIO of (rest + actions) via:\n",
    "          target_task0 = total_other_balanced * r / (1 - r)\n",
    "\n",
    "    VAL/TEST currently never call this (we keep their original distribution).\n",
    "    \"\"\"\n",
    "    task0_data       = split_data[split_data[\"task_target\"] == 0]\n",
    "    other_tasks_data = split_data[split_data[\"task_target\"] != 0]\n",
    "\n",
    "    if is_train:\n",
    "        target_task0_ratio = MAX_TASK0_RATIO\n",
    "        min_task_samples   = MIN_TASK_SAMPLES\n",
    "    else:\n",
    "        # Not really used now (we don't rebalance val/test), but kept for completeness\n",
    "        target_task0_ratio = min(MAX_TASK0_RATIO + 0.1, 0.3)\n",
    "        min_task_samples   = max(10, MIN_TASK_SAMPLES // 2)\n",
    "\n",
    "    if len(other_tasks_data) == 0:\n",
    "        # Degenerate case: only Task 0 present\n",
    "        return split_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # --- Step 1: equalize all non-zero action classes ---\n",
    "    other_task_dist = other_tasks_data[\"task_target\"].value_counts().sort_index()\n",
    "    max_cur = other_task_dist.max()\n",
    "    max_action_samples = min_task_samples * ACTION_MAX_FACTOR\n",
    "\n",
    "    # Shared target per action:\n",
    "    #   - at least MIN_TASK_SAMPLES\n",
    "    #   - at most min(max_cur, MAX_ACTION_SAMPLES)\n",
    "    target_per_action = max(min_task_samples, min(max_cur, max_action_samples))\n",
    "\n",
    "    print(\"  [rebalance_split] Action-task distribution BEFORE balancing:\")\n",
    "    for t, c in other_task_dist.items():\n",
    "        print(f\"    Task {t}: {c}\")\n",
    "\n",
    "    print(f\"  [rebalance_split] Using target_per_action = {target_per_action} per non-zero task\")\n",
    "\n",
    "    balanced_other = []\n",
    "    for task in other_task_dist.index:\n",
    "        task_data = other_tasks_data[other_tasks_data[\"task_target\"] == task]\n",
    "        cur = len(task_data)\n",
    "\n",
    "        if cur < target_per_action:\n",
    "            # Oversample up to target_per_action\n",
    "            balanced_task = task_data.sample(\n",
    "                n=target_per_action, random_state=42, replace=True\n",
    "            )\n",
    "        elif cur > target_per_action:\n",
    "            # Undersample down to target_per_action\n",
    "            balanced_task = task_data.sample(\n",
    "                n=target_per_action, random_state=42, replace=False\n",
    "            )\n",
    "        else:\n",
    "            balanced_task = task_data\n",
    "\n",
    "        balanced_other.append(balanced_task)\n",
    "\n",
    "    all_balanced_other = pd.concat(balanced_other, ignore_index=True)\n",
    "    total_other_balanced = len(all_balanced_other)\n",
    "\n",
    "    # --- Step 2: set Task 0 (rest) to respect MAX_TASK0_RATIO ---\n",
    "    if (1.0 - target_task0_ratio) > 0.0:\n",
    "        target_task0 = int(total_other_balanced * target_task0_ratio / (1.0 - target_task0_ratio))\n",
    "    else:\n",
    "        target_task0 = len(task0_data)\n",
    "\n",
    "    print(f\"  [rebalance_split] total_other_balanced = {total_other_balanced}\")\n",
    "    print(f\"  [rebalance_split] target_task0 (rest) = {target_task0} (ratio â‰ˆ {target_task0_ratio:.2f})\")\n",
    "\n",
    "    if len(task0_data) < target_task0:\n",
    "        # Oversample rest\n",
    "        task0_balanced = task0_data.sample(\n",
    "            n=target_task0, random_state=42, replace=True\n",
    "        )\n",
    "    elif len(task0_data) > target_task0:\n",
    "        # Undersample rest\n",
    "        task0_balanced = task0_data.sample(\n",
    "            n=target_task0, random_state=42, replace=False\n",
    "        )\n",
    "    else:\n",
    "        task0_balanced = task0_data\n",
    "\n",
    "    # Concatenate rest + balanced actions\n",
    "    split_balanced = pd.concat([task0_balanced, all_balanced_other], ignore_index=True)\n",
    "\n",
    "    # Shuffle\n",
    "    split_balanced = split_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Debug print AFTER balancing\n",
    "    dist = split_balanced[\"task_target\"].value_counts().sort_index()\n",
    "    print(\"  [rebalance_split] New TRAIN distribution (after balancing):\")\n",
    "    for t, c in dist.items():\n",
    "        print(f\"    Task {t}: {c}\")\n",
    "\n",
    "    return split_balanced\n",
    "\n",
    "# ---------------- ORIGINAL PHASE 5 FUNCTIONS (mostly unchanged) ----------------\n",
    "EEG_KEY      = \"EEG\"\n",
    "EEG_MASK_KEY = \"EEG_mask\"\n",
    "EEG_CH_KEY   = \"EEG_ch\"\n",
    "\n",
    "EMG_KEY      = \"EMG_env\"\n",
    "EMG_MASK_KEY = \"EMG_mask\"\n",
    "EMG_CH_KEY   = \"EMG_ch\"\n",
    "\n",
    "ET_KEY       = \"ET\"\n",
    "ET_MASK_KEY  = \"ET_mask\"\n",
    "ET_CH_KEY    = \"ET_ch\"\n",
    "\n",
    "NPZ_SUFFIX = \".preproc.npz\"\n",
    "\n",
    "def _ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _load_npz(npz_path: Path) -> dict:\n",
    "    with np.load(npz_path, allow_pickle=True) as z:\n",
    "        out = {k: z[k] for k in z.files}\n",
    "    # convert object arrays of channel names back to list[str]\n",
    "    for k in (EEG_CH_KEY, EMG_CH_KEY, ET_CH_KEY):\n",
    "        if k in out:\n",
    "            out[k] = [str(x) for x in out[k].tolist()]\n",
    "    return out\n",
    "\n",
    "class NPZCacheLRU:\n",
    "    def __init__(self, capacity=16):\n",
    "        self.capacity = int(capacity)\n",
    "        self._d: OrderedDict[str, dict] = OrderedDict()\n",
    "    def get(self, csv_path: str):\n",
    "        key = csv_path + NPZ_SUFFIX\n",
    "        if key in self._d:\n",
    "            self._d.move_to_end(key)\n",
    "            return self._d[key]\n",
    "        npz_path = Path(csv_path + NPZ_SUFFIX)\n",
    "        if not npz_path.exists():\n",
    "            raise FileNotFoundError(f\"cache missing: {npz_path}\")\n",
    "        val = _load_npz(npz_path)\n",
    "        self._d[key] = val\n",
    "        self._d.move_to_end(key)\n",
    "        if len(self._d) > self.capacity:\n",
    "            self._d.popitem(last=False)\n",
    "        return val\n",
    "    def clear(self):\n",
    "        self._d.clear()\n",
    "\n",
    "def _apply_window(arr: np.ndarray, s: int, e: int) -> np.ndarray:\n",
    "    s = max(0, int(s)); e = min(int(e), arr.shape[0])\n",
    "    if e <= s:\n",
    "        return arr[:0]\n",
    "    return arr[s:e]\n",
    "\n",
    "def _accumulate_stats(stats, X: np.ndarray, M: np.ndarray):\n",
    "    \"\"\"Accumulate per-channel sums/sumsq/count across time using mask (0/1).\"\"\"\n",
    "    if X.size == 0:\n",
    "        return\n",
    "    # Extra guard: avoid any NaNs/Infs sneaking into stats\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    m = M.astype(np.float32)\n",
    "    cnt = m.sum(axis=0)                 # (C,)\n",
    "    if cnt.sum() == 0:\n",
    "        return\n",
    "    stats[\"cnt\"]   += cnt\n",
    "    stats[\"sum\"]   += (X * m).sum(axis=0)\n",
    "    stats[\"sum2\"]  += ((X ** 2) * m).sum(axis=0)\n",
    "\n",
    "def _finalize_stats(stats):\n",
    "    cnt  = np.maximum(stats[\"cnt\"], 1.0)\n",
    "    mean = stats[\"sum\"] / cnt\n",
    "    var  = np.maximum(stats[\"sum2\"] / cnt - mean**2, 0.0)\n",
    "    std  = np.sqrt(var) + 1e-8  # Slightly larger epsilon for stability\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "def _new_stats(nc: int):\n",
    "    return {\"cnt\":  np.zeros(nc, dtype=np.float64),\n",
    "            \"sum\":  np.zeros(nc, dtype=np.float64),\n",
    "            \"sum2\": np.zeros(nc, dtype=np.float64)}\n",
    "\n",
    "def _normalize(X: np.ndarray, M: np.ndarray, mean: np.ndarray, std: np.ndarray):\n",
    "    if X.size == 0:\n",
    "        return X.astype(np.float32)\n",
    "    Y = (X - mean[None, :]) / std[None, :]\n",
    "    # where mask==0, set to 0 to avoid injecting arbitrary values\n",
    "    Y[M <= 0.0] = 0.0\n",
    "    # Hard guard: no NaN/Inf going to model\n",
    "    Y = np.nan_to_num(Y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return Y.astype(np.float32)\n",
    "\n",
    "def _coverage_ok(M: np.ndarray, thr: float) -> bool:\n",
    "    if M.size == 0:\n",
    "        return True\n",
    "    cov = M.mean(axis=0)  # coverage per channel\n",
    "    return bool((cov >= thr).mean() > 0.5)  # majority of channels OK\n",
    "\n",
    "def _row_limit(split: str):\n",
    "    if split == \"train\" and LIMIT_TRAIN:\n",
    "        return LIMIT_TRAIN\n",
    "    if split == \"val\"   and LIMIT_VAL:\n",
    "        return LIMIT_VAL\n",
    "    if split == \"test\"  and LIMIT_TEST:\n",
    "        return LIMIT_TEST\n",
    "    return None\n",
    "\n",
    "def _pad_or_trim_time(X: np.ndarray, target_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ensure X has shape (target_len, C):\n",
    "      - If longer, truncate.\n",
    "      - If shorter, pad with zeros at the end.\n",
    "    \"\"\"\n",
    "    if X is None or X.size == 0:\n",
    "        return np.zeros((target_len, 0), dtype=np.float32)\n",
    "\n",
    "    T, C = X.shape\n",
    "    if T == target_len:\n",
    "        return X\n",
    "    if T > target_len:\n",
    "        return X[:target_len]\n",
    "    # T < target_len â†’ pad\n",
    "    pad = np.zeros((target_len - T, C), dtype=X.dtype)\n",
    "    return np.concatenate([X, pad], axis=0)\n",
    "\n",
    "# ---------------- EXPORT CORE ----------------\n",
    "def export_fold(splits: pd.DataFrame, fold_id: int, out_root: Path, cache_cap=12, mode: str = \"balanced\"):\n",
    "    \"\"\"\n",
    "    Export one fold in either:\n",
    "        mode = \"balanced\"  â†’ supervised, with TRAIN balancing\n",
    "        mode = \"ssl\"       â†’ SSL, NO balancing (original splits_v1 distribution)\n",
    "    Labels are ALWAYS exported (y_action, y_task) in both modes.\n",
    "    \"\"\"\n",
    "    if mode not in (\"balanced\", \"ssl\"):\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "    is_balanced = (mode == \"balanced\")\n",
    "    out_name    = OUT_NAME_BALANCED if is_balanced else OUT_NAME_SSL\n",
    "\n",
    "    mode_label = \"BALANCED (supervised)\" if is_balanced else \"SSL (unbalanced)\"\n",
    "    print(f\"\\n========== FOLD {fold_id} â€” export mode: {mode_label} ==========\")\n",
    "\n",
    "    fold_rows = splits[splits[\"fold_id\"] == fold_id]\n",
    "    if fold_rows.empty:\n",
    "        print(f\"[skip] fold {fold_id}: no rows\")\n",
    "        return\n",
    "\n",
    "    # Keep only desired window types\n",
    "    fold_rows = fold_rows[fold_rows[\"type\"].isin(WINDOW_TYPES)]\n",
    "    if fold_rows.empty:\n",
    "        print(f\"[skip] fold {fold_id}: no rows after WINDOW_TYPES filter {WINDOW_TYPES}\")\n",
    "        return\n",
    "\n",
    "    cache = NPZCacheLRU(cache_cap)\n",
    "\n",
    "    # ---- PASS 1: accumulate TRAIN stats (UNBALANCED) ----\n",
    "    tr_rows_stats = fold_rows[fold_rows[\"split\"] == \"train\"]\n",
    "    if tr_rows_stats.empty:\n",
    "        print(f\"[warn] fold {fold_id}: no TRAIN rows; skipping\")\n",
    "        return\n",
    "\n",
    "    # limit for speed if requested\n",
    "    lim = _row_limit(\"train\")\n",
    "    if lim:\n",
    "        tr_rows_stats = tr_rows_stats.iloc[:lim]\n",
    "\n",
    "    # Analyze fold TRAIN distribution before processing\n",
    "    print(f\"[fold {fold_id}] Pre-export TRAIN task distribution (mode={mode}):\")\n",
    "    fold_dist = tr_rows_stats[\"task_target\"].value_counts().sort_index()\n",
    "    for task, count in fold_dist.items():\n",
    "        print(f\"  Task {task}: {count} samples\")\n",
    "\n",
    "    # Determine channel counts from a sample cache\n",
    "    stats = {}\n",
    "    def _ensure_stat(key, nchan):\n",
    "        if key not in stats:\n",
    "            stats[key] = _new_stats(nchan)\n",
    "\n",
    "    print(f\"[fold {fold_id}] pass1: accumulating TRAIN stats over {len(tr_rows_stats):,} windows â€¦\")\n",
    "    for r in tr_rows_stats.itertuples(index=False):\n",
    "        npz = cache.get(r.file)\n",
    "        s, e = int(r.start_idx), int(r.end_idx)\n",
    "\n",
    "        # EEG\n",
    "        if INCLUDE_EEG and EEG_KEY in npz and EEG_MASK_KEY in npz:\n",
    "            Xe = _apply_window(npz[EEG_KEY], s, e)\n",
    "            Me = _apply_window(npz[EEG_MASK_KEY], s, e)\n",
    "            if Xe.size and _coverage_ok(Me, MIN_COVERAGE):\n",
    "                _ensure_stat(EEG_KEY, Xe.shape[1])\n",
    "                _accumulate_stats(stats[EEG_KEY], Xe, Me)\n",
    "\n",
    "        # EMG\n",
    "        if INCLUDE_EMG and EMG_KEY in npz and EMG_MASK_KEY in npz:\n",
    "            Xm = _apply_window(npz[EMG_KEY], s, e)\n",
    "            Mm = _apply_window(npz[EMG_MASK_KEY], s, e)\n",
    "            if Xm.size and _coverage_ok(Mm, MIN_COVERAGE):\n",
    "                _ensure_stat(EMG_KEY, Xm.shape[1])\n",
    "                _accumulate_stats(stats[EMG_KEY], Xm, Mm)\n",
    "\n",
    "        # ET\n",
    "        if INCLUDE_ET and ET_KEY in npz and ET_MASK_KEY in npz:\n",
    "            Xt = _apply_window(npz[ET_KEY], s, e)\n",
    "            Mt = _apply_window(npz[ET_MASK_KEY], s, e)\n",
    "            if Xt.size and _coverage_ok(Mt, MIN_COVERAGE):\n",
    "                _ensure_stat(ET_KEY, Xt.shape[1])\n",
    "                _accumulate_stats(stats[ET_KEY], Xt, Mt)\n",
    "\n",
    "    # Finalize stats\n",
    "    means, stds = {}, {}\n",
    "    for key in (EEG_KEY, EMG_KEY, ET_KEY):\n",
    "        if key in stats:\n",
    "            m, s = _finalize_stats(stats[key])\n",
    "            means[key], stds[key] = m, s\n",
    "\n",
    "    # Save per-fold stats (channel names included)\n",
    "    fold_dir = out_root / f\"{out_name}_fold{fold_id}\"\n",
    "    _ensure_dir(fold_dir)\n",
    "    meta = {\n",
    "        \"fold_id\": int(fold_id),\n",
    "        \"export_mode\": mode,   # \"balanced\" or \"ssl\"\n",
    "        \"window_types\": WINDOW_TYPES,\n",
    "        \"fixed_window_len\": FIXED_WINDOW_LEN,\n",
    "        \"min_coverage\": MIN_COVERAGE,\n",
    "        \"include\": {\"EEG\": INCLUDE_EEG, \"EMG\": INCLUDE_EMG, \"ET\": INCLUDE_ET},\n",
    "        \"balancing_info\": {},\n",
    "        \"channels\": {},\n",
    "        \"stats\": {}\n",
    "    }\n",
    "    if is_balanced:\n",
    "        meta[\"balancing_info\"] = {\n",
    "            \"max_task0_ratio_train\": MAX_TASK0_RATIO,\n",
    "            \"min_task_samples_train\": MIN_TASK_SAMPLES,\n",
    "            \"action_max_factor\": ACTION_MAX_FACTOR,\n",
    "            \"note\": (\n",
    "                \"Balancing applied ONLY to TRAIN split in this export: \"\n",
    "                \"all non-zero tasks are equalized to target_per_action; \"\n",
    "                \"Task 0 (rest) capped via MAX_TASK0_RATIO.\"\n",
    "            ),\n",
    "        }\n",
    "    else:\n",
    "        meta[\"balancing_info\"] = {\n",
    "            \"note\": \"NO within-fold balancing; keeps original splits_v1 distribution.\"\n",
    "        }\n",
    "\n",
    "    # get channel lists from any cached file\n",
    "    sample_npz = cache.get(tr_rows_stats.iloc[0].file)\n",
    "    if INCLUDE_EEG and EEG_CH_KEY in sample_npz:\n",
    "        meta[\"channels\"][\"EEG\"] = sample_npz[EEG_CH_KEY]\n",
    "    if INCLUDE_EMG and EMG_CH_KEY in sample_npz:\n",
    "        meta[\"channels\"][\"EMG\"] = sample_npz[EMG_CH_KEY]\n",
    "    if INCLUDE_ET  and ET_CH_KEY  in sample_npz:\n",
    "        meta[\"channels\"][\"ET\"]  = sample_npz[ET_CH_KEY]\n",
    "    for key in (EEG_KEY, EMG_KEY, ET_KEY):\n",
    "        if key in means:\n",
    "            meta[\"stats\"][key] = {\"mean\": means[key].tolist(), \"std\": stds[key].tolist()}\n",
    "    (fold_dir / \"stats_fold.json\").write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "    # ---- PASS 2: write normalized shards for all splits ----\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        rows = fold_rows[fold_rows[\"split\"] == split]\n",
    "        if rows.empty:\n",
    "            print(f\"[fold {fold_id}] no rows for {split}\")\n",
    "            continue\n",
    "        lim = _row_limit(split)\n",
    "        if lim:\n",
    "            rows = rows.iloc[:lim]\n",
    "\n",
    "        # BALANCED export: apply balancing ONLY to TRAIN\n",
    "        # SSL export: NEVER balance, keep original distribution\n",
    "        if split == \"train\" and is_balanced:\n",
    "            print(f\"[fold {fold_id}] applying within-fold TRAIN balancing (mode=balanced) â€¦\")\n",
    "            rows = rebalance_split(rows, is_train=True)\n",
    "        else:\n",
    "            print(f\"[fold {fold_id}] keeping original {split} distribution (mode={mode}, no balancing)\")\n",
    "\n",
    "        out_split = fold_dir / split\n",
    "        _ensure_dir(out_split)\n",
    "\n",
    "        # reset shard buffers\n",
    "        shard_idx = 1\n",
    "        buf = defaultdict(list)  # keys: X_EEG, X_EMG, X_ET, y_action, y_task, meta\n",
    "        total_windows = 0\n",
    "\n",
    "        def _flush():\n",
    "            nonlocal shard_idx, buf\n",
    "            if not buf[\"y_action\"]:  # empty shard\n",
    "                return\n",
    "            shard_path = out_split / f\"{split}_shard_{shard_idx:04d}.npz\"\n",
    "            np.savez_compressed(\n",
    "                shard_path,\n",
    "                # Stacked arrays per modality (N, T, C)\n",
    "                X_EEG=np.stack(buf[\"X_EEG\"]) if buf[\"X_EEG\"] else np.zeros((0, 1, 0), np.float32),\n",
    "                X_EMG=np.stack(buf[\"X_EMG\"]) if buf[\"X_EMG\"] else np.zeros((0, 1, 0), np.float32),\n",
    "                X_ET =np.stack(buf[\"X_ET\" ]) if buf[\"X_ET\" ] else np.zeros((0, 1, 0), np.float32),\n",
    "                # Labels (still present in SSL; SSL just ignores them at training time)\n",
    "                y_action = np.asarray(buf[\"y_action\"], dtype=np.int8),\n",
    "                y_task   = np.asarray(buf[\"y_task\"],   dtype=np.int16),\n",
    "                # Optional metadata (can help debugging or SSL sampling)\n",
    "                subject_id = np.asarray(buf[\"subject_id\"], dtype=np.int16),\n",
    "                task_code  = np.asarray(buf[\"task_code\"],  dtype=np.int16),\n",
    "                trial_id   = np.asarray(buf[\"trial_id\"],   dtype=np.int16),\n",
    "                win_type   = np.asarray(buf[\"win_type\"],   dtype=object),\n",
    "                # Save window lengths for safety\n",
    "                win_len    = np.asarray(buf[\"win_len\"],    dtype=np.int32),\n",
    "            )\n",
    "            shard_idx += 1\n",
    "            buf = defaultdict(list)\n",
    "\n",
    "        print(f\"[fold {fold_id}] pass2: exporting {split} ({len(rows):,} windows, mode={mode}) â€¦\")\n",
    "\n",
    "        # Print split distribution (after balancing for TRAIN if mode=balanced)\n",
    "        split_dist = rows[\"task_target\"].value_counts().sort_index()\n",
    "        print(f\"  {split} distribution (task_target):\")\n",
    "        for task, count in split_dist.items():\n",
    "            print(f\"    Task {task}: {count} samples\")\n",
    "\n",
    "        for r in rows.itertuples(index=False):\n",
    "            npz = cache.get(r.file)\n",
    "            s, e = int(r.start_idx), int(r.end_idx)\n",
    "            win_len = max(0, e - s)  # original logical length (for metadata only)\n",
    "\n",
    "            # Modality windows (T, C)\n",
    "            Xe = Me = Xm = Mm = Xt = Mt = None\n",
    "\n",
    "            if INCLUDE_EEG and EEG_KEY in npz and EEG_MASK_KEY in npz and EEG_KEY in means:\n",
    "                Xe = _apply_window(npz[EEG_KEY], s, e)\n",
    "                Me = _apply_window(npz[EEG_MASK_KEY], s, e)\n",
    "                if Xe.size == 0 or not _coverage_ok(Me, MIN_COVERAGE):\n",
    "                    Xe = None\n",
    "                else:\n",
    "                    Xe = _normalize(Xe, Me, means[EEG_KEY], stds[EEG_KEY])\n",
    "\n",
    "            if INCLUDE_EMG and EMG_KEY in npz and EMG_MASK_KEY in npz and EMG_KEY in means:\n",
    "                Xm = _apply_window(npz[EMG_KEY], s, e)\n",
    "                Mm = _apply_window(npz[EMG_MASK_KEY], s, e)\n",
    "                if Xm.size == 0 or not _coverage_ok(Mm, MIN_COVERAGE):\n",
    "                    Xm = None\n",
    "                else:\n",
    "                    Xm = _normalize(Xm, Mm, means[EMG_KEY], stds[EMG_KEY])\n",
    "\n",
    "            if INCLUDE_ET and ET_KEY in npz and ET_MASK_KEY in npz and ET_KEY in means:\n",
    "                Xt = _apply_window(npz[ET_KEY], s, e)\n",
    "                Mt = _apply_window(npz[ET_MASK_KEY], s, e)\n",
    "                if Xt.size == 0 or not _coverage_ok(Mt, MIN_COVERAGE):\n",
    "                    Xt = None\n",
    "                else:\n",
    "                    Xt = _normalize(Xt, Mt, means[ET_KEY], stds[ET_KEY])\n",
    "\n",
    "            # If no modality survives coverage/stat checks, skip this window\n",
    "            if (Xe is None) and (Xm is None) and (Xt is None):\n",
    "                continue\n",
    "\n",
    "            # Enforce FIXED_WINDOW_LEN for all modalities\n",
    "            T_len = FIXED_WINDOW_LEN\n",
    "\n",
    "            def _pad_trim_if_not_none(X):\n",
    "                if X is None:\n",
    "                    return None\n",
    "                return _pad_or_trim_time(X, T_len)\n",
    "\n",
    "            Xe = _pad_trim_if_not_none(Xe)\n",
    "            Xm = _pad_trim_if_not_none(Xm)\n",
    "            Xt = _pad_trim_if_not_none(Xt)\n",
    "\n",
    "            # Stack modality dims to match (T_len, C). Use empty (T_len, 0) if absent.\n",
    "            if Xe is None:\n",
    "                Xe = np.zeros((T_len, 0), np.float32)\n",
    "            if Xm is None:\n",
    "                Xm = np.zeros((T_len, 0), np.float32)\n",
    "            if Xt is None:\n",
    "                Xt = np.zeros((T_len, 0), np.float32)\n",
    "\n",
    "            # Extra safety: ensure no NaN/Inf after normalization + padding\n",
    "            Xe = np.nan_to_num(Xe, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            Xm = np.nan_to_num(Xm, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            Xt = np.nan_to_num(Xt, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "            # Append to buffers\n",
    "            buf[\"X_EEG\"].append(Xe)\n",
    "            buf[\"X_EMG\"].append(Xm)\n",
    "            buf[\"X_ET\"].append(Xt)\n",
    "            buf[\"y_action\"].append(int(r.label_action))\n",
    "            buf[\"y_task\"].append(int(r.task_target))\n",
    "            buf[\"subject_id\"].append(int(r.subject_id))\n",
    "            buf[\"task_code\"].append(int(r.task_code))\n",
    "            buf[\"trial_id\"].append(int(r.trial_id))\n",
    "            buf[\"win_type\"].append(str(r.type))\n",
    "            buf[\"win_len\"].append(win_len)  # original (may differ from FIXED_WINDOW_LEN)\n",
    "            total_windows += 1\n",
    "\n",
    "            if len(buf[\"y_action\"]) >= SHARD_SIZE:\n",
    "                _flush()\n",
    "\n",
    "        _flush()  # final shard\n",
    "        # Write a tiny split-level meta\n",
    "        (out_split / \"split_meta.json\").write_text(json.dumps({\n",
    "            \"num_windows\": int(total_windows),\n",
    "            \"shard_size\": SHARD_SIZE,\n",
    "            \"limit\": _row_limit(split),\n",
    "            \"task_distribution\": rows[\"task_target\"].value_counts().sort_index().to_dict(),\n",
    "            \"export_mode\": mode,\n",
    "            \"fixed_window_len\": FIXED_WINDOW_LEN\n",
    "        }, indent=2))\n",
    "\n",
    "    cache.clear()\n",
    "    print(f\"[fold {fold_id}] done â†’ {out_name}_fold{fold_id} (mode={mode})\")\n",
    "\n",
    "# ---------------- MAIN ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not SPLITS_CSV.exists():\n",
    "        raise SystemExit(f\"[stop] splits not found: {SPLITS_CSV}\")\n",
    "\n",
    "    print(\"Loading splits data...\")\n",
    "    splits = pd.read_csv(SPLITS_CSV)\n",
    "\n",
    "    # ---------------- APPLY QC FILTER (Phase 4 â†’ Phase 5 bridge) ----------------\n",
    "    if QC_CSV.exists():\n",
    "        print(f\"\\n[QC] Loading QC summary from: {QC_CSV}\")\n",
    "        qc = pd.read_csv(QC_CSV)\n",
    "\n",
    "        # Normalise to a \"keep\" mask\n",
    "        if \"qc_keep\" in qc.columns:\n",
    "            qc_use = qc[qc[\"qc_keep\"] == 1].copy()\n",
    "            print(f\"[QC] Found 'qc_keep' column â†’ keeping {len(qc_use)}/{len(qc)} files\")\n",
    "        elif \"qc_flag\" in qc.columns:\n",
    "            # Treat 'bad' (or similar) as reject; others as keep\n",
    "            bad_vals = {\"bad\", \"reject\", \"fail\", \"drop\"}\n",
    "            qc[\"qc_flag_str\"] = qc[\"qc_flag\"].astype(str).str.lower()\n",
    "            qc_use = qc[~qc[\"qc_flag_str\"].isin(bad_vals)].copy()\n",
    "            print(f\"[QC] Found 'qc_flag' column â†’ keeping {len(qc_use)}/{len(qc)} files\")\n",
    "        else:\n",
    "            qc_use = qc.copy()\n",
    "            print(\"[QC] No 'qc_keep' or 'qc_flag' columns found â†’ treating all QC entries as keep\")\n",
    "\n",
    "        # Join by filename (basename) to be robust to absolute/relative paths\n",
    "        splits[\"file_base\"] = splits[\"file\"].apply(lambda x: Path(x).name)\n",
    "        qc_use[\"file_base\"] = qc_use[\"file\"].apply(lambda x: Path(x).name)\n",
    "\n",
    "        good_files = set(qc_use[\"file_base\"].unique())\n",
    "        before = len(splits)\n",
    "        splits = splits[splits[\"file_base\"].isin(good_files)].reset_index(drop=True)\n",
    "        after = len(splits)\n",
    "        dropped = before - after\n",
    "\n",
    "        print(f\"[QC] Applied QC filter to splits_v1:\")\n",
    "        print(f\"     windows before: {before}\")\n",
    "        print(f\"     windows after : {after}\")\n",
    "        print(f\"     windows dropped (qc_bad files): {dropped}\\n\")\n",
    "\n",
    "        splits.drop(columns=[\"file_base\"], inplace=True)\n",
    "    else:\n",
    "        print(f\"\\n[QC] qc_summary_v1.csv not found at {QC_CSV}\")\n",
    "        print(\"     Proceeding without per-file QC filtering.\\n\")\n",
    "\n",
    "    # Basic sanity check\n",
    "    must = {\n",
    "        \"file\", \"subject_id\", \"split\", \"type\", \"task_target\",\n",
    "        \"label_action\", \"start_idx\", \"end_idx\", \"task_code\",\n",
    "        \"trial_id\", \"fold_id\"\n",
    "    }\n",
    "    missing = must - set(splits.columns)\n",
    "    if missing:\n",
    "        raise SystemExit(f\"[stop] splits missing columns: {sorted(missing)}\")\n",
    "\n",
    "    # Analyze original distribution (logging only, after QC)\n",
    "    analyze_task_distribution(splits)\n",
    "\n",
    "    # Export folds\n",
    "    folds = sorted(splits[\"fold_id\"].unique().tolist())\n",
    "    out_root = DATASET_DIR\n",
    "\n",
    "    print(f\"\\nExporting {len(folds)} folds...\")\n",
    "    for k in folds:\n",
    "        k = int(k)\n",
    "        if EXPORT_BALANCED:\n",
    "            export_fold(splits, fold_id=k, out_root=out_root, cache_cap=12, mode=\"balanced\")\n",
    "        if EXPORT_SSL:\n",
    "            export_fold(splits, fold_id=k, out_root=out_root, cache_cap=12, mode=\"ssl\")\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Phase 5 complete!\")\n",
    "    if EXPORT_BALANCED:\n",
    "        print(f\"   Supervised (balanced) output prefix: {OUT_NAME_BALANCED}_fold*\")\n",
    "        print(f\"   Train Task 0 target ratio: â‰¤{MAX_TASK0_RATIO:.1%}\")\n",
    "        print(f\"   Minimum TRAIN samples per non-zero task: {MIN_TASK_SAMPLES}\")\n",
    "        print(f\"   Non-zero tasks equalized with ACTION_MAX_FACTOR={ACTION_MAX_FACTOR}\")\n",
    "    if EXPORT_SSL:\n",
    "        print(f\"   SSL (unbalanced) output prefix: {OUT_NAME_SSL}_fold*\")\n",
    "        print(\"   SSL exports keep full original distribution and all labels.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
