{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491c8df5-7baa-413c-a1f5-25d3cd88817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ====== SETTINGS ======\n",
    "ROOT_DIR = Path(r\"/home/tsultan1/BioRob(Final)/Data\")\n",
    "SUBJECT_GLOB = \"Sub-*\"\n",
    "TARGET_SUBDIR = \"cleaned\"\n",
    "REPORT_NAME = \"trainschema_prune_report.csv\"\n",
    "SCHEMA_LOCK = \"train_schema.json\"\n",
    "\n",
    "\n",
    "DELETE_POLICY = \"add_missing\"\n",
    "\n",
    "# Real vs MI: require EMG only for real trials\n",
    "REQUIRE_EMG = True\n",
    "\n",
    "# --- Keep for later (NOT fed to model directly, but retained in files) ---\n",
    "META_ONLY = [\"subject_id\", \"task\", \"trial\", \"Timestamp_seconds\"]\n",
    "\n",
    "# --- Core inputs for training ---\n",
    "EEG_COLS = [f\"Ch{i}\" for i in range(1, 9)]  # µV\n",
    "EMG_RAW  = [f\"Ch{i} EMG raw\" for i in range(1, 5)]\n",
    "\n",
    "ET_CORE = [\n",
    "    \"ET_GazeLeftx\",\"ET_GazeLefty\",\"ET_GazeRightx\",\"ET_GazeRighty\",\n",
    "    \"ET_PupilLeft\",\"ET_PupilRight\",\n",
    "    \"ET_ValidityLeftEye\",\"ET_ValidityRightEye\",\n",
    "    \"ET_Blink\",\"ET_Fixation\",\"ET_Worn\",\n",
    "]\n",
    "\n",
    "IMU_HEAD = [\n",
    "    \"ET_GyroX\",\"ET_GyroY\",\"ET_GyroZ\",\"ET_AccX\",\"ET_AccY\",\"ET_AccZ\",\n",
    "    \"ET_HeadRotationPitch\",\"ET_HeadRotationYaw\",\"ET_HeadRotationRoll\"\n",
    "]\n",
    "\n",
    "ET_DIST = [\"ET_DistanceLeft\",\"ET_DistanceRight\"]  # include but fill -1 if missing\n",
    "\n",
    "# Always drop: logging/timekeeping/3D eye geometry/stray event columns\n",
    "ALWAYS_DROP = {\n",
    "    \"Row\",\"Timestamp\",\"SampleNumber\",\"ET_TimeSignal\",\"LSL Timestamp\",\n",
    "    \"EventSource\",\"SlideEvent\",\"StimType\",\"Duration\",\"CollectionPhase\",\"SourceStimuliName\",\n",
    "    \"EventSource.1\",\"EventSource.2\",\"EventSource.3\",\n",
    "    \"ET_CameraLeftX\",\"ET_CameraLeftY\",\"ET_CameraRightX\",\"ET_CameraRightY\",\n",
    "    \"ET_Gaze3dEyeballXLeft\",\"ET_Gaze3dEyeballYLeft\",\"ET_Gaze3dEyeballZLeft\",\n",
    "    \"ET_Gaze3dEyeballXRight\",\"ET_Gaze3dEyeballYRight\",\"ET_Gaze3dEyeballZRight\",\n",
    "    \"ET_Gaze3dOpticalAxisXLeft\",\"ET_Gaze3dOpticalAxisYLeft\",\"ET_Gaze3dOpticalAxisZLeft\",\n",
    "    \"ET_Gaze3dOpticalAxisXRight\",\"ET_Gaze3dOpticalAxisYRight\",\"ET_Gaze3dOpticalAxisZRight\",\n",
    "    \"ET_Gaze3dEyelidAngleTopLeft\",\"ET_Gaze3dEyelidAngleBottomLeft\",\n",
    "    \"ET_Gaze3dEyelidAngleTopRight\",\"ET_Gaze3dEyelidAngleBottomRight\",\n",
    "    \"ET_Gaze3dEyelidApertureLeft\",\"ET_Gaze3dEyelidApertureRight\",\n",
    "}\n",
    "\n",
    "# Canonical training schema (order enforced)\n",
    "CANONICAL_ORDER = (\n",
    "    META_ONLY\n",
    "    + EEG_COLS\n",
    "    + EMG_RAW\n",
    "    + ET_CORE\n",
    "    + ET_DIST\n",
    "    + IMU_HEAD\n",
    ")\n",
    "\n",
    "# Which of the canonical columns are considered required (must exist or be addable)\n",
    "REQUIRED_MIN = set(META_ONLY + EEG_COLS + ET_CORE + IMU_HEAD + ET_DIST)\n",
    "if REQUIRE_EMG:\n",
    "    REQUIRED_MIN |= set(EMG_RAW)\n",
    "\n",
    "# Default fillers for when we must add missing columns\n",
    "FILL_DEFAULTS = {\n",
    "    # numeric eye distances (if absent)\n",
    "    \"ET_DistanceLeft\": -1.0,\n",
    "    \"ET_DistanceRight\": -1.0,\n",
    "    # binary-ish eye state flags (if absent)\n",
    "    \"ET_Blink\": 0, \"ET_Fixation\": 0, \"ET_Worn\": 1,\n",
    "    # validity flags (0=valid/1=invalid varies by vendor; keep numeric placeholder)\n",
    "    \"ET_ValidityLeftEye\": 0, \"ET_ValidityRightEye\": 0,\n",
    "}\n",
    "\n",
    "NUMERIC_LIKE = set(CANONICAL_ORDER) - set(META_ONLY)\n",
    "\n",
    "def should_delete_for_missing(missing_required: list) -> bool:\n",
    "    if DELETE_POLICY == \"keep_bad\":\n",
    "        return False\n",
    "    if DELETE_POLICY == \"add_missing\":\n",
    "        return False\n",
    "    # missing_only\n",
    "    return bool(missing_required)\n",
    "\n",
    "def read_csv_safely(path: Path) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=\"utf-8-sig\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def coerce_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Make sure numeric-like columns are numeric\n",
    "    for c in df.columns:\n",
    "        if c in NUMERIC_LIKE:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def add_missing_columns(df: pd.DataFrame, missing_cols: list) -> pd.DataFrame:\n",
    "    for c in missing_cols:\n",
    "        default = FILL_DEFAULTS.get(c, np.nan)\n",
    "        df[c] = default\n",
    "    return df\n",
    "\n",
    "def prune_to_train_schema(df: pd.DataFrame) -> (pd.DataFrame, dict):\n",
    "    cols_present = set(df.columns)\n",
    "\n",
    "    # Build keep set: canonical minus anything we know we always drop\n",
    "    keep_set = set(CANONICAL_ORDER)  # we now ALWAYS include ET distances for schema uniformity\n",
    "\n",
    "    # Figure out what's missing from canonical\n",
    "    missing_canonical = [c for c in CANONICAL_ORDER if c not in cols_present]\n",
    "\n",
    "    # If we’re allowed to add missing, do it\n",
    "    added_missing = []\n",
    "    if DELETE_POLICY == \"add_missing\" and missing_canonical:\n",
    "        df = add_missing_columns(df, missing_canonical)\n",
    "        added_missing = missing_canonical\n",
    "        cols_present = set(df.columns)\n",
    "\n",
    "    # Compute required-missing against REQUIRED_MIN (after optional add)\n",
    "    missing_required = sorted([c for c in REQUIRED_MIN if c not in cols_present])\n",
    "\n",
    "    # Drop everything not in keep_set, plus ALWAYS_DROP\n",
    "    drop_set = (cols_present - keep_set) | (cols_present & ALWAYS_DROP)\n",
    "    pruned = df.drop(columns=[c for c in drop_set if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "    # Coerce types and reorder canonically\n",
    "    pruned = coerce_types(pruned)\n",
    "    pruned = pruned.reindex(columns=CANONICAL_ORDER, fill_value=np.nan)\n",
    "\n",
    "    # Simple schema hash for reporting\n",
    "    schema_sig = hashlib.md5((\"|\".join(pruned.columns)).encode()).hexdigest()[:8]\n",
    "\n",
    "    info = {\n",
    "        \"kept_n\": len(pruned.columns),\n",
    "        \"dropped_n\": len(drop_set),\n",
    "        \"missing_required\": \";\".join(missing_required),\n",
    "        \"added_missing\": \";\".join(added_missing),\n",
    "        \"kept_cols\": \";\".join(pruned.columns),\n",
    "        \"dropped_cols\": \";\".join(sorted(drop_set)),\n",
    "        \"schema_sig\": schema_sig,\n",
    "    }\n",
    "    return pruned, info\n",
    "\n",
    "def write_schema_lock():\n",
    "    lock = {\n",
    "        \"meta_only\": META_ONLY,\n",
    "        \"eeg_cols\": EEG_COLS,\n",
    "        \"emg_raw\": EMG_RAW,\n",
    "        \"et_core\": ET_CORE,\n",
    "        \"et_dist\": ET_DIST,\n",
    "        \"imu_head\": IMU_HEAD,\n",
    "        \"canonical_order\": CANONICAL_ORDER,\n",
    "        \"required_min\": sorted(REQUIRED_MIN),\n",
    "        \"always_drop\": sorted(ALWAYS_DROP),\n",
    "        \"defaults\": FILL_DEFAULTS,\n",
    "        \"require_emg\": REQUIRE_EMG,\n",
    "    }\n",
    "    out = ROOT_DIR / SCHEMA_LOCK\n",
    "    with out.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(lock, f, indent=2)\n",
    "    return out\n",
    "\n",
    "def verify_prune_save():\n",
    "    rows = []\n",
    "    total = good = bad = 0\n",
    "    schema_sigs = set()\n",
    "\n",
    "    print(f\"DELETE_POLICY = {DELETE_POLICY} | REQUIRE_EMG = {REQUIRE_EMG}\")\n",
    "    lock_path = write_schema_lock()\n",
    "    print(f\"[lock] schema → {lock_path}\")\n",
    "\n",
    "    for subj_dir in sorted([p for p in ROOT_DIR.glob(SUBJECT_GLOB) if p.is_dir()]):\n",
    "        scan_dir = subj_dir / TARGET_SUBDIR\n",
    "        if not scan_dir.exists():\n",
    "            continue\n",
    "\n",
    "        csvs = sorted(scan_dir.glob(\"*.csv\"))\n",
    "        if not csvs:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Pruning {subj_dir.name}/{TARGET_SUBDIR}: {len(csvs)} files ===\")\n",
    "\n",
    "        for csv_path in csvs:\n",
    "            total += 1\n",
    "            df = read_csv_safely(csv_path)\n",
    "\n",
    "            if df is None:\n",
    "                print(f\"[READ-FAIL] {csv_path.name} (kept untouched)\")\n",
    "                rows.append({\n",
    "                    \"subject\": subj_dir.name, \"file\": csv_path.name,\n",
    "                    \"status\": \"read_failed_kept\",\n",
    "                    \"reason\": \"read_failed\",\n",
    "                    \"missing_required\": \"\", \"added_missing\": \"\",\n",
    "                    \"kept_n\": \"\", \"dropped_n\": \"\", \"kept_cols\": \"\", \"dropped_cols\": \"\",\n",
    "                    \"schema_sig\": \"\"\n",
    "                })\n",
    "                bad += 1\n",
    "                continue\n",
    "\n",
    "            pruned, info = prune_to_train_schema(df)\n",
    "            missing_req = info[\"missing_required\"].split(\";\") if info[\"missing_required\"] else []\n",
    "\n",
    "            if should_delete_for_missing(missing_req):\n",
    "                try:\n",
    "                    csv_path.unlink()\n",
    "                    print(f\"[DELETED] {csv_path.name} (missing required: {missing_req})\")\n",
    "                    status = \"deleted_missing_required\"\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Could not delete {csv_path.name}: {e}\")\n",
    "                    status = \"delete_failed\"\n",
    "                bad += 1\n",
    "            else:\n",
    "                pruned.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "                add_msg = f\", +{info['added_missing']}\" if info[\"added_missing\"] else \"\"\n",
    "                print(f\"[SAVED] {csv_path.name}: kept {info['kept_n']} cols, dropped {info['dropped_n']}{add_msg} | schema {info['schema_sig']}\")\n",
    "                status = \"ok\" if not missing_req else \"ok_with_missing\"\n",
    "                good += 1\n",
    "                schema_sigs.add(info[\"schema_sig\"])\n",
    "\n",
    "            rows.append({\n",
    "                \"subject\": subj_dir.name, \"file\": csv_path.name, \"status\": status,\n",
    "                \"reason\": \"\" if status.startswith(\"ok\") else (\"missing_required\" if missing_req else \"\"),\n",
    "                **info\n",
    "            })\n",
    "\n",
    "    report_path = ROOT_DIR / REPORT_NAME\n",
    "    pd.DataFrame(rows).to_csv(report_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"\\n=== Summary ===\\nTotal: {total} | Good: {good} | Bad: {bad}\")\n",
    "    print(f\"Report saved: {report_path}\")\n",
    "    if len(schema_sigs) == 1:\n",
    "        print(\"[schema] All files share ONE canonical schema.\")\n",
    "    else:\n",
    "        print(f\"[schema] WARNING: {len(schema_sigs)} different schema signatures encountered (see report).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    verify_prune_save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
