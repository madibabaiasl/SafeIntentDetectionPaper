{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d3d6c2-5783-459f-bf9a-68a93933a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "BioRob Column Guard + Active-Only Export (no overwrite; T0-safe)\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "from __future__ import annotations\n",
    "import csv, glob, re, shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# --------- CONFIG ---------\n",
    "ROOT_DIR = Path(r\"/home/tsultan1/BioRob(Final)/Data\")\n",
    "SRC_LABEL_SUBPATH  = Path(r\"cleaned/synchronized_proper_lite_union_v3/label\")\n",
    "DEST_LABEL_SUBNAME = \"labelonly\"  # sibling to 'label'\n",
    "CSV_GLOB = \"*_icml_consensus_labels.csv\"\n",
    "\n",
    "COPY_SIDECARS = True       # copy matching *_onsets.json and *.unitconv.json if present\n",
    "DELETE_IF_EMPTY = False    # even in dest; normally keep empty outputs for traceability\n",
    "ALLOW_SOURCE_DELETION = False  # NEVER delete source files (we only log)\n",
    "\n",
    "# Canonical header order (matches your shown columns)\n",
    "CANONICAL_BASE = [\n",
    "    \"Timestamp_seconds\",\n",
    "    # (optional 'Timestamp_ms' will be inserted here if present)\n",
    "    \"EMG_Ch1\",\"EMG_Ch2\",\"EMG_Ch3\",\"EMG_Ch4\",\n",
    "    \"EEG_Ch1\",\"EEG_Ch2\",\"EEG_Ch3\",\"EEG_Ch4\",\"EEG_Ch5\",\"EEG_Ch6\",\"EEG_Ch7\",\"EEG_Ch8\",\n",
    "    \"ET_GazeLeftx\",\"ET_GazeLefty\",\"ET_GazeRightx\",\"ET_GazeRighty\",\n",
    "    \"ET_PupilLeft\",\"ET_PupilRight\",\n",
    "    \"ET_ValidityLeftEye\",\"ET_ValidityRightEye\",\n",
    "    \"ET_Blink\",\"ET_Fixation\",\"ET_Worn\",\n",
    "    # (optional ET_DistanceLeft/ET_DistanceRight will be inserted here if present)\n",
    "    \"ET_GyroX\",\"ET_GyroY\",\"ET_GyroZ\",\n",
    "    \"ET_AccX\",\"ET_AccY\",\"ET_AccZ\",\n",
    "    \"ET_HeadRotationPitch\",\"ET_HeadRotationYaw\",\"ET_HeadRotationRoll\",\n",
    "    \"active_raw\",\"active\",\"active_prob\",\"label_action\",\n",
    "    \"subject_id\",\"task\",\"trial\",\"label_11\",\"task_target\",\n",
    "]\n",
    "\n",
    "OPTIONAL_COLS = {\n",
    "    \"Timestamp_ms\",\n",
    "    \"ET_DistanceLeft\",\"ET_DistanceRight\",\n",
    "}\n",
    "\n",
    "# Logs (written under ROOT_DIR)\n",
    "LOG_INVALID_PATH = ROOT_DIR / \"column_invalid_or_skipped.csv\"\n",
    "LOG_FILTER_PATH  = ROOT_DIR / \"active_zero_rows_removed.csv\"\n",
    "\n",
    "# --------- HELPERS ---------\n",
    "def _is_subject_dir(name: str) -> bool:\n",
    "    return re.match(r\"(?i)^sub-?\\d+$\", name) is not None\n",
    "\n",
    "def _iter_label_csvs():\n",
    "    for sub in sorted(d for d in ROOT_DIR.iterdir() if d.is_dir() and _is_subject_dir(d.name)):\n",
    "        src_label_dir = sub / SRC_LABEL_SUBPATH\n",
    "        if not src_label_dir.exists():\n",
    "            print(f\"[skip] label folder missing: {src_label_dir}\")\n",
    "            continue\n",
    "        for f in sorted(Path(p) for p in glob.glob(str(src_label_dir / CSV_GLOB))):\n",
    "            yield f\n",
    "\n",
    "def _clean_header(header):\n",
    "    out = []\n",
    "    for i, h in enumerate(header):\n",
    "        h = (h or \"\")\n",
    "        if i == 0:\n",
    "            h = h.replace(\"\\ufeff\", \"\")\n",
    "        out.append(h.strip())\n",
    "    return out\n",
    "\n",
    "def _target_order_for(header: list[str]) -> list[str]:\n",
    "    has_ts_ms = \"Timestamp_ms\" in header\n",
    "    has_distL = \"ET_DistanceLeft\" in header\n",
    "    has_distR = \"ET_DistanceRight\" in header\n",
    "\n",
    "    target = []\n",
    "    for name in CANONICAL_BASE:\n",
    "        if name == \"Timestamp_seconds\":\n",
    "            target.append(name)\n",
    "            if has_ts_ms:\n",
    "                target.append(\"Timestamp_ms\")\n",
    "        elif name == \"ET_Worn\":\n",
    "            target.append(name)\n",
    "            if has_distL: target.append(\"ET_DistanceLeft\")\n",
    "            if has_distR: target.append(\"ET_DistanceRight\")\n",
    "        else:\n",
    "            target.append(name)\n",
    "    # Only keep those that actually exist in the file\n",
    "    return [c for c in target if c in header]\n",
    "\n",
    "def _is_task0_file(path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Detect Task 0 from filename using the same convention as the labeler:\n",
    "        - (T|M)(\\d{2,})\n",
    "        - FIRST digit after T/M = task\n",
    "        - remaining digits = trial\n",
    "\n",
    "    Examples:\n",
    "        ..._T02_...   → task=0, trial=2   (REST)\n",
    "        ..._T016_...  → task=0, trial=16  (REST)\n",
    "        ..._T114_...  → task=1, trial=14  (NOT REST)\n",
    "        ..._M305_...  → imagery; ignored here\n",
    "    \"\"\"\n",
    "    stem = path.stem\n",
    "    m = re.search(r'(?:^|_)(T|M)(\\d{2,})', stem, flags=re.I)\n",
    "    if not m:\n",
    "        return False\n",
    "\n",
    "    kind = m.group(1).upper()\n",
    "    if kind != 'T':\n",
    "        # Only physical T-trials get the REST/T0 treatment\n",
    "        return False\n",
    "\n",
    "    digits = m.group(2)\n",
    "    task = int(digits[0])  # FIRST digit only (matches parse_ids_from_stem)\n",
    "    return task == 0\n",
    "\n",
    "\n",
    "def _dest_path_for(src_csv: Path) -> Path:\n",
    "    # src: .../synchronized_proper_lite_union_v3/label/file.csv\n",
    "    # dst: .../synchronized_proper_lite_union_v3/label_active_only/file.csv\n",
    "    src_label_dir = src_csv.parent\n",
    "    dest_label_dir = src_label_dir.parent / DEST_LABEL_SUBNAME\n",
    "    dest_label_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return dest_label_dir / src_csv.name\n",
    "\n",
    "def _sidecars_for(src_csv: Path):\n",
    "    return [\n",
    "        src_csv.with_name(src_csv.stem.replace(\"_icml_consensus_labels\",\"_onsets\") + \".json\"),\n",
    "        src_csv.with_suffix(\".unitconv.json\"),\n",
    "    ]\n",
    "\n",
    "# --------- MAIN ---------\n",
    "invalid_log = []\n",
    "filter_rows_log = []\n",
    "\n",
    "n_files = 0\n",
    "n_exported = 0\n",
    "n_reordered = 0\n",
    "n_filtered = 0\n",
    "n_t0_skipped_filter = 0\n",
    "n_invalid = 0\n",
    "\n",
    "ALLOWED_SET = set(CANONICAL_BASE) | OPTIONAL_COLS\n",
    "REQUIRED_SET = set(CANONICAL_BASE)  # optionals not required\n",
    "\n",
    "for src_csv in _iter_label_csvs():\n",
    "    n_files += 1\n",
    "    try:\n",
    "        with open(src_csv, \"r\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "            reader = csv.reader(fh)\n",
    "            header_raw = next(reader)\n",
    "        header = _clean_header(header_raw)\n",
    "\n",
    "        set_cur = set(header)\n",
    "        extras = sorted(list(set_cur - ALLOWED_SET))\n",
    "        missing_required = sorted(list(REQUIRED_SET - set_cur))\n",
    "\n",
    "        if extras or missing_required:\n",
    "            msg = f\"invalid schema: missing={missing_required} extras={extras}\"\n",
    "            print(f\"[skip-invalid] {src_csv} | {msg}\")\n",
    "            invalid_log.append({\n",
    "                \"file\": str(src_csv),\n",
    "                \"when\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "                \"missing\": \";\".join(missing_required),\n",
    "                \"extra\": \";\".join(extras),\n",
    "            })\n",
    "            n_invalid += 1\n",
    "            # Do NOT delete source; just skip\n",
    "            continue\n",
    "\n",
    "        # Load and reorder\n",
    "        df = pd.read_csv(src_csv, low_memory=False)\n",
    "        header_now = [c.strip() for c in df.columns]\n",
    "        target_order = _target_order_for(header_now)\n",
    "        if header_now != target_order:\n",
    "            print(f\"[reorder] {src_csv}\")\n",
    "            df = df[target_order]\n",
    "            n_reordered += 1\n",
    "\n",
    "        dest_csv = _dest_path_for(src_csv)\n",
    "\n",
    "        # T0 safeguard → no filtering, just export reordered\n",
    "        if _is_task0_file(src_csv):\n",
    "            print(f\"[export T0-no-filter] {dest_csv.name}\")\n",
    "            if COPY_SIDECARS:\n",
    "                for sc in _sidecars_for(src_csv):\n",
    "                    if sc.exists():\n",
    "                        dest_sc = dest_csv.parent / sc.name\n",
    "                        try: shutil.copy2(sc, dest_sc)\n",
    "                        except Exception as e: print(f\"  [warn] sidecar copy failed {sc} → {dest_sc}: {e}\")\n",
    "            df.to_csv(dest_csv, index=False)\n",
    "            n_t0_skipped_filter += 1\n",
    "            n_exported += 1\n",
    "            continue\n",
    "\n",
    "        # Non-T0: filter active==1\n",
    "        if \"active\" not in df.columns:\n",
    "            msg = \"missing 'active' after load\"\n",
    "            print(f\"[skip-invalid] {src_csv} | {msg}\")\n",
    "            invalid_log.append({\n",
    "                \"file\": str(src_csv),\n",
    "                \"when\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "                \"missing\": \"'active'\",\n",
    "                \"extra\": \"\",\n",
    "            })\n",
    "            n_invalid += 1\n",
    "            continue\n",
    "\n",
    "        before = len(df)\n",
    "        df = df[df[\"active\"].astype(\"int64\") == 1].copy()\n",
    "        removed = before - len(df)\n",
    "\n",
    "        if removed > 0:\n",
    "            print(f\"[filter] {src_csv.name}  kept={len(df)}  removed={removed}\")\n",
    "            n_filtered += 1\n",
    "            filter_rows_log.append({\n",
    "                \"file\": str(src_csv),\n",
    "                \"when\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "                \"rows_before\": before,\n",
    "                \"rows_removed\": removed,\n",
    "                \"rows_after\": len(df),\n",
    "                \"dest\": str(dest_csv),\n",
    "            })\n",
    "\n",
    "        if len(df) == 0 and DELETE_IF_EMPTY:\n",
    "            # export nothing; still create an empty CSV if you prefer traceability:\n",
    "            # df.to_csv(dest_csv, index=False)\n",
    "            print(f\"[note] empty after filter → not exporting: {src_csv.name}\")\n",
    "            continue\n",
    "\n",
    "        if COPY_SIDECARS:\n",
    "            for sc in _sidecars_for(src_csv):\n",
    "                if sc.exists():\n",
    "                    dest_sc = dest_csv.parent / sc.name\n",
    "                    try: shutil.copy2(sc, dest_sc)\n",
    "                    except Exception as e: print(f\"  [warn] sidecar copy failed {sc} → {dest_sc}: {e}\")\n",
    "\n",
    "        df.to_csv(dest_csv, index=False)\n",
    "        n_exported += 1\n",
    "\n",
    "    except StopIteration:\n",
    "        print(f\"[skip-invalid] {src_csv} | empty or no header\")\n",
    "        invalid_log.append({\n",
    "            \"file\": str(src_csv),\n",
    "            \"when\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "            \"missing\": \"ALL\",\n",
    "            \"extra\": \"\",\n",
    "        })\n",
    "        n_invalid += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {src_csv} | {e}\")\n",
    "        invalid_log.append({\n",
    "            \"file\": str(src_csv),\n",
    "            \"when\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "            \"missing\": \"EXCEPTION\",\n",
    "            \"extra\": str(e),\n",
    "        })\n",
    "        n_invalid += 1\n",
    "\n",
    "# Write logs\n",
    "if invalid_log:\n",
    "    pd.DataFrame(invalid_log).to_csv(LOG_INVALID_PATH, index=False)\n",
    "    print(f\"[log] Invalid/Skipped list → {LOG_INVALID_PATH}\")\n",
    "\n",
    "if filter_rows_log:\n",
    "    pd.DataFrame(filter_rows_log).to_csv(LOG_FILTER_PATH, index=False)\n",
    "    print(f\"[log] Active-row filter stats → {LOG_FILTER_PATH}\")\n",
    "\n",
    "print(\n",
    "    f\"\\n[summary] scanned={n_files}  exported={n_exported}  \"\n",
    "    f\"reordered={n_reordered}  filtered_files={n_filtered}  \"\n",
    "    f\"T0_passthrough={n_t0_skipped_filter}  invalid_or_skipped={n_invalid}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
